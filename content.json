{"pages":[{"title":"Courses","text":"Basic Cources of Computer Science Basis for computer principles and system. Catagory Course Notes Null MIT 6.NULL: The Missing Semester of Your CS Education Computation Structure MIT 6.004: Computation Structures Operating System MIT 6.S081: Operating System Engineering Parallel Computing Stanford CS 149: Parallel Computing Computer Network Stanford CS 144: Introduction to Computer Networking Database System MIT 6.830/6.814: Database Systems(Java) Lab Notes CMU 15-445/645: Database Systems(C++) Advanced Cources for System Development Advanced courses related to system development. Catagory Course Notes Distributed System MIT 6.824: Distributed Systems Database Development Learning Map These are for how to develop a database but not for how to use a database. CMU 15-445/645 OR MIT 6.830/6.814 Lectures and readings cover all topics in the database, such as index, query algorithms, query optimization, transaction and distributed database. Labs focus on the realization of core modules of relational database. MIT 6.824 The basic theory of distributed systems. An indispensable step towards a distributed database. TinyKV Implement a distributed KV storage, inspired by MIT 6.824. TinySQL Realize a SQL layer compatible with the MySQL protocol and work with TinyKV to form a complete distributed relational database. Materials Awesome Database Learning","link":"/courses/index.html"},{"title":"About","text":"I AMLiebing Yu Interests Streaming Systems &amp; OLAPs Data Architecture Contact Emial: liebing.yu@outlook.com Experience 2024.06 - Present: Alibaba Taobao &amp; Tmall Group, Hangzhou 2022.07 - 2024.05: Alibaba Cloud, Hangzhou 2021.05 - 2021.08: Alibaba Cloud, Intern, Hangzhou Last update: 2024.06","link":"/about/index.html"},{"title":"Collections","text":".card { width: 250px; box-shadow: 0 1px 1px 0 rgba(0, 0, 0, 0.15), 0 6px 10px 0 rgba(0, 0, 0, 0.19);//设置两层阴影 text-align: center; vertical-align: middle; border-radius: 5px; } .card:hover { box-shadow: 1px 10px 10px 1px rgba(0, 0, 0, .25); transition: all .1s ease-in-out; } .header { background-color: #fefefe; color: white; padding: 10px; padding-bottom: 0px; font-size: 40px; } .container { padding: 10px; padding-top: 0px; } .img{ width: 230px; height: 130px; } #wrap{ display: grid; grid-template-columns: repeat(3, 250px); grid-gap: 20px; } Apache Flink原理与实践 Apache Calcite原理与实践 数据系统经典论文阅读","link":"/collections/index.html"},{"title":"Friends","text":"Honly’s Blog xcTorres’s Blog","link":"/friends/index.html"},{"title":"Photos","text":".card { width: 250px; box-shadow: 0 1px 1px 0 rgba(0, 0, 0, 0.15), 0 6px 10px 0 rgba(0, 0, 0, 0.19);//设置两层阴影 text-align: center; vertical-align: middle; border-radius: 5px; } .card:hover { box-shadow: 1px 10px 10px 1px rgba(0, 0, 0, .25); transition: all .1s ease-in-out; } .header { background-color: #fefefe; color: white; padding: 10px; padding-bottom: 0px; font-size: 40px; } .container { padding: 10px; padding-top: 0px; } .img{ width: 230px; height: 130px; } #wrap{ display: grid; grid-template-columns: repeat(3, 250px); grid-gap: 20px; } 2022-02-04 桐乡·朱家桥 2022-01-24 台州 2022-01-16 杭州·茅家埠 2020-11-20 深圳 2020-11-15 成都 2020-10-27 武汉·欢乐谷 2020-10-02 长沙 2020-07-08 南京 2019-10-19 上海·外滩 2019-09-21 西安·大唐不夜城 2019-07-28 厦门","link":"/photos/index.html"},{"title":"Apache Flink原理与实践","text":"Apache Flink原理与实践本合集是对笔者所写的Apache Flink原理与实践系列博客的整理, 包括Flink在使用中的最佳实践及系统的源码分析. Flink前沿探索 Flink MT + Paimon - 开源低成本增量计算的曙光 Flink最佳实践 Flink最佳实践 - Watermark原理及实践问题解析 Flink最佳实践 - Table与DataStream互相转换 Flink SQL最佳实践 - HBase SQL Connector应用 Glink SQL最佳实践 - GeoMesa SQL Connector应用 Flink源码解析Flink SQL整体框架 Flink SQL源码 - 整体架构及处理流程 Flink SQL源码 - Table API &amp; SQL概览 具体实现 Flink SQL源码 - Changelog原理与实现 Flink SQL源码 - Mini-Batch原理与实现 Flink SQL源码 - SQL函数原理与实现 Flink Runtime Flink源码 - RPC原理与实现 Flink Connector Flink源码 - 从KafkaConnector看Source接口重构","link":"/collections/flink/index.html"},{"title":"Apache Calcite原理与实践","text":"Apache Calcite原理与实践本合集是对笔者所写的Apache Calcite原理与实践系列博客的整理, 后续若有新发布的相关文章也会收录进来. 目录 Apache Calcite整体架构及处理流程 Apache Calcite SQL解析及语法扩展 Apache Calcite SQL验证 Apache Calcite关系代数 Apache Calcite查询优化概述 Apache Calcite查询优化器之HepPlanner Apache Calcite查询优化器之VolcanoPlanner","link":"/collections/calcite/index.html"},{"title":"数据系统经典论文阅读","text":"数据系统经典论文阅读阅读数据系统的经典论文, 不仅能从中了解相关系统的实现原理, 更重要的是能了解系统产生的前因后果和来龙去脉. 笔者认为对于需要深入学习的系统而言, 理解其架构和实现原理固然重要, 更重要的是能够理解其产生原因, 能解决的问题以及在真实场景下的局限性, 只有了解这些才能真正将对系统的理解转化为生产力. 本合集是对笔者总结的数据系统经典论文阅读笔记的整理. Lakehouse三驾马车Lakehouse(湖仓一体)架构由Databricks首次提出, 它是一个通用的数据平台架构. 为了实现Lakehouse架构, 需要一个存储, 一个计算. Databricks发表了三篇论文, 分别描述Lakehouse架构, 存储引擎Delta Lake和计算引擎Photon, 笔者将其统称为Lakehouse三驾马车. 论文阅读总结如下(Photon论文总结暂未完成): 论文阅读 - Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics 论文阅读 - Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores","link":"/collections/paper/index.html"},{"title":"2019-09-21 西安·大唐不夜城","text":"","link":"/photos/2019-09-21-xian-datangbuyecheng/index.html"},{"title":"2019-07-28 厦门","text":"","link":"/photos/2019-07-28-xiamen/index.html"},{"title":"2020-10-27 武汉·欢乐谷","text":"","link":"/photos/2020-10-27-wuhan-huanlegu/index.html"},{"title":"2020-1-15 成都","text":"","link":"/photos/2020-11-15-chengdu/index.html"},{"title":"2019-10-19 上海·外滩","text":"","link":"/photos/2019-10-19-shanghai-waitan/index.html"},{"title":"2020-07-08 南京","text":"","link":"/photos/2020-07-08-nanjing/index.html"},{"title":"2020-10-02 长沙","text":"","link":"/photos/2020-10-02-changsha/index.html"},{"title":"2020-1-15 成都","text":"","link":"/photos/2020-11-20-shenzhen/index.html"},{"title":"2022-01-24 台州","text":"","link":"/photos/2022-01-24-taizhou/index.html"},{"title":"2022-02-24 桐乡·朱家桥","text":"","link":"/photos/2022-02-24-tongxiang-zhujiaqiao/index.html"},{"title":"2022-01-16 杭州·茅家埠","text":"","link":"/photos/2022-01-16-hangzhou-maojiabu/index.html"}],"posts":[{"title":"编译原理实践 - JavaCC解析表达式并生成抽象语法树","text":"本文使用JavaCC实现表达式的解析, 并将解析结果生成为抽象语法树(Abstract Syntax Tree, AST). 实际上对于表达式这种简单的”语言”, 可以边解析边计算从而直接得出结果, 生成抽象语法树有点”杀鸡焉用牛刀”了. 但是对于更加复杂的语言, 如通用计算机编程语言(C, Java等)或数据库查询语言SQL, 生成抽象语法树就是必须的了. 只有依托于抽象语法树才能进一步进行语义分析(如引用消解, 类型检查等), 代码生成或将SQL转化为关系代数等工作. 然而, 直接上手编写JavaCC的语法文件, 构建复杂语言的抽象语法树难度较大. 本文以表达式这个较为简单的”语言”为例, 通过编写JavaCC的语法文件将其转化为抽象语法树, 并使用Visitor模式访问抽象语法树对表达式进行计算. 这个例子可以说是”麻雀虽小, 五脏俱全”, 包含词法分析和语法分析的完整内容, 通过这个例子可以了解JavaCC语法文件的编写以及抽象语法树的构建方式, 在此基础上便可进一步构建更加复杂的语言的解析器. 本文的完整代码见这里. JavaCC介绍JavaCC功能JavaCC的全称是Java Compiler Compiler, 以下是其官网的介绍. Java Compiler Compiler (JavaCC) is the most popular parser generator for use with Java applications.A parser generator is a tool that reads a grammar specification and converts it to a Java program that can recognize matches to the grammar.Java 编译器编译器(JavaCC)是Java应用程序中最流行的解析器(Parser)生成器.解析器生成器是一种工具, 它可以读取语法规范并将其转换为能够识别与语法匹配的Java程序. 在编译器实现中, 首先要做的就是词法分析和语法分析: 词法分析: 解析代码并生成Token(一个单词的字面和它的种类及语义值)序列, 词法分析的实现一般称为扫描器(Scanner); 语法分析: 利用扫描器生成的Token序列来生成抽象语法树, 语法分析的实现一般称为解析器(Parser). 如果不依赖任何工具, 那就必须手写扫描器和解析器, 但是随着人们对编译原理的深入研究, 发现扫描器和解析器都可以根据一定的规则自动生成. 于是就出现了一系列的解析器生成器, 如Yacc, Anltr, JavaCC等. 这些解析器生成器都可以根据自定义的语法规则文件自动生成解析器代码, 比如JavaCC可以根据后缀为.jj的语法规则文件生成解析器的Java代码, 这就避免了手动编写扫描器和解析器的繁琐, 可以让我们专注于语法规则的设计. JavaCC语法文件JavaCC可根据用户编写的后缀名为.jj的语法规则文件自动生成解析器. 由于本文重点关注的是解析表达式并构建抽象语法树这个系统过程, 这里遵循”够用即可”的原则, 仅讲述在表达式解析中需要用到的语法. 关于JavaCC语法规则更详细的描述可阅读参考[1][2], 其中”自制编译器”的第一, 二部分对JavaCC的语法规则有详尽的解释, 推荐阅读. JavaCC的语法文件一般包含如下内容.12345678910111213141516options { JavaCC 的选项}PARSER_BEGIN(解析器类名)package 包名;import 库名;public class 解析器类名 { 任意的 Java 代码}PARSER_END(解析器类名)扫描器的描述解析器的描述 语法文件的开头是JavaCC选项的options块, 可以省略; PARSER_BEGIN和PARSER_END之间是一个Java类, 可以支持任何Java语法, 这里定义的类成员变量或方法也能在解析器描述的actions中使用; 最后是扫描器的描述和解析器的描述, 后面会进一步介绍. 有了以上概念之后, 我们可以看如下语法文件Adder.jj, 运行javacc Adder.jj命令即可生成一个Adder.java文件, 该文件是一个普通的Java类文件, 在命令行或IDE中编译运行后即可读取输入的整数加法表达式进行解析并计算.12345678910111213141516171819202122232425262728293031323334353637383940414243options { STATIC = false;}PARSER_BEGIN(Adder)import java.io.*;public class Adder { public static void main(String[] args){ for (String arg : args) { try{ System.out.println(evaluate(arg)); } catch (ParseException e) { e.printStackTrace(); } } } public static long evaluate(String src) throws ParseException { Reader reader = new StringReader(src); return new Adder(reader).expr(); }}PARSER_END(Adder)// 扫描器的描述SKIP : { &lt;[&quot; &quot;, &quot;\\t&quot;, &quot;\\r&quot;, &quot;\\n&quot;]&gt; }TOKEN : { &lt;INTEGER: ([&quot;0&quot;-&quot;9&quot;])+&gt; }// 解析器的描述long expr():{ Token x, y;}{ x=&lt;INTEGER&gt; &quot;+&quot; y=&lt;INTEGER&gt; &lt;EOF&gt; { return Long.parseLong(x.image) + Long.parseLong(y.image); }} 在PARSER_BEGIN和PARSER_END之间就是一个普通的Java类定义, 可以定义包含main函数在内的所有内容, 此外JavaCC还会自动生成以下构造函数, 上述文件中的evaluate方法就使用了参数为Reader的构造函数: Parser(InputStream s) Parser(InputStream s, String encoding) Parser(Reader r) Parser(××××TokenManager tm) 扫描器的描述主要是利用正则表达式描述各种Token SKIP表示匹配的字符都可以跳过; TOKEN : { &lt;INTEGER: ([&quot;0&quot;-&quot;9&quot;])+&gt; }表示定义了名为INTEGER的Token, 它可以由1个或多个数字组成. 解析器的描述支持扩展巴科斯范式(Extended Backus–Naur Form, EBNF), 可以在适当位置嵌入任意Java代码, 使用Adder类所定义的成员变量或方法. 这是由于这里的expr()实际上会生成为Adder类中的一个同名方法. 在expr()之后的{}中可以定义任何临时变量, 这里的Token是JavaCC预定义的表示Token的类. 在之后的{}中可定义方法体, 方法体支持EBNF, 在EBNF中可随时嵌入{}并在里面编写Java代码, 这在JavaCC中称为action. 比如在expr()中, 在解析到&lt;INTEGER&gt; &quot;+&quot; &lt;INTEGER&gt; &lt;EOF&gt;之后就会执行之后action中的代码. 这里只是简单的将对于的字符转换为整数相加后范围, 如果有需要也可以在action中添加生成抽象语法树的逻辑. 在EBNF中可以将任何元素赋值给临时变量. 表达式解析表达式扫描器为了构建表达式扫描器, 我们需要编写正则表达式, 以解析表达式中可能出现的所有字符, 并将其转化为相应的Token. 以下是表达式扫描器的描述, 第一个Token主要是数字的正则表达式, 第二个Token是一些三角函数, 读者可以加入更多的自定义Token, 比如sqrt等, 支持更丰富的运算. 123456789101112131415SKIP : { &quot; &quot; | &quot;\\r&quot; | &quot;\\t&quot; }TOKEN:{ &lt; NUMBER: (&lt;DIGIT&gt;)+ ( &quot;.&quot; (&lt;DIGIT&gt;)+ )? &gt;| &lt; DIGIT: [&quot;0&quot;-&quot;9&quot;] &gt;| &lt; EOL: &quot;\\n&quot; &gt;}TOKEN:{ &lt;SIN: &quot;sin&quot;&gt;| &lt;COS: &quot;cos&quot;&gt;| &lt;TAN: &quot;tan&quot;&gt;} 表达式解析器表达式解析器的描述相对复杂一些, 为方便起见, 本文将表达式中可能出现的元素分为三类: primary: 是表达式中的一个独立元素, 可以是一个数字(如123, 987.34), 也可以是括号包围的表达式(如(1+2)), 也可以是在数字或表达式上附带一元运算符形成的元素(如7!, sin(3*4+2)). 由此可见这里的独立只是相对而言的, 表达式中可以有多个primary元素. term: 是表达式中高优先级的元素, 需要优先计算, 它可以是一个单独的primary, 也可以是高优先级的二元运算符(和/)连接的元素, 如32, 9/3. expr: 是一个表达式元素, 它可以是一个单独的term, 也可以是第优先级的二元运算符(+和-)连接的元素, 如3-2, 42+34. 依据上述描述的编写的词法解析规则如下, 为方便理解, 暂时去掉了所有action, 读者可根据注释仔细理解.123456789101112131415161718192021222324void expr(): { }{ term() (&quot;+&quot; expr() | &quot;-&quot; expr())* // term开头, 后面可能有+expr或-expr, 也可能没有}void term(): { }{ primary() (&quot;*&quot; term() | &quot;/&quot; term())* // primary开头, 后面可能有*term或/term, 也可能没有}void primary(): { }{ &lt;NUMBER&gt; // 数字, 如123, 789.98| LOOKAHEAD(&lt;NUMBER&gt; &quot;!&quot;) // 数字的阶乘, 如3!, 5! &lt;NUMBER&gt; &quot;!&quot;| LOOKAHEAD(&quot;(&quot; expr() &quot;)&quot; &quot;!&quot;) // 表达式的阶乘, 如(3+2*3)! &quot;(&quot; expr() &quot;)&quot; &quot;!&quot; | &quot;+&quot; primary() // &quot;+&quot;号前缀, 如+3, +(3+3*2)| &quot;-&quot; primary() // &quot;-&quot;号前缀, 如-3, -(3+3*2)| &quot;(&quot; expr() &quot;)&quot; // 括号包围的表达式, 如(3+3*2)| &lt;SIN&gt; &quot;(&quot; expr() &quot;)&quot; // sin运算, 如sin(3), sin(3+3*4)| &lt;COS&gt; &quot;(&quot; expr() &quot;)&quot; // cos运算, 如cos(3), cos(3+3*4)| &lt;TAN&gt; &quot;(&quot; n=expr() &quot;)&quot; // tan运算, 如tan(3), tan(3+3*4)} 抽象语法树构建有了上述词法和语法描述就可以实现表达式的解析了, 但也仅仅是解析, 除了检查输入的表达式在语法上是否合规并没有其他任何作用. 要实现表达式的计算就需要在解析器语法描述的适当位置加入action. 对于表达式计算这种简单的应用我们可以直接在相应位置插入计算的代码, 类似于Adder.jj那样. 不过本文会在action中添加生成抽象语法树的代码, 从而将表达式转化为抽象语法树, 然后在使用Visitor模式遍历抽象语法树计算结果. 抽象语法树的节点在插入action代码之前, 我们先来设计一下抽象语法树的各个节点. 首先, 抽象语法树需要一个抽象的节点基类Node. Node类中只有一个属性sign用于指示当前节点的正负号. 抽象方法accept用于接收Visitor实现对节点的遍历. 1234567891011121314public abstract class Node { protected int sign = 1; public int getSign() { return sign; } public void setSign(int sign) { this.sign = sign; } public abstract &lt;T&gt; T accept(ASTVisitor&lt;T&gt; visitor);} Node类有多个实现类: ExprNode表示表示一个expr; TermNode表示一个term; UnaryNode表示一元运算符对应的节点, 有4个实现类; ValueNode表示一个数值节点. 各个类的继承关系如下图, 由于其实现较为简单, 这里不再展示完整的代码, 如有需要可参考这里. 基于上述节点类, 我们便可在语法文件中添加action, 从而在解析的过程中构建抽象语法树. primary添加action后如下, 其返回值为Node类型, 因为可能返回各种类型的节点, 这里统一用基类表示. 在每种情况后都增加了action, 其主要功能是为当前解析到的字符构建抽象语法树节点. 比如, 读到&lt;NUMBER&gt;表示当前读到了一个数字, 之后就会创建一个ValueNode; LOOKAHEAD(&lt;NUMBER&gt; &quot;!&quot;)表示超前扫描, 也就是说当扫描到&lt;NUMBER&gt;后还要继续往后扫描, 判断下一个字符是不是!, 如果不是则回到&lt;NUMBER&gt;的情况, 否则需要生成一个阶乘节点FactorialNode. 其他情况类似, 这里不再赘述. 1234567891011121314151617181920212223242526272829303132Node primary():{ Token t; Token p; Node n;}{ t=&lt;NUMBER&gt; { double number = Double.parseDouble(t.image); return new ValueNode(number); }| LOOKAHEAD(&lt;NUMBER&gt; &quot;!&quot;) t=&lt;NUMBER&gt; &quot;!&quot; { String value = t.image; double number = Double.parseDouble(value); return new FactorialNode(new ValueNode(number)); }| LOOKAHEAD(&quot;(&quot; n=expr() &quot;)&quot; &quot;!&quot;) &quot;(&quot; n=expr() &quot;)&quot; &quot;!&quot; { return new FactorialNode(n); }| &quot;+&quot; n=primary() { return n; }| &quot;-&quot; n=primary() { n.setSign(-1); return n; }| &quot;(&quot; n=expr() &quot;)&quot; { return n; }| &lt;SIN&gt; &quot;(&quot; n=expr() &quot;)&quot; { return new SinNode(n); }| &lt;COS&gt; &quot;(&quot; n=expr() &quot;)&quot; { return new CosNode(n); }| &lt;TAN&gt; &quot;(&quot; n=expr() &quot;)&quot; { return new TanNode(n); }} term添加action之后如下, 其返回值同样为Node. term可能有一个单独的primary组成, 也可能在之后*或/ 另一个term, 每种情况下的action都返回了对应的节点. 12345678910111213Node term():{ Node left; Node right;}{ left=primary() ( &quot;*&quot; right=term() { return new TermNode(left, right, Operator.MUL); } | &quot;/&quot; right=term() { return new TermNode(left, right, Operator.DIV); } )* { return left; }} expr与term类似, 可能有一个单独的term组成, 也可能在之后+或-另一个expr, 每种情况都返回对应的节点.12345678910111213Node expr():{ Node left; Node right;}{ left=term() ( &quot;+&quot; right=expr() { return new ExprNode(left, right, Operator.PLUS); } | &quot;-&quot; right=expr() { return new ExprNode(left, right, Operator.MINUS); } )* { return left; }} 有了上述语法规则之后, 便可在PARSER_BEGIN和PARSER_END定义一个解析器类了. 这里我们将其称为Calculator.123456789101112PARSER_BEGIN(Calculator)package javacc.learning.calculator.parser;import javacc.learning.calculator.ast.*;public class Calculator { public Node parse() throws ParseException { return expr(); }}PARSER_END(Calculator) Calculator只有一个parse函数, 它调用了expr语法生成的同名函数. 通过JavaCC生成Calculator.java文件之后, 我们便可通过以下方式解析表达式并生成抽象语法树.123Calculator calculator = new Calculator(System.in);// node为抽象语法树根节点Node node = calculator.parse(); 上述表达式解析和生成抽象语法树的完整代码可参见Calculator.jj. Vsitor模式遍历抽象语法树生成抽象语法树, 相当于利用JavaCC将无结构的表达式字符串转化为了内存中结构化的树. 完成了抽象语法树的生成JavaCC的任务也就完成了, 之后如何通过抽象语法树计算表达式的结果就需要我们自己解决了. 在编译器中, 通常会将源代码解析为抽象语法树, 然后使用Visitor模式遍历抽象语法树进行语义分析, 如引用消解, 静态类型检查等. 这里我们也使用Visitor模式对表达式抽象语法树进行遍历计算结果. 为了遍历抽象语法树计算结果, 我们也可以不使用Visitor模式, 而利用多态实现不同节点的计算. 比如我们可以在Node中增加一个calculate抽象方法, 让每个实现类依据节点语义实现不同的计算方法. 这样当调用抽象语法树根节点的calculate方法后, 就会递归调用子节点的calculate方法直到叶节点返回结果.123456789101112131415161718192021222324252627public abstract class Node { ... public abstract double calculate();}public class ValueNode extends Node { ... public double calculate() { return value; }}public class SinNode extends UnaryNode { ... public double calculate() { double value = node.calculate(); double result = 1; for (int i = 1; i &lt;= value; ++i) { result *= i; } return result * getSign(); }} 然而使用上述方法存在诸多缺点: 对不同的遍历场景需要为节点类添加不同的方法, 比如上面为了计算表达式结果添加了calculate方法, 如果需要打印抽象语法树就需要再新增一个方法dump. 这样一旦有新的需求就必须不断改动节点类群, 由于节点类群众多, 修改相当困难. 由于对于一种场景, 其实现逻辑都分散在各个节点类中, 不便于阅读相关代码. 由于上述缺点我们有必要引入Visitor模式对抽象语法树进行遍历. Visitor模式有一个抽象接口, 定义了对各种类型的节点进行访问的方法. 比如在表达式抽象语法树的遍历中, 我们定义了如下ASTVisitor接口, 其中包含对各种节点的visit方法.123456789public interface ASTVisitor&lt;T&gt; { T visit(ExprNode node); T visit(TermNode node); T visit(SinNode node); T visit(CosNode node); T visit(TanNode node); T visit(FactorialNode node); T visit(ValueNode node);} 有了ASTVisitor接口, 我们只需在Node类中定义一个抽象方法accept用于实现不同场景下各个节点的遍历逻辑.1234567891011121314public abstract class Node { ... public abstract &lt;T&gt; T accept(ASTVisitor&lt;T&gt; visitor);}public class ValueNode extends Node { ... @Override public &lt;T&gt; T accept(ASTVisitor&lt;T&gt; visitor) { return visitor.visit(this); }} 有了上述接口之后, 我们只需要为不同的场景添加不同的实现类即可对抽象语法树进行遍历, 从而实现不同的逻辑. 以计算表达式结果为例, 可以添加如下实现类. 在实现类的visit方法中我们根据节点类型的不同编写对于的计算逻辑即可.1234567891011121314151617181920212223public class CalculateVisitor implements ASTVisitor&lt;Double&gt; { public double calculate(Node node) { return node.accept(this); } ... @Override public Double visit(FactorialNode node) { double value = node.getNode().accept(this); double result = 1; for (int i = 1; i &lt;= value; ++i) { result *= i; } return result * node.getSign(); } @Override public Double visit(ValueNode node) { return node.getValue() * node.getSign(); }} 如果要添加新的遍历逻辑, 比如打印抽象语法树, 我们只需要新增一个DumpVisitor并实现相应的方法即可. CalculateVisitor和DumpVisitor的完整代码都在这里. 总结本文以表达式这种简单的”语言”为依托, 讲述了使用JavaCC构建解析器并生成抽象语法树的方法. 写文本的主要目的是笔者在阅读一些编译器实战书籍时, 由于通用编程语言一般较为复杂解析难度大, 直接看这些语言的语法文件有时难以理解从解析到抽象语法树构建这个系统流程. 前文也说到, 表达式这种简单场景完全可以不构建抽象语法树, 但本文还是”费劲”构建并用Visitor模式进行遍历, 目的是理解语法分析到抽象语法树构建和遍历的整个流程. 理解了本文所描述的内容之后, 再去看通用编程语言的编译前端或SQL解析就会变得一目了然了, 因为框架原理都是一样的, 无非是要在语法文件中逐步添加更多的规则, 增加更多类型的抽象语法树节点, 实现更多类型的Visitor类以支持不同类型的语义分析. 参考[1] JavaCC Tutorials[2] 自制编译器","link":"/javacc-expression-ast.html"},{"title":"Apche Calcite查询优化概述","text":"本文已收录在合集Apche Calcite原理与实践中. 本文是Apache Calcite原理与实践系列的第五篇. 经过前面几篇文章的铺垫, 本文终于开始进入Calcite中最为核心的查询优化器的介绍. 由于查询优化器所涉及的概念多且实现逻辑复杂, 后续将分几篇文章进行介绍. 本文首先介绍与查询优化相关的理论基础, 之后介绍Calcite中与查询优化相关的概念和数据结构. 后面的两篇文章将具体介绍Calcite中的两个优化器, HepPlanner和VolcanoPlanner的实现细节. 需要说明的是, 查询优化器的研究可以追溯到20世纪70年代, 有漫长的研究历史. 其本身也是一个十分复杂的问题, 本文所描述的基础理论意在后续更方便地讲解Calcite中的相关实现, 更为全面和准确的查询优化理论请参考相关论文. 查询优化基础查询优化的目的当前几乎所有的数据管理系统(包括传统的数据库管理系统, 以及大数据计算引擎, 如Hive, Spark, Flink等)都支持SQL访问. 而SQL是一种高级的声明式语言, 相比于命令式语言, SQL仅描述了需要的结果. 具体的执行步骤则交由数据系统决定. 尽管SQL在使用上十分方便, 然而使用SQL描述的查询或计算逻辑, 通常会有两个问题. 一是在逻辑上不是最优的, 比如不小心写入了恒等的过滤条件; 其次在物理上无法考虑底层数据存储的分布. 因此支持SQL的系统需要将原始的SQL经过解析与查询优化之后, 生成最优的执行计划并执行. 在OLTP中, 上线的SQL通常经过DBA的严格审核, 充分考虑了数据库内部的细节, 因此对查询优化器的挑战并不大. 然而在大数据时代, OLAP和大数据计算引擎大多直接面向用户, 且查询复杂度极高, 此时对查询优化器的考验就比较大了. 当前SQL已经成为了几乎所有数据管理系统的标准访问接口, 甚至连NoSQL都开始支持SQL访问, 在这背后查询优化器是不可缺少的技术. 查询优化的历史从E. F. Codd在1970年发表关系模型的论文之后, 就有了大量关于查询优化器的研究. 下图列举了几个具有重要意义的查询优化器研究或系统. Ingres是最早的基于关系代数的数据库管理系统, 它的查询优化器以启发式的规则优化为主. System R是IBM的关系数据库研究项目, 其关于查询优化的论文提出了基于成本的优化, 以及Bottom-up的动态规划搜索算法，如今许多成熟的OLTP系统仍采用类似的优化方式, 如MySQL. Starburst主要面向逻辑优化, 提出了实现一套可扩展的规则引擎来更好的实现逻辑关系代数转换. Volcano/Cascades则是查询优化领域具有重要里程碑意义的研究, 它设计了一整套查询优化器所需的组件, 包括逻辑和物理算子, 用于关系算子转换的规则集, 物理属性, 成本模型等, 以及Top-Down风格的搜索策略. Columbia是一篇硕士论文, 注重于Volcano/Cascades风格优化器的高效实现. 目前成熟的开源查询优化器主要有Apache Calcite和GPORCA, 它们都是Volcano/Cascades风格的优化器实现, 且均可作为独立模块存在, 服务于不同的数据管理系统. 其中GPORCA采用了多线程优化搜索, 更适用于OLAP类需要低延时的系统, 例如阿里云的Hologres就使用了GPORCA作为优化器. 查询优化的原理从用户的角度来看, 查询优化的结果就是将输入的SQL语句转化为高效的执行计划. 而从数据管理系统的角度来看, 查询优化由外到里可分为两个层次: 语义级优化: 基于SQL语法进行优化. 代数级优化: 根据关系代数的原理进行优化. 对于一个成熟的数据库管理系统或大数据计算引擎而言, 在上面所说的两个层面一般都会根据执行引擎自身的特点有相应的优化. 其中发挥重要作用的是代数级的优化, 又包括基于规则的优化(或称逻辑优化)和基于成本的优化(或称物理优化). 本文所描述的查询优化是代数级的优化, 后文所说的查询优化如果不作特殊说明均指在关系代数基础上进行的查询优化. 术语为了更方便地进行讲述, 这里规定一些术语, 在本文及之后的文章中常会用到. 术语 含义 逻辑算子(Logical Operator) 逻辑算子是高级算子, 仅用于指定数据转换而不指定物理算法, 例如Calcite中的LogicalJoin 物理算子(Physical Operator) 物理算子代表了特定的物理实现算法, 如Calcite中的EnumerableHashJoin 查询树/查询计划(Query Tree/ Query Plan) 由逻辑算子组成的树状结构, 由SQL转换而来, 是查询优化器的输入. 在Calcite中等价于逻辑的RelNode树 执行计划(Execution Plan) 有物理算子组成的树状结构, 明确指示了如何执行查询, 是查询优化器的输出. 在Calcite中等价于物理的RelNode树 表达式(Expression) 包含一个算子以及零个或多个输入, 既可以是逻辑的也可以是物理的. 查询计划是逻辑表达式, 执行计划是物理表达式. 基础算法基于关系代数的查询优化背后的数学原理是关系代数的等价变换, 其本质是将原始的查询计划, 通过等价变换, 转换为更低成本的执行计划. 这个变换并寻找更低成本执行计划的过程有两种算法, 一种是纯粹基于预定义规则的方法, 另一种是在规则的基础上引入成本计算, 从而可以进一步扩大搜索空间, 寻找更优的执行计划. 基于规则的优化(Rule-Based Optimization, RBO)基于规则的优化根据指定的优化规则对关系表达式进行转换, 从而得到一种执行成本更低的查询计划. 规则的定义需要Pattern和Substitution, 其中Pattern表示规则可匹配的查询子树, 而Substitution表示与Pattern等价的关系表达式. 可供优化规则有很多, 这里仍以上一篇文章中的SQL语句为例, 列举几个常见的优化规则. 首先是投影下推(或称列裁剪), 观察下图中的原始查询计划, 可以发现整个查询树中只用到了user表的id和name列, 以及order表的user_id和price列. 我们可以将这个信息下推到Scan算子, 这样只需读取所需的4列数据即可, 如果这两个表有很多其他列就可以节省大量IO. 其次是谓词下推, 可以发现Filter算子只对order表的price列进行了过滤, 因此可以把Filter算子下推到Join算子之下, 这样可以在Join之前对order表中的数据进行过滤, 减少Join表的数据量, 提升执行效率. 基于成本的优化(Cost-Based Optimization, CBO)基于规则的优化可应用一些一定有正向收益的规则, 比如上文所列举的投影下推, 谓词下推等. 但是基于规则的优化无法感知数据分布, 对一些问题就束手无策, 比如典型的Join重排问题. 而基于成本的优化可通过应用更多的规则变换来生成更多的备选计划, 从中根据执行成本挑选最优的查询计划. 需要注意的是, 尽管被称为基于成本的优化, 它仍然要结合规则生成多个等价的查询计划, 然后结合每个查询计划的执行成本, 挑选最优的计划. 也就是说, 无论是基于规则的优化还是基于成本的优化, 规则都是其中的核心之一, 不同之处在于: 在基于规则的优化中, 只能应用少量有明显正向收益的规则; 而在基于成本的优化中, 可以应用更多的规则, 产生更多的查询计划, 从中结合成本挑选最优的计划. 基于成本的优化有两个核心依赖, 即: 统计信息, 例如表的行数, 列索引上唯一键的数量等. 成本模型, 在单机环境下一般考虑IO, CPU, 内存成本; 在分布式环境下还需考虑网络成本. 搜索空间由上文的描述可知, 原始的查询计划, 通过规则变换可以生成一系列等价的查询和执行计划. 我们把所有可能生成的等价查询和执行计划称为搜索空间, 查询优化的目标就是在搜索空间中找到成本最低的执行计划. 查询优化的搜索空间会随着关系数量的增长而迅速增长, 对于3个关系的连接, 在仅考虑逻辑算子的情况下, 也有12种等价的查询计划, 如果考虑连接的物理算法, 搜索空间将会进一步扩大. 在上图中, 第一层的6棵连接树一般称为左深树, 第二层的6棵连接树一般称为右深树. 当关系数量扩展到4时还会出现稠密树, 如下图所示. 下表展示了N个关系时的逻辑搜索空间, 可以看到在仅考虑左深树的情况下, 搜索空间都呈阶乘级增长, 考虑稠密树的情况下其增长更快. 这还仅仅是考虑逻辑算子的情况, 如果考虑算子的物理实现, 搜索空间还会进一步增长. 由此可见, 查询优化的复杂度非常高, 它也被证明是NP-Hard的问题. 关系数 逻辑表达式个数 左深树个数 稠密树个数 N $3^N-(2^{n+1}-1)+N$ $N!$ $\\frac{(2N-2)!}{(N-1)!}$ 1 1 1 1 2 4 2 2 3 15 6 12 4 54 24 120 5 185 120 1680 6 608 720 30240 7 1939 5040 665280 8 6058 40320 17297280 9 18669 362880 518918400 10 57012 3628800 17643225600 搜索策略由上文我们已经知道, 查询优化是一个非常复杂的的问题, 其搜索空间巨大. 这类问题一般采用动态规划的解法, 根据请求发起的位置, 可分为Bottom-up(即从叶节点开始优化)和Top-Down(即从根节点开始优化)两种风格的优化算法. System R引入了Bottom-up风格的动态规划优化算法, 并且为了减少搜索空间引入了一些启发策略, 如只考虑左深树, 将笛卡尔积连接放到最后优化等. System R风格的优化算法对连接数较少的查询十分高效, 在OLTP中得到了广泛应用, MySQL的查询优化器整体上仍沿用这一框架. 然而对于更复杂的查询, Bottom-up的搜索算法无法进行高效的剪枝, 为此Volcano/Cascades提出了Top-Down风格的优化算法, 可以进行更高效的剪枝, 能搜索的空间也更大. 越来越多的数据库系统采用这一框架进行查询优化, 如TiDB, Greenplum等. 查询优化器查询优化的研究经历了漫长的过程, System R最先引入了基于成本的动态规划搜索算法, 极大地推动了查询优化的研究. 但是查询优化器的实现仍非常复杂, 直到Volcano/Cascades的出现, 不仅提出了一套新的搜索算法, 还提供了一整套可扩展的查询优化器实现, 基于这个框架, 如果要对查询优化器进行扩展, 只需提供额外的算子和对应的优化规则即可, 无需再关注规则的应用顺序以及优化算法的执行流程. Calcite查询优化器实现Calcite提供了完整的查询优化器实现, 其中包括完全基于规则的优化器HepPlanner, 以及基于成本的优化器VolcanoPlanner, 在实现上它们都继承于RelOptPlanner. 其中VolcanoPlanner经过多次迭代, 从1.24版本开始已经支持Volcano/Cascades风格的Top-Down搜索算法. 查询优化器相关概念和数据结构Calcite中, 与查询优化器相关的实现引入了大量的概念和数据结构, 理解这些概念和数据结构是理解优化器执行流程的关键. 本文先介绍HepPlanner与VolcanoPlanner中通用的概念和数据结构, 后续在详细介绍各个优化器的实现时, 还会介绍其独有的概念和数据结构. RelOptRule通过前文的介绍我们知道, 无论是RBO还是CBO, 实际都是由规则驱动的. 在Calcite中规则由RelOptRule表示, 它是目前所有规则的顶层基类, 不过最终它将被RelRule取代, 新实现的规则建议都继承RelRule. Calcite中已经实现了上百种优化规则, 从优化内容来看可以分为以下两种类型: TransformationRule: 用于将一种逻辑表达式转换为另一种等价的逻辑表达式. 对应Cascades中的Transformation Rule, 是一个独立的接口, 并不继承于RelOptRule. 需要注意的是TransformationRule接口仅在VolcanoPlanner中有效, HepPlanner会直接忽略这一接口. ConverterRule: 用于将一种Calling Convention的表达式转换为另一种Calling Convention的表达式, 可用于将逻辑表达式转换为物理表达式. 对应Cascades中的Implementation Rule, ConverterRule继承于RelOptRule. Calcite中还有一个用于标识规则种类的接口SubstitutionRule, 它继承于TransformationRule, 在VolcanoPlaner中使用, 用于标识一定有正收益的规则, 后续会进一步介绍. 定义一个规则至少需要两部分信息, 即Pattern和Sustitution, 在Calcite中: Pattern由RelOptRuleOperand实现, 用于表示该规则可匹配的表达式结构. Substitution表示该规则可产生的逻辑等价表达式, 在函数onMatch()中实现. TransformationRule仅涉及到逻辑算子的转换, FilterMergeRule是一个简单的TransformationRule案例, 它将查询树中的上下两个Filter算子合并为一个. 规则触发的逻辑实现在onMatch()方法中, 在Config接口中通过RelOptRuleOperand定义了规则的Pattern.1234567891011121314151617181920212223242526272829303132333435363738/** Planner rule that combines two LogicalFilters */@Value.Enclosingpublic class FilterMergeRule extends RelRule&lt;FilterMergeRule.Config&gt; implements SubstitutionRule { /** Creates a FilterMergeRule. */ protected FilterMergeRule(Config config) { super(config); } @Override public void onMatch(RelOptRuleCall call) { final Filter topFilter = call.rel(0); final Filter bottomFilter = call.rel(1); final RelBuilder relBuilder = call.builder(); relBuilder.push(bottomFilter.getInput()) .filter(bottomFilter.getCondition(), topFilter.getCondition()); call.transformTo(relBuilder.build()); } /** Rule configuration. */ @Value.Immutable public interface Config extends RelRule.Config { Config DEFAULT = ImmutableFilterMergeRule.Config.of().withOperandFor(Filter.class); @Override default FilterMergeRule toRule() { return new FilterMergeRule(this); } /** Defines an operand tree for the given classes. */ default Config withOperandFor(Class&lt;? extends Filter&gt; filterClass) { return withOperandSupplier(b0 -&gt; b0.operand(filterClass).oneInput(b1 -&gt; // 匹配Filter类型且仅有一个子节点的 b1.operand(filterClass).anyInputs())) // 其子节点也必须为Filter类型, 支持任何输入 .as(Config.class); } }} ConverterRule可用于在逻辑算子和物理算子之间进行转换, 基类的onMatch()方法会调用子类的convert()方法实现转换.123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Abstract base class for a rule which converts from one calling convention to * another without changing semantics. */@Value.Enclosingpublic abstract class ConverterRule extends RelRule&lt;ConverterRule.Config&gt; { @Override public void onMatch(RelOptRuleCall call) { RelNode rel = call.rel(0); if (rel.getTraitSet().contains(inTrait)) { final RelNode converted = convert(rel); if (converted != null) { call.transformTo(converted); } } }}/** * Rule to convert a {@link LogicalFilter} to an {@link EnumerableFilter}. * You may provide a custom config to convert other nodes that extend {@link Filter}. * * @see EnumerableRules#ENUMERABLE_FILTER_RULE */class EnumerableFilterRule extends ConverterRule { /** Default configuration. */ public static final Config DEFAULT_CONFIG = Config.INSTANCE .withConversion(LogicalFilter.class, f -&gt; !f.containsOver(), Convention.NONE, EnumerableConvention.INSTANCE, &quot;EnumerableFilterRule&quot;) .withRuleFactory(EnumerableFilterRule::new); protected EnumerableFilterRule(Config config) { super(config); } @Override public RelNode convert(RelNode rel) { final Filter filter = (Filter) rel; return new EnumerableFilter(rel.getCluster(), rel.getTraitSet().replace(EnumerableConvention.INSTANCE), convert(filter.getInput(), filter.getInput().getTraitSet() .replace(EnumerableConvention.INSTANCE)), filter.getCondition()); }} RelOptRuleCall在Calcite中, RelOptRuleCall代表一个规则的触发, 其实现关系如下图所示. 其中HepRuleCall用于HepPlanner, VolcanoRuleCall用于VolcanoPlanner. VolcanoRuleCall进一步有两个实现类, 在VolcanoPlanner的初始及优化过程中, 如果发现查询子树与某个规则匹配, 则会创建一个DeferringRuleCall, 在它的onMatch()方法中会创建一个VolcanoRuleMatch并放入队列中等待优化. RelOptRuleCall.rels保存了与规则匹配的RelNode节点, 例如对于上文中的FilterMergeRule, 在其onMatch()方法中传入的RelOptRuleCall是一个长度为2的数组, 第一个元素是一个Filter节点, 第二个元素是第一个元素的子节点, 且仍为Filter节点.12345public abstract class RelOptRuleCall { ... public final RelNode[] rels; ...} 总结本文首先介绍了与SQL查询优化相关的基础理论, 随后介绍了Calcite中的查询优化器及不同优化器间通用的概念和数据结构. 本文并不涉及任何实现细节, 核心是想厘清查询优化要解决的问题, 只有理解了这一点才能掌握查询优化器的本质. 后续的文章将在本文的基础上, 深入查询优化器的实现细节. 参考[1] Access path selectionin arelational database management system[2] Thevolcano optimizer generator: Extensibilityandefficient search[3] The cascades framework for query optimization[4] Efficiency in the Columbia Database Query Optimizer[5] Orca: A Modular Query Optimizer Architecture for Big Data[6] Introduction to the Join Ordering Problem[7] Rule-based Query Optimization[8] What is Cost-based Optimization?[9] 数据库挖矿系列-优化器设计探索穿越之旅[10] 揭秘TiDB新优化器: Cascades Planner原理解析","link":"/apache-calcite-query-optimization-overview.html"},{"title":"Apache Calcite整体架构及处理流程","text":"本文已收录在合集Apche Calcite原理与实践中. Apache Calcite是一个动态的数据管理框架, 它可以实现SQL的解析, 验证, 优化和执行. 称之为”动态”是因为Calcite是模块化和插件式的, 上述任何一个步骤在Calcite中都对应着一个相对独立的模块. 用户可以选择使用其中的一个或多个模块, 也可以对任意模块进行定制化的扩展. 正是这种灵活性使得Calcite可以在现有的存储或计算系统上方便地构建SQL访问层, 甚至在已有SQL能力的系统中也可引入Calcite中的某个模块实现相应的功能, 比如Apche Hive就仅使用了Calcite进行优化, 但却有自己的SQL解析器. Calcite的这种特性使其在大数据系统中得到了广泛的运用, 比如Apache Flink, Apache Drill等都大量使用了Calcite, 因此理解Calcite的原理已经成为理解大数据系统中SQL访问层实现原理的必备条件. 笔者在学习Calcite的过程中发现关于Calcite的实践案例十分稀缺, Calcite文档中对于原理和使用方法的介绍也比较笼统, 因此准备对Calcite的相关内容进行总结整理, 由于整体内容较多, 后续计划每个模块安排一到两篇文章进行详细介绍. 本文是这一系列的第一篇, 重点介绍Calcite的架构, 并用一个可运行的例子来一步步分析Calcite在SQL解析, 验证, 优化和执行各个阶段所做的工作和输出的结果, 以形成对Calcite的整体了解. 关于Calcite的历史背景, 可以阅读参考[1], 本文不再赘述. Calcite整体架构Calcite的整体架构如下图(图片来自Calcite论文)所示, 它包含以下组成部分: JDBC接口: 用于使用标准的JDBC接口访问Calcite获取数据, 为了提供JDBC/ODBC接口, Calcite构建了一个独立的Avatica框架. SQL Parser和SQL Validator: 用于进行SQL的解析和验证, 将原始的SQL字符串解析并转化为内部的SqlNode树表示. Query Optimizer: 用于进行查询优化, 查询优化是在关系代数的基础上进行的, 在Calcite内部有一种关系代数表示方法, 即将关系代数表示为RelNode树. RelNode树既可由SqlNode树转化而来, 也可通过Calcite提供的Expressions Builder接口构建. Enumerator执行计划: Calcite提供了一种将优化后的RelNode树生成为Enumerator执行计划的方法, Enumerator执行计划基于Linq4j实现, 这部分并未在图中画出. 由于多数系统有自己的执行接口, 因此Calcite的这部分组件在成熟的系统中较少使用. Calcite的一些Adapter使用了Enumerator执行计划. 由上述架构可以看出, Calcite包含许多组成典型数据库管理系统的部件. 不过, 它省略了一些关键的组成部分, 例如, 数据的存储, 处理数据的算法和存储元数据的存储库. 这些省略是有意为之的, 因为在大数据时代, 对不同的数据类型有不同的存储和计算引擎, 想要将它们统一到一个框架中是不太可能的. Calcite的目的是仅提供构建SQL访问的框架, 这也是其广泛适用的原因. 这种省略带来的另一个好处是, 使用Calcite可以十分方便地构建联邦查询引擎, 即屏蔽底层物理存储和计算引擎, 使用一个统一的SQL接口实现数据访问. Calcite处理流程Calcite的完整处理流程实际上就是SQL的解析, 优化与执行流程, 具体步骤如下图所示. 从图中可以看出, Calcite的处理流程主要分为5个阶段: Parser用于解析SQL, 将输入的SQL字符串转化为抽象语法树(AST), Calcite中用SqlNode树表示. Validator根据元数据信息对SqlNode树进行验证, 其输出仍是SqlNode树. Converter将SqlNode树转化为关系代数, 以方便进一步优化, Calcite中使用RelNode树表示关系代数. Optimizer对输入的关系代数进行优化, 输出优化后的RelNode树. Execute阶段会根据优化后的RelNode生成执行计划, 在Calcite中内置了一种基于Enumerator的执行计划生成方法. 为了更加深入地理解上述步骤, 笔者设计了一个小而全的案例, 通过Calcite实现使用SQL访问CSV文件. 后文我们将依据Calcite处理的5个阶段, 逐步分析每个阶段所做的工作和输出结果. 为方便阅读本文只给出了核心代码, 完整的可执行代码可参考这里. 如果读者阅读过Calcite的文档或源代码, 可能会发现Calcite已经包含了一个CSV Adapter, 可以实现CSV文件的访问. 不过Adapter需要借助JDBC接口使用, 而JDBC的调用链十分复杂, 不利于我们理解Calcite的核心处理步骤. 因此这里绕过了JDBC, 使用了一个更简易的案例. 通过本案例也可帮助理解Calcite Adapter的实现原理. 案例数据及SQL在本文的案例中, 我们使用了两张表. 其中users表存储用户数据, orders表存储订单数据. users表的内容如下.12345id:string,name:string,age:int1,Jack,282,John,213,Tom,324,Peter,24 orders表的内容如下.123456id:string,user_id:string,goods:string,price:double001,1,Cola,3.5002,1,Hat,38.9003,2,Shoes,199.9004,3,Book,39.9005,4,Phone,2499.9 后文将基于上述两张表, 逐步分析以下SQL语句的解析和执行流程, 这一SQL语句用于查询每个用户的订单消费总额, 并按用户id排序后输出.12345SELECT u.id, name, age, sum(price)FROM users AS u join orders AS o ON u.id = o.user_idWHERE age &gt;= 20 AND age &lt;= 30GROUP BY u.id, name, ageORDER BY u.id SQL解析SQL语句处理的第一步便是通过词法分析和语法分析将SQL字符串转化为AST. 在Calcite中, 借助JavaCC实现了SQL的解析, 并转化为SqlNode表示. JavaCC是一种解析器生成器工具, 可以根据用户提供的语法规则文件自动生成解析器, 如果对如何使用JavaCC生成抽象语法树感兴趣可阅读笔者之前的博文编译原理实践 - JavaCC解析表达式并生成抽象语法树. 在Calcite中, SqlNode是AST节点的抽象基类, 不同类型的节点有对应的实现类. 比如上述SQL语句便会生成SqlSelect和SqlOrderBy两个主要的节点. 在Calcite中, 我们可以简单地使用如下代码将SQL字符串转化为SqlNode.123456789String sql = &quot;SELECT u.id, name, age, sum(price) &quot; + &quot;FROM users AS u join orders AS o ON u.id = o.user_id &quot; + &quot;WHERE age &gt;= 20 AND age &lt;= 30 &quot; + &quot;GROUP BY u.id, name, age &quot; + &quot;ORDER BY u.id&quot;;// 创建SqlParser, 用于解析SQL字符串SqlParser parser = SqlParser.create(sql, SqlParser.Config.DEFAULT);// 解析SQL字符串, 生成SqlNode树SqlNode sqlNode = parser.parseStmt(); 上述代码中的sqlNode是AST的根节点, 下图是将其展开后的结果. 可以看到sqlNode其实是SqlOrderBy类型, 它的query字段是一个SqlSelect类型, 即代表原始的SQL语句去掉ORDER BY部分. 图中红色矩形框内的其实都是SqlNode类型. SQL验证SQL解析阶段只是简单地将SQL字符串转化为SqlNode树, 并没有对SQL语句进行语义上的检查, 比如SQL中指定的表是否存在于数据库中, 字段是否存在于表中等. Calcite中的SQL验证阶段一方面会借助元数据信息执行上述验证, 另一方面会对SqlNode树进行一些改写, 以转化为统一的格式, 方便下一步处理. 在Calcite中可通过以下代码进行SQL验证.1234567891011121314151617181920212223242526272829303132333435363738394041// 创建Schema, 一个Schema中包含多个表. Calcite中的Schema类似于RDBMS中的DatabaseSimpleTable userTable = SimpleTable.newBuilder(&quot;users&quot;) .addField(&quot;id&quot;, SqlTypeName.VARCHAR) .addField(&quot;name&quot;, SqlTypeName.VARCHAR) .addField(&quot;age&quot;, SqlTypeName.INTEGER) .withFilePath(&quot;/path/to/user.csv&quot;) .withRowCount(10) .build();SimpleTable orderTable = SimpleTable.newBuilder(&quot;orders&quot;) .addField(&quot;id&quot;, SqlTypeName.VARCHAR) .addField(&quot;user_id&quot;, SqlTypeName.VARCHAR) .addField(&quot;goods&quot;, SqlTypeName.VARCHAR) .addField(&quot;price&quot;, SqlTypeName.DECIMAL) .withFilePath(&quot;/path/to/order.csv&quot;) .withRowCount(10) .build();SimpleSchema schema = SimpleSchema.newBuilder(&quot;s&quot;) .addTable(userTable) .addTable(orderTable) .build(); CalciteSchema rootSchema = CalciteSchema.createRootSchema(false, false);rootSchema.add(schema.getSchemaName(), schema);RelDataTypeFactory typeFactory = new JavaTypeFactoryImpl();// 创建CatalogReader, 用于指示如何读取Schema信息Prepare.CatalogReader catalogReader = new CalciteCatalogReader( rootSchema, Collections.singletonList(schema.getSchemaName()), typeFactory, config);// 创建SqlValidator, 用于执行SQL验证SqlValidator.Config validatorConfig = SqlValidator.Config.DEFAULT .withLenientOperatorLookup(config.lenientOperatorLookup()) .withSqlConformance(config.conformance()) .withDefaultNullCollation(config.defaultNullCollation()) .withIdentifierExpansion(true);SqlValidator validator = SqlValidatorUtil.newValidator( SqlStdOperatorTable.instance(), catalogReader, typeFactory, validatorConfig);// 执行SQL验证SqlNode validateSqlNode = validator.validate(node); 从上述代码中可以看到, SQL验证后的输出结果仍是SqlNode树, 不过其内部结构发生了改变. 一个明显的变化是验证后的SqlOrderBy节点被改写为了SqlSelect节点, 并在其orderBy变量中记录了排序字段. 另外, 如果把验证前后的SqlNode完全打印出来, 我们可以发现Calcite在验证时会为每个字段加上表名限定, 为每个表加上Schema限定. 读者可以试着故意把表名或者字段写错, 运行时在这一阶段就会报错.123456789101112131415-- 验证前的SqlNode树打印结果SELECT `u`.`id`, `name`, `age`, SUM(`price`)FROM `users` AS `u`INNER JOIN `orders` AS `o` ON `u`.`id` = `o`.`user_id`WHERE `age` &gt;= 20 AND `age` &lt;= 30GROUP BY `u`.`id`, `name`, `age`ORDER BY `u`.`id`-- 验证后的SqlNode树打印结果SELECT `u`.`id`, `u`.`name`, `u`.`age`, SUM(`o`.`price`)FROM `s`.`users` AS `u`INNER JOIN `s`.`orders` AS `o` ON `u`.`id` = `o`.`user_id`WHERE `u`.`age` &gt;= 20 AND `u`.`age` &lt;= 30GROUP BY `u`.`id`, `u`.`name`, `u`.`age`ORDER BY `u`.`id` 转化为关系代数关系代数是SQL背后的理论基础, 如果读者不了解关系代数可阅读Introduction of Relational Algebra in DBMS作简单了解, “数据库系统概念“中对关系代数有更深入的介绍. 在Calcite中, 关系代数由RelNode表示, 我们可以通过以下代码, 将验证后的SqlNode树转化为RelNode树. 可以看到创建SqlToRelConverter的代码其实设计的并不十分优雅, VolcanoPlanner其实是在优化阶段使用的, 但是在转化为关系代数的时候就必须创建一个Planner, 这也是Calcite在抽象上有待提升的地方.1234567891011121314151617// 创建VolcanoPlanner, VolcanoPlanner在后面的优化中还需要用到VolcanoPlanner planner = new VolcanoPlanner(RelOptCostImpl.FACTORY, Contexts.of(config));planner.addRelTraitDef(ConventionTraitDef.INSTANCE);// 创建SqlToRelConverterRelOptCluster cluster = RelOptCluster.create(planner, new RexBuilder(typeFactory));SqlToRelConverter.Config converterConfig = SqlToRelConverter.config() .withTrimUnusedFields(true) .withExpand(false);SqlToRelConverter converter = new SqlToRelConverter( null, validator, catalogReader, cluster, StandardConvertletTable.INSTANCE, converterConfig);// 将SqlNode树转化为RelNode树RelNode relNode = converter.convertQuery(validateSqlNode, false, true); RelNode树其实可以理解为一个逻辑执行计划, 上述SQL对应的逻辑执行计划如下, 其中每一行都表示一个节点, 是RelNode的实现类, 缩进表示父子关系.1234567LogicalSort(sort0=[$0], dir0=[ASC]) LogicalAggregate(group=[{0, 1, 2}], EXPR$3=[SUM($3)]) LogicalProject(id=[$0], name=[$1], age=[$2], price=[$6]) LogicalFilter(condition=[AND(&gt;=($2, 20), &lt;=($2, 30))]) LogicalJoin(condition=[=($0, $4)], joinType=[inner]) LogicalTableScan(table=[[s, users]]) LogicalTableScan(table=[[s, orders]]) 查询优化查询优化是Calcite的核心模块, 它主要有三部分组成: Planner rules: 即优化规则, Calcite已经内置了很多优化规则, 如谓词下推, 投影下推等. 用户也可定义自己的优化规则. Metadata providers: 这里的元数据主要用于基于成本的优化(Cost-based Optimize, CBO)中, 包括表的行数, 表的大小, 给定列的值是否唯一等信息. Planner engines: Calcite提供了两种优化器实现, HepPlanner用于实现基于规则的优化(Rule-based Optimize, RBO), VolcanoPlanner用于实现基于成本的优化. 本文中暂且只使用VolcanoPlanner, 执行优化的代码如下. EnumerableRules中的规则用于将逻辑计划中的节点转化为对应的EnumerableRel节点, 它是Calcite中提供的物理节点, 可用于生成执行代码.123456789101112131415161718192021// 优化规则RuleSet rules = RuleSets.ofList( CoreRules.FILTER_TO_CALC, CoreRules.PROJECT_TO_CALC, CoreRules.FILTER_CALC_MERGE, CoreRules.PROJECT_CALC_MERGE, CoreRules.FILTER_INTO_JOIN, // 过滤谓词下推到Join之前 EnumerableRules.ENUMERABLE_TABLE_SCAN_RULE, EnumerableRules.ENUMERABLE_PROJECT_TO_CALC_RULE, EnumerableRules.ENUMERABLE_FILTER_TO_CALC_RULE, EnumerableRules.ENUMERABLE_JOIN_RULE, EnumerableRules.ENUMERABLE_SORT_RULE, EnumerableRules.ENUMERABLE_CALC_RULE, EnumerableRules.ENUMERABLE_AGGREGATE_RULE);Program program = Programs.of(RuleSets.ofList(rules));RelNode optimizerRelTree = program.run( planner, relNode, relNode.getTraitSet().plus(EnumerableConvention.INSTANCE), Collections.emptyList(), Collections.emptyList()); 经过优化后的输出如下, 可以看到所有的节点都变成了Enumerable开头的物理节点, 它们的基类是EnumerableRel.1234567EnumerableSort(sort0=[$0], dir0=[ASC]) EnumerableAggregate(group=[{0, 1, 2}], EXPR$3=[SUM($3)]) EnumerableCalc(expr#0..6=[{inputs}], proj#0..2=[{exprs}], price=[$t6]) EnumerableHashJoin(condition=[=($0, $4)], joinType=[inner]) EnumerableCalc(expr#0..2=[{inputs}], expr#3=[Sarg[[20..30]]], expr#4=[SEARCH($t2, $t3)], proj#0..2=[{exprs}], $condition=[$t4]) EnumerableTableScan(table=[[s, users]]) EnumerableTableScan(table=[[s, orders]]) 对比优化前后的计划, 另一个值得注意的变化是对users表的过滤位置发生了变动, 从先Join后过滤变成了先过滤后Join, 如下图所示. 生成执行计划一般的存储或计算系统都有自己的执行计划, 因此为了将物理计划转化为执行计划通常需要用户编写代码. 不过Calcite中也提供了一种执行计划生成方法, 为了完整性我们这里使用它来生成访问CSV文件的执行计划. 通过如下代码即可生成执行计划并读取CSV文件中的数据.1234567891011121314151617EnumerableRel enumerable = (EnumerableRel) optimizerRelTree;Map&lt;String, Object&gt; internalParameters = new LinkedHashMap&lt;&gt;();EnumerableRel.Prefer prefer = EnumerableRel.Prefer.ARRAY;Bindable bindable = EnumerableInterpretable.toBindable(internalParameters, null, enumerable, prefer);Enumerable bind = bindable.bind(new SimpleDataContext(rootSchema.plus()));Enumerator enumerator = bind.enumerator();while (enumerator.moveNext()) { Object current = enumerator.current(); Object[] values = (Object[]) current; StringBuilder sb = new StringBuilder(); for (Object v : values) { sb.append(v).append(&quot;,&quot;); } sb.setLength(sb.length() - 1); System.out.println(sb);} 执行上述代码后可以看到如下结果.1231,Jack,28,42.402,John,21,199.904,Peter,24,2499.90 至此, 我们成功地借助Calcite实现了使用SQL查询CSV文件中的数据, 其核心代码也就200行左右. 总结本文通过一个小而全的案例囫囵地过了一遍Calcite的整个处理流程, 希望能借此构建对Calcite的整体理解. 具体到各个模块, Calcite其实还有很多细节和概念, 笔者计划在后续的文章中再深入讲解. 通过本文的案例也可以看到, 借助Calcite可以十分方便地构建SQL访问层. 不过Calcite最强大的还是在于其可扩展性, 本文所述的每个步骤都可以进行自定义的扩展, 这也使其在业界得到了广泛的运用, 比如Apache Flink的SQL API就重度依赖Calcite, 相信未来更多的大数据系统在构建SQL访问层时都会优先考虑Calcite. 参考[1] Apache Calcite: Hadoop中新型大数据查询引擎[2] SQL over anything with an Optiq Adapter[3] Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources[4] Apache Calcite 处理流程详解(一)","link":"/apache-calcite-overview.html"},{"title":"Apache Calcite查询优化器之HepPlanner","text":"本文已收录在合集Apche Calcite原理与实践中. 本文是Apache Calcite原理与实践系列的第六篇. 上一篇文章介绍了与查询优化器相关的基本理论, 本文开始介绍Calcite中的查询优化器HepPlanner的实现, HepPlanner是基于规则的优化器, 相对于VolcanoPlanner来说实现比较简单. 本文首先介绍HepPlanner中引入的相关概念和数据结构, 之后介绍HepPlanner的整个优化流程. HepPlanner概念和数据结构HepRelVertex在HepPlanner内部, 会将待优化的RelNode算子树转化为一个有向无环图(DAG), 而HepRelVertex就是这个DAG的顶点. 其实现关系如下图所示. HepRelVertex就是对一个RelNode的包装, 其currentRel成员变量指向一个原始的RelNode. 待优化的RelNode树中的每个RelNode节点都会转化为一个HepRelVertex.1234public class HepRelVertex extends AbstractRelNode { private RelNode currentRel;} 将RelNode树转化为DAG的过程在HepPlanner的addRelToGraph()函数中, 其本质就是对RelNode树进行深度优先遍历, 笔者已经在其中加了详细的注释.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private HepRelVertex addRelToGraph(RelNode rel) { // Check if a transformation already produced a reference to an existing vertex. // 检查DAG顶点中是否已经包含了当前RelNode, 若包含证明该RelNode已经转换过, 直接返回即可 if (graph.vertexSet().contains(rel)) { return (HepRelVertex) rel; } // Recursively add children, replacing this rel&apos;s inputs // with corresponding child vertices. // 递归将当前RelNode的子节点加入到DAG中 final List&lt;RelNode&gt; inputs = rel.getInputs(); // 原来的子节点 final List&lt;RelNode&gt; newInputs = new ArrayList&lt;&gt;(); // 转化为HepRelVertex之后的子节点 for (RelNode input1 : inputs) { // 递归将子节点加入到DAG中(深度优先遍历) HepRelVertex childVertex = addRelToGraph(input1); newInputs.add(childVertex); } if (!Util.equalShallow(inputs, newInputs)) { RelNode oldRel = rel; rel = rel.copy(rel.getTraitSet(), newInputs); onCopy(oldRel, rel); } // Compute digest first time we add to DAG, // otherwise can&apos;t get equivVertex for common sub-expression // 计算RelNode的digest(唯一标识当前节点), 用于后续获取等价节点 rel.recomputeDigest(); // try to find equivalent rel only if DAG is allowed // 如果允许DAG, 尝试从已转化的顶点中获取等价HepRelVertex // 注意这里的noDag变量仅用于指示是否重用等价的表达式, // 与文中所说的由HepRelVertex组成的DAG无关 if (!noDag) { // Now, check if an equivalent vertex already exists in graph. HepRelVertex equivVertex = mapDigestToVertex.get(rel.getRelDigest()); if (equivVertex != null) { // Use existing vertex. // 如果已存在等价节点则直接返回 return equivVertex; } } // No equivalence: create a new vertex to represent this rel. // 创建新的顶点并添加到DAG中 HepRelVertex newVertex = new HepRelVertex(rel); graph.addVertex(newVertex); updateVertex(newVertex, rel); // 在当前节点和其子节点之间创建边 for (RelNode input : rel.getInputs()) { graph.addEdge(newVertex, (HepRelVertex) input); } nTransformations++; return newVertex;} HepProgramHepProgram是一个工具类, 包含了初始化HepPlanner所需的一些信息, 几个重要的成员变量如下: instructions包含了所有需要执行的优化规则; matchLimit指定最大匹配次数, 防止规则匹配的过程陷入无限循环. 默认值是MATCH_UNTIL_FIXPOINT = Integer.MAX_VALUE; matchOrder定义了规则匹配的顺序(即DAG的遍历顺序), HepPlanner支持4中匹配顺序: ARBITRARY: 按任意顺序匹配, 这是默认值, 实际在执行的时候用的是DEPTH_FIRST. BOTTOM_UP: 以拓扑排序的顺序进行遍历. TOP_DOWN：以拓扑排序的逆序进行遍历. DEPTH_FIRST: 深度优先匹配.12345678910public class HepProgram { // 需要执行的优化规则 final ImmutableList&lt;HepInstruction&gt; instructions; // 最大匹配次数 int matchLimit; // 遍历顺序 HepMatchOrder matchOrder; HepInstruction.EndGroup group;} HepInstructionHepInstruction是对HepPlanner所执行指令的统一封装, 包含多种类型的操作, 其实现类如下图所示. 具体的HepInstruction可分为以下几类: RuleClass和RelInstance用于执行一个优化规则, 不同的是RuleClass需要当前优化器中已经存在对应的优化规则, 即通过RelOptPlanner.addRule()添加过该规则, 而RelInstance不需要. RuleCollection用于执行一组优化规则, 它也不需要调用RelOptPlanner.addRule()添加规则. BeginGroup用于开始一个规则组, EndGroup用于结束一个规则组, 在BeginGroup和EndGroup之间的所有优化规则会整组一起执行. SubProgram包装了一个子HepProgram. MatchLimit用于指示最大匹配次数, 执行该指令会修改当前HepProgram的matchLimit值; MatchOrder用于指示匹配匹配顺序, 执行该指令会修改当前HepProgram的matchOrder. HepPlanner优化过程以下是一个使用HepPlanner的简单示例, 完整的代码在HepPlannerExample中.123456789101112HepProgramBuilder hepProgramBuilder = new HepProgramBuilder();// 添加优化规则hepProgramBuilder.addRuleInstance(CoreRules.FILTER_TO_CALC);hepProgramBuilder.addRuleInstance(EnumerableRules.ENUMERABLE_TABLE_SCAN_RULE);hepProgramBuilder.addRuleInstance(EnumerableRules.ENUMERABLE_CALC_RULE);// 1. 构建HepPlannerHepPlanner planner = new HepPlanner(hepProgramBuilder.build());// 2. 构建DAGplanner.setRoot(root);// 3. 执行优化RelNode optimizedRoot = planner.findBestExp(); 从上述代码中可以看到, 使用HepPlanner进行查询优化分为三步: 创建HepPlanner; 调用HepPlanner.setRoot()构建DAG; 调用HepPlanner.findBestExp()执行查询优化. 步骤一: 创建HepPlanner创建HepPlanner需要一个HepProgram, Calcite提供了HepProgramBuilder来创建HepProgram, 其方法都是用来创建HepInstruction, 每种HepInstruction实例都有一个对应的方法, 比如: addRuleClass()用于创建RuleClass; addRuleCollection()用于创建RuleCollection; addMatchOrder()用于创建MatchOrder. 步骤二: 构建DAG构建DAG的过程是通过调用HepPlanner.setRoot()触发的, 通过源代码也可以看到最终是调用了addRelToGraph(), 其具体流程已经在上文分析过, 不再赘述.1234public void setRoot(RelNode rel) { root = addRelToGraph(rel); dumpGraph();} 步骤三: 执行查询优化执行查询优化的过程是在HepPlanner.findBestExp()中触发的.12345678910public RelNode findBestExp() { assert root != null; executeProgram(mainProgram); // Get rid of everything except what&apos;s in the final plan. collectGarbage(); dumpRuleAttemptsInfo(); return buildFinalPlan(root);} 可以看到在findBestExp()中, 主要是调用了executeProgram()来执行优化, 其主要流程就是遍历当前HepProgram中的HepInstruction, 并逐个执行.123456789101112131415161718192021private void executeProgram(HepProgram program) { HepProgram savedProgram = currentProgram; currentProgram = program; currentProgram.initialize(program == mainProgram); // 遍历当前HepProgram中的HepInstruction, 逐个执行 for (HepInstruction instruction : currentProgram.instructions) { instruction.execute(this); int delta = nTransformations - nTransformationsLastGC; if (delta &gt; graphSizeLastGC) { // The number of transformations performed since the last // garbage collection is greater than the number of vertices in // the graph at that time. That means there should be a // reasonable amount of garbage to collect now. We do it this // way to amortize garbage collection cost over multiple // instructions, while keeping the highwater memory usage // proportional to the graph size. collectGarbage(); } } currentProgram = savedProgram;} 真正的优化规则是保存在HepInstruction中的, 这里我们以RuleInstance为例来说明HepPlanner是如何执行优化规则的. RuleInstance.execute()会调用HepPlanner.executeInstruction()方法.123456789101112void executeInstruction(HepInstruction.RuleInstance instruction) { if (skippingGroup()) { return; } if (instruction.rule == null) { assert instruction.ruleDescription != null; instruction.rule = getRuleByDescription(instruction.ruleDescription); } if (instruction.rule != null) { applyRules(Collections.singleton(instruction.rule), true); }} 对于需要执行优化规则的HepInstruction, 最终都会调用applyRules().12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758private void applyRules(Collection&lt;RelOptRule&gt; rules, boolean forceConversions) { if (currentProgram.group != null) { assert currentProgram.group.collecting; currentProgram.group.ruleSet.addAll(rules); return; } // 当遍历顺序是ARBITRARY或DEPTH_FIRST时为false // 此时不会每次匹配成功之后都从root顶点开始 boolean fullRestartAfterTransformation = currentProgram.matchOrder != HepMatchOrder.ARBITRARY &amp;&amp; currentProgram.matchOrder != HepMatchOrder.DEPTH_FIRST; int nMatches = 0; boolean fixedPoint; do { // 根据遍历顺序获取对应的迭代器 Iterator&lt;HepRelVertex&gt; iter = getGraphIterator(root); fixedPoint = true; while (iter.hasNext()) { // 遍历每个顶点 HepRelVertex vertex = iter.next(); for (RelOptRule rule : rules) { // 遍历每个RelOptRule // 对每个规则尝试进行匹配和转换 HepRelVertex newVertex = applyRule(rule, vertex, forceConversions); if (newVertex == null || newVertex == vertex) { continue; } ++nMatches; // 匹配次数超过最大次数, 则直接退出 if (nMatches &gt;= currentProgram.matchLimit) { return; } if (fullRestartAfterTransformation) { // 当不是DEPTH_FIRST时, 匹配成功后从root节点重新开始, // 因为根节点可能因为转换而改变 iter = getGraphIterator(root); } else { // To the extent possible, pick up where we left // off; have to create a new iterator because old // one was invalidated by transformation. // 如果是DEPTH_FIRST, 那么直接递归匹配新节点的子节点. // 因为转换后的子节点可能被重新匹配到现有规则. iter = getGraphIterator(newVertex); if (currentProgram.matchOrder == HepMatchOrder.DEPTH_FIRST) { nMatches = depthFirstApply(iter, rules, forceConversions, nMatches); if (nMatches &gt;= currentProgram.matchLimit) { return; } } // Remember to go around again since we&apos;re skipping some stuff. fixedPoint = false; } break; } } } while (!fixedPoint);} 在applyRules()会遍历DAG中的所有顶点, 并调用applyRule()依次匹配当前规则组中的所有规则. 在applyRule()中会具体判断当前节点子树的Pattern与规则所指定的Pattern是否一致, 如果一直则触发规则, 进行转换.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263private HepRelVertex applyRule(RelOptRule rule, HepRelVertex vertex, boolean forceConversions) { if (!graph.vertexSet().contains(vertex)) { return null; } RelTrait parentTrait = null; List&lt;RelNode&gt; parents = null; if (rule instanceof ConverterRule) { // Guaranteed converter rules require special casing to make sure // they only fire where actually needed, otherwise they tend to // fire to infinity and beyond. ConverterRule converterRule = (ConverterRule) rule; if (converterRule.isGuaranteed() || !forceConversions) { if (!doesConverterApply(converterRule, vertex)) { return null; } parentTrait = converterRule.getOutTrait(); } } else if (rule instanceof CommonRelSubExprRule) { // Only fire CommonRelSubExprRules if the vertex is a common // subexpression. List&lt;HepRelVertex&gt; parentVertices = getVertexParents(vertex); if (parentVertices.size() &lt; 2) { return null; } parents = new ArrayList&lt;&gt;(); for (HepRelVertex pVertex : parentVertices) { parents.add(pVertex.getCurrentRel()); } } final List&lt;RelNode&gt; bindings = new ArrayList&lt;&gt;(); final Map&lt;RelNode, List&lt;RelNode&gt;&gt; nodeChildren = new HashMap&lt;&gt;(); // 判断当前节点子树的Pattern与规则指定的Pattern是否一致 boolean match = matchOperands(rule.getOperand(), vertex.getCurrentRel(), bindings, nodeChildren); if (!match) { return null; } // 创建RelOptRuleCall, 最终会传递给RelRule.onMatch()方法进行转换 HepRuleCall call = new HepRuleCall( this, rule.getOperand(), bindings.toArray(new RelNode[0]), nodeChildren, parents); // Allow the rule to apply its own side-conditions. // 调用RelRule.matches(), 允许规则添加自己的判断条件 if (!rule.matches(call)) { return null; } // 触发规则, 进行转换 fireRule(call); if (!call.getResults().isEmpty()) { return applyTransformationResults(vertex, call, parentTrait); } return null;} 整体执行流程HepPlanner的整个查询优化流程如下图所示. 总结可以看到HepPlanner的优化流程还是比较简单的, 无非是暴力地遍历整个RelNode树和所有规则, 一旦发现某个节点的子树Pattern符合规则所指定的Pattern就进行转换. 需要注意的是, 尽管HepPlanner也可以用来生成物理执行计划(如HepPlannerExample中那样), 但是在生产实践中千万不要这么做, 而仅是用HepPlanner进行一些肯定有正收益的规则优化, 如投影下推, 谓词下推等. 因为HepPlanner无法保证输出计划的物理属性, 并且无法对比不同计划之间的成本, 如果要生成物理计划, 需要使用下一篇文章中介绍的VolcanoPlanner. 参考[1] Rule-based Query Optimization","link":"/apache-calcite-hepplanner.html"},{"title":"Apache Calcite SQL解析及语法扩展","text":"本文已收录在合集Apche Calcite原理与实践中. 本文是Apache Calcite原理与实践系列的第二篇, 将会详细介绍Calcite的SQL解析器的实现原理. 最后讲述如何通过扩展Calcite的SQL解析器来实现自定义SQL语法的解析, 比如解析Flink中的CREATE TABLE (…) WITH (…)语法等. 如果读者对Calcite不甚了解, 建议先阅读本系列的第一篇文章, 可以对Calcite的功能和处理流程有一个整体的把握. Calcite SQL解析SQL解析器构建在第一篇文章中已经说到, Calcite的处理流程类似于编译器的处理流程, 第一步就是对SQL字符串进行词法和语法分析, 将其转化为AST. 在现代化的编译器构建中, 一般会借助解析器生成器工具(如Yacc, Anltr, JavaCC等)来自动生成解析器实现词法和语法分析并构建AST. Calcite的SQL解析器同样是基于JavaCC实现的, 要使用JavaCC生成SQL解析器就要提供一个描述SQL词法和语法的Parser.jj文件. 我们当然可以手动编写该文件, 不过Calcite为了方便用户对SQL解析器进行扩展, 使用了FMPP来生成Parser.jj. 这样用就只需要在相关的配置文件中更改或添加新的SQL语法, FMPP就会为我们生成相应的Parser.jj文件, 而无需在扩展时复制整个Parser.jj再进行更改. Calcite解析器的生成流程如下图所示. 对上述流程的具体说明如下: compoundIdentifier.ftl与parserImpls.ftl是扩展文件, 里面可以添加自定义的SQL语法规则, config.fmpp是FMPP的配置文件, 指定需要包含哪些扩展文件. 模板Parser.jj是一个模板文件, 里面引用了compoundIdentifier.ftl与parserImpls.ftl中的内容, 注意模板Parser.jj并不能直接输入JavaCC. 上述文件输入FMPP后, 会组合生成一个可用的Parser.jj文件, 这就是Calcite的SQL解析器语法规则文件, 里面包含预定义的SQL语法规则, 也包含用户新增的规则. Parser.jj文件输入JavaCC后就会生成一个继承自SqlAbstractParserImpl的SqlParserImpl类, 它就是Calcite中真正负责解析SQL语句并生成SqlNode树的类. 当然解析器的类名是可以自定义的. 上述文件都可以在Calcite core模块的codegen文件夹下找到. 以下是其目录结构, 其中default_config.fmpp是一个默认的config.fmpp文件, 可以仿照其中的格式新增相关内容. 关于这些文件的具体内容在后文SQL语法扩展部分还会进一步讲解, 现在只需要知道这些文件都是用来生成Parser.jj文件的, 之所以要使用FMPP是为了方便用户扩展.12345678codegen├── config.fmpp├── default_config.fmpp├── includes│   ├── compoundIdentifier.ftl│   └── parserImpls.ftl└── templates └── Parser.jj # 模板Parser.jj SQL解析树相关概念在Calcite中, 把SQL解析后的结果称为解析树(Parse tree), 实际上就是我们之前说过的SqlNode树. SqlNode是解析树中节点的抽象基类, 不同类型的节点有不同的实现类. 为了更好地理解解析树的结构, 这里先介绍一下SqlNode的相关实现. SqlNode子类如下图所示. 1234567CREATE TABLE t ( ca INT, cb DOUBLE, cc VARCHAR);SELECT ca, cb, cc FROM t WHERE ca = 10; 为了有更直观的感受, 我们配合以上SQL语句来讲解SqlNode各个子类所代表的含义. SqlIdentifier代表标识符, 上述SELECT语句中ca, cb, cc以及t在解析树中都是一个SqlIdentifier实例. SqlLiteral代表常量, 上述SELECT语句中10在解析树中就是一个SqlLiteral实例, 它的具体实现类是SqlNumericLiteral, 表示数字常量. SqlNodeList表示SqlNode列表, 上述SELECT语句中ca, cb, cc会共同组成一个SqlNodeList实例. SqlCall是对SqlOperator的调用. (SqlOperator可以用来描述任何语法结构, 所以实际上SQL解析树中的每个非叶节点都是某种SqlCall). 上述整个SELECT语句就是一个SqlCall实例, 它的具体实现类是SqlSelect. SqlDataTypeSpec表示解析树中的SQL数据类型, 上述CREATE语句中的INT, DOUBLE, VARCHAR在解析树中都是一个SqlDataTypeSpec实例. SqlIntervalQualifier代表时间间隔限定符, 比如SQL中的INTERVAL &apos;1:23:45.678&apos; HOUR TO SECOND在解析树中就是一个SqlIntervalQualifier实例. SqlDynamicParam表示SQL语句中的动态参数标记. 在SqlNode的子类中, SqlLiteral和SqlCall有各自的实现类. 我们先分析简单的SqlLiteral及其实现类, 它的类继承结构如下图所示. 其实每种实现类就代表了一种特定的常量类型, 比如字符串, 数字, 时间, 时间间隔. 根据类名即可望文生义, 这里不再过多介绍. 由于SqlCall的实现类较多, 这里我们仅选择部分有代表性的实现类进行详细介绍. SqlSelect表示整个SELECT语句的解析结果, 内部有from, where, group by等成员变量保存对应字句内的解析结果. SqlOrderBy表示带有ORDER BY的SELECT语句的解析结果. SqlInsert和SqlDelete分别代表INSERT和DELETE语句的解析结果. SqlJoin表示JOIN子句的解析结果. SqlBasicCall表示一个基本的计算单元, 持有操作符和操作数, 如WHERE子句中的一个谓词表达式就会被解析为SqlBasicCall. SqlDdl是DDL语句解析结果的基类. 以CREATE TABLE语句为例, 它就会被解析成SqlCreateTable实例. 上文说到SqlCall其实是对SqlOperator的调用, 因此我们有必要进一步看一下SqlOperator的实现. SqlOperator其实可以表达SQL语句中的任意运算, 它包括函数, 操作符(如=)和语法结构(如case语句). SqlOperator可以表示查询级表达式(如SqlSelectOperator或行级表达式(如SqlBinaryOperator). 由于SqlOperator的实现类较多, 这里我们同样仅挑选几个有代表性的类进行说明. SqlFunction表示SQL语句中的函数调用, 如SqlCastFunction表示cast函数, 在解析阶段所有自定义函数都会被表示为SqlUnresolvedFunction, 在验证阶段才会转化为对应的SqlUserDefinedFunction. SqlSelectOperator表示整个SELECT查询语句. SqlBinaryOperator表示二元运算, 如WHERE子句中的=运算. SQL解析流程有了上一节的介绍, 相信读者对SQL解析树的组成结构已经有了了解, 接下来我们再来讲述Calcite是如何解析SQL字符串, 并将其组成为解析树的. 在上一篇文章中我们是使用SqlParser作为入口来解析SQL语句的, 只不过当时我们使用了默认的配置, 实际上等同于以下代码.1234SqlParser.Config config = SqlParser.config() .withParserFactory(SqlParserImpl.FACTORY);SqlParser parser = SqlParser.create(sql, config);SqlNode sqlNode = parser.parseStmt(); SqlParserImpl.FACTORY静态成员变量是定义在Paser.jj中的, 因此会生成到SqlParserImpl类中. 它的定义如下, 调用其getParser函数就会得到一个SqlParserImpl实例.1234567891011public static final SqlParserImplFactory FACTORY = new SqlParserImplFactory() { public SqlAbstractParserImpl getParser(Reader reader) { final SqlParserImpl parser = new SqlParserImpl(reader); if (reader instanceof SourceStringReader) { final String sql = ((SourceStringReader) reader).getSourceString(); parser.setOriginalSql(sql); } return parser; }}; SqlParserImpl.FACTORY在SqlParser.create中会被用到, SqlParser中的相关代码如下. 可以看到, SqlParser中实际包含了一个SqlParserImpl, 当我们调用SqlParser.parseStmt解析SQL语句时, 内部其实会调用SqlParserImpl.parseSqlStmtEof, 这个函数是定义在Parser.jj中的.1234567891011121314151617181920public static SqlParser create(String sql, Config config) { return create(new SourceStringReader(sql), config);}public static SqlParser create(Reader reader, Config config) { SqlAbstractParserImpl parser = config.parserFactory().getParser(reader); return new SqlParser(parser, config);}public SqlNode parseStmt() throws SqlParseException { return parseQuery();}public SqlNode parseQuery() throws SqlParseException { try { return parser.parseSqlStmtEof(); } catch (Throwable ex) { throw handleException(ex); }} 现在我们终于来到Parser.jj中了, 由于SqlParserImpl是由Parser.jj自动生成的, 比较难阅读, 又因为两者间的函数其实是一一对应的, 所以我们这里主要分析Parser.jj中的代码. 只不过Parser.jj中的函数是用扩展的巴科斯范式(EBNF)以及JavaCC的action描述的, 如果对相关内容不熟悉建议先阅读笔者之前的博文编译原理实践 - JavaCC解析表达式并生成抽象语法树, 以快速了解Parser.jj的相关语法. parseSqlStmtEof函数的调用链是比较长的, 其到SELECT语句解析的调用链如下. 这里我们只具体讲述调用链中的两个重要函数SqlStmt()和SqlSelect().12345678910111213parseSqlStmtEof() SqlStmtEof() SqlStmt() // 解析各类语句的总入口, 如INSERT, DELETE, UPDATE, SELECT等 OrderedQueryOrExpr(ExprContext exprContext) QueryOrExpr(ExprContext exprContext) LeafQueryOrExpr(ExprContext exprContext) LeafQuery(ExprContext exprContext) SqlSelect() // 真正开始解析SELECT语句 SqlSelectKeywords(List&lt;SqlLiteral&gt; keywords) FromClause() WhereOpt() HavingOpt() WindowOpt() SqlStmt()的定义如下, 可以看到这是一个解析各类SQL语句的总入口, |表示或. 由于查询语句相对复杂, 会在OrderedQueryOrExpr实现.12345678910111213141516171819SqlNode SqlStmt() : // 会生成SqlParserImpl中的SqlStmt()函数{ SqlNode stmt; // Java代码, 定义临时变量}{ ( stmt = SqlSetOption(Span.of(), null)| stmt = SqlAlter()| stmt = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY)| stmt = SqlExplain()| stmt = SqlDescribe()| stmt = SqlInsert()| stmt = SqlDelete()| stmt = SqlUpdate()| stmt = SqlMerge()| stmt = SqlProcedureCall() ) { return stmt; } // Java代码, 返回解析结果} SELECT语句最终会通过SqlSelect()来解析, 其详细代码如下, 即使不了解JavaCC的EBNF语法, 只要了解正则表达式, 详细配合注释也能大致理解以下代码.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748SqlSelect SqlSelect() :{ final List&lt;SqlLiteral&gt; keywords = new ArrayList&lt;SqlLiteral&gt;(); final SqlNodeList keywordList; List&lt;SqlNode&gt; selectList; final SqlNode fromClause; final SqlNode where; final SqlNodeList groupBy; final SqlNode having; final SqlNodeList windowDecls; final List&lt;SqlNode&gt; hints = new ArrayList&lt;SqlNode&gt;(); final Span s;}{ &lt;SELECT&gt; { s = span(); } [ &lt;HINT_BEG&gt; CommaSeparatedSqlHints(hints) &lt;COMMENT_END&gt; ] SqlSelectKeywords(keywords) ( &lt;STREAM&gt; { keywords.add(SqlSelectKeyword.STREAM.symbol(getPos())); } )? ( &lt;DISTINCT&gt; { keywords.add(SqlSelectKeyword.DISTINCT.symbol(getPos())); } | &lt;ALL&gt; { keywords.add(SqlSelectKeyword.ALL.symbol(getPos())); } )? { keywordList = new SqlNodeList(keywords, s.addAll(keywords).pos()); } selectList = SelectList() // 解析SELECT后的列 ( &lt;FROM&gt; fromClause = FromClause() // 解析FROM子句 where = WhereOpt() // 解析WHERE子句 groupBy = GroupByOpt() // 解析GROUP BY子句 having = HavingOpt() // 解析HAVING子句 windowDecls = WindowOpt() | E() { fromClause = null; where = null; groupBy = null; having = null; windowDecls = null; } ) { return new SqlSelect(s.end(this), keywordList, new SqlNodeList(selectList, Span.of(selectList).pos()), fromClause, where, groupBy, having, windowDecls, null, null, null, new SqlNodeList(hints, getPos())); }} Calcite的Parser.jj文件内容是比较多的, 默认实现下总共有八千多行, 不过也没有必要阅读所有的代码, 只要在需要时通过调用链路阅读关键代码即可. Calcite SQL语法扩展上文介绍了Calcite SQL解析器的实现原理, 并具体介绍了SELECT语句是如何解析的. 尽管Calcite已经提供了SQL语言的一个超集, 但是底层系统丰富多样, 实践中我们仍可能需要扩展一些自定义的SQL语法来支持特定功能. 比如Flink和Spark在使用SQL创建表时, 需要一些额外信息用于指定数据源的类型, 位置和格式. 本文以Flink的CREATE TABLE (…) WITH (…)语法为例, 介绍如何扩展Calcite的SQL解析器. 上文已经介绍过Calcite的解析器是如何构建的, 在扩展时我们也需要准备相应的文件. 一般来说, 我们会使用与Calcite类似的目录组织, 在codegen文件夹下放置相关的扩展文件, 目录结构如下所示. 这里的目录结构借鉴自Flink, 增加了Parser.tdd文件, 用于简化config.fmpp的编写. 这里我们不需要复制Calcite的模板Parser.jj文件, 因为该文件不需要修改, 在编译时可以从Calcite的JAR包中自动提取.1234567codegen├── config.fmpp├── data│   └── Parser.tdd└── includes ├── compoundIdentifier.ftl └── parserImpls.ftl 下面我们来具体介绍一下各个文件中的内容. config.fmpp文件中的内容如下, 通过引入Parser.tdd文件, 我们可以把data部分的内容转移到Parser.tdd中, 从而使config.fmpp文件更加简洁.1234567data: { parser: tdd(../data/Parser.tdd)}freemarkerLinks: { includes: includes/} 这里需要注意的一点是, Calcite的core模块并未提供DDL语法的解析, 这部分是在server模块中扩展的, 当我们需要扩展DDL语法时最简单的做法是将server模块中的实现先复制过来, 再进行更改. 操作步骤如下: 将Calcite server模块中parserImpls.ftl文件中的内容复制到我们自己的parserImpls.ftl文件中. 将Calcite server模块中config.fmpp文件中data部分的内容复制到我们自己的Parser.tdd文件中. 经过上述操作之后, 其实我们就可以编译生成可以解析DDL的解析器了, 当然我们需要在pom.xml文件中引入一些插件并做一些配置, 来自动生成解析器, 详细配置可参考这里. 编译完成之后我们可以通过如下代码解析DDL, 注意这里使用的是SqlDdlParserImpl.FACTORY而不再是SqlParserImpl.FACTORY.1234SqlParser.Config config = SqlParser.config() .withParserFactory(SqlDdlParserImpl.FACTORY);SqlParser parser = SqlParser.create(ddl, config);SqlNode sqlNode = parser.parseStmt(); 经过上述准备, 我们已经可以在自己的工程中生成可以解析DDL语句的解析器了. 为了实现CREATE TABLE (...) WITH (...)语法, 我们只需要在现有基础上进行一些修改即可. 首先介绍Parser.tdd文件, 其主要内容如下, 这里面配置了生成的解析器类名, 以及需要引入的新的关键字以及语法规则等.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253{ package: &quot;org.apache.calcite.sql.parser.impl&quot;, # 解析器的包名, 可自定义 class: &quot;CustomSqlParserImpl&quot;, # 解析器的类名, 可自定义 imports: [ &quot;org.apache.calcite.sql.SqlCreate&quot; # 需要导入的Java类 ] keywords: [ # 新增关键字 &quot;IF&quot; ] nonReservedKeywords: [ ] # 上述keywords中的非保留字 nonReservedKeywordsToAdd: [ &quot;IF&quot; ] nonReservedKeywordsToRemove: [ ] statementParserMethods: [ ] # 新增的用于解析SQL语句的方法, 例如SqlShowTables() literalParserMethods: [ ] dataTypeParserMethods: [ ] builtinFunctionCallMethods: [ ] alterStatementParserMethods: [ ] createStatementParserMethods: [ # 新增的用于解析CREATE语句的方法 &quot;SqlCreateTable&quot; ] dropStatementParserMethods: [ # 新增的用于解析DROP语句的方法 &quot;SqlDropTable&quot; ] binaryOperatorsTokens: [ ] extraBinaryExpressions: [ ] implementationFiles: [ # 方法的实现文件 &quot;parserImpls.ftl&quot; ] joinTypes: [ ] includePosixOperators: false includeCompoundIdentifier: true includeBraces: true includeAdditionalDeclarations: false} 真正实现解析CREATE TABLE语句的是parserImpls.ftl中的SqlCreateTable方法, 编译时它会合并到Parser.jj文件中, 它的默认实现如下. 可以看到默认试下是不支持WITH选项的.12345678910111213141516SqlCreate SqlCreateTable(Span s, boolean replace) :{ final boolean ifNotExists; final SqlIdentifier id; SqlNodeList tableElementList = null; SqlNode query = null;}{ &lt;TABLE&gt; ifNotExists = IfNotExistsOpt() id = CompoundIdentifier() [ tableElementList = TableElementList() ] [ &lt;AS&gt; query = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY) ] { return SqlDdlNodes.createTable(s.end(this), replace, ifNotExists, id, tableElementList, query); }} 为了支持WITH选项, 我们对SqlCreateTable方法做如下修改.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051SqlCreate SqlCreateTable(Span s, boolean replace) :{ final boolean ifNotExists; final SqlIdentifier id; SqlNodeList tableElementList = null; SqlNodeList propertyList = null; SqlNode query = null;}{ &lt;TABLE&gt; ifNotExists = IfNotExistsOpt() id = CompoundIdentifier() [ tableElementList = TableElementList() ] [ &lt;WITH&gt; propertyList = TableProperties() ] // 用于解析WITH选项 [ &lt;AS&gt; query = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY) ] { return new SqlCreateTable(s.end(this), replace, ifNotExists, id, tableElementList, propertyList, query); }}// 解析WITH选择中的各个键值对SqlNodeList TableProperties():{ SqlNode property; final List&lt;SqlNode&gt; proList = new ArrayList&lt;SqlNode&gt;(); final Span span;}{ &lt;LPAREN&gt; { span = span(); } [ property = TableOption() { proList.add(property); } ( &lt;COMMA&gt; property = TableOption() { proList.add(property); } )* ] &lt;RPAREN&gt; { return new SqlNodeList(proList, span.end(this)); }}// 解析键值对SqlNode TableOption() :{ SqlNode key; SqlNode value; SqlParserPos pos;}{ key = StringLiteral() { pos = getPos(); } &lt;EQ&gt; value = StringLiteral() { return new SqlTableOption(key, value, getPos()); }} 在上述函数中, 我们引入了一个新的类SqlTableOption, 这个类是需要我们自己定义的. 另外由于引入了WITH语句, 我们也需要对SqlCreateTable进行修改, 在其中增加一个SqlNodeList类型的成员变量用于保存WITH语句中的键值对. 核心代码如下.12345678910111213141516public class SqlCreateTable extends SqlCreate { public final SqlIdentifier name; public final SqlNodeList columnList; public final SqlNodeList propertyList; // 保存WITH语句中的键值对 public final SqlNode query; public SqlCreateTable(SqlParserPos pos, boolean replace, boolean ifNotExists, SqlIdentifier name, SqlNodeList columnList, SqlNodeList propertyList, SqlNode query) { super(OPERATOR, pos, replace, ifNotExists); this.name = Objects.requireNonNull(name); this.columnList = columnList; // may be null this.propertyList = propertyList; // may be null this.query = query; // for &quot;CREATE TABLE ... AS query&quot;; may be null }} 到这里为止, 我们就完成了CREATE TABLE (...) WITH (...)语法的扩展, 完整的代码在这里. 读者可以下载相关代码进行体验, mvn clean package编译整个项目后, 运行CalciteSQLParser即可. 在CalciteSQLParser中, 我们使用了扩展后的解析器, 主要传入的工厂类是CustomSqlParserImpl.FACTORY.123456String ddl = &quot;CREATE TABLE aa (id INT) WITH (&apos;connector&apos; = &apos;file&apos;)&quot;;SqlParser.Config config = SqlParser.config() .withParserFactory(CustomSqlParserImpl.FACTORY);SqlParser parser = SqlParser.create(ddl, config);SqlNode sqlNode = parser.parseStmt(); 总结Calcite的解析器是基于JavaCC构建的, 要真正理解解析器的实现原理, JavaCC相关的知识肯定是越多越好. 如果熟悉类似的解析器生成工具如Antlr等, 相信可以很快掌握JavaCC的语法. SQL解析的过程其实就是编译器前端的工作, 都是为了生成AST, 只不过AST的结构有所区别, 如果对这方面不太了解的可以参考笔者之前的博文编译原理实践 - JavaCC解析表达式并生成抽象语法树, 以表达式为例, 讲述如何通过JavaCC将其解析为AST并计算. Calcite的强大之处就在于其扩展性, 我们可以通过JavaCC的EBNF语法快速实现自定义语法的解析. 本文的案例给出了如何扩展SQL语法的模板, 读者可以依葫芦画瓢实现自己的SQL语法.","link":"/apache-calcite-sql-parser.html"},{"title":"Apache Calcite关系代数","text":"本文已收录在合集Apche Calcite原理与实践中. 本文是Apache Calcite原理与实践系列的第四篇, 前两篇文章介绍了SQL语句的解析及验证, 本文开始介绍关系代数的原理与实现. 关系代数最早由E. F. Codd在1970年的论文”A Relational Model of Data for Large Shared Data Banks“中提出, 是关系型数据库查询语言的基础, 也是查询优化技术的理论基础. 随着关系代数和关系模型的不断发展和完善, 目前几乎所有对外支持SQL访问的系统, 都会将SQL转化为等价的关系代数表达, 并基于此进行查询优化. 在Calcite内部, 同样会将SQL查询转化为一颗等价的关系算子树, 并在此基础上进行查询优化. 本文首先介绍通用的关系代数理论, 之后介绍其在Calciate中的实现. 关系代数Codd提出的关系代数的6个原始运算是选择(Select), 投影(Project), 笛卡尔积(Cartesian-product), 并集(Union), 差集(Difference), 和重命名(Rename). 随着关系模型的不断发展, 更多的关系代数运算被提出, 目前主要的关系代数运算如下表所示. 类别 名称 符号 示例 一元运算 选择(Select) $\\sigma$ 符号: $\\sigma_{condition}(R)$SQL: SELECT * FROM R WHERE condition 投影(Project) $\\Pi$ 符号: $\\Pi_{x, y}(R)$ SQL: SELECT x, y FROM R 赋值(Assignment) $\\gets$ 符号: $$ t \\gets \\Pi_{x, y}(\\sigma_{condition}(R)) $$ SQL: CREATE VIEW t AS SELECT x, y FROM R WHERE condition 重命名(Rename) $\\rho$ 符号: $\\rho_{S(x1, y1)}(R)$SQL: SELECT * FROM (SELECT x AS x1, y AS y1 FROM R) AS S 二元运算 并(Union) $\\cup$ 符号: $R \\cup S$SQL: SELECT * FROM R UNION SELECT * FROM S 交(Intersection) $\\cap$ 符号: $R \\cap S = R - (R-S)$SQL: SELECT * FROM R WHERE x NOT IN (SELECT x FROM R WHERE x NOT IN (SELECT x FROM S)) 差(Difference) $-$ 符号: $R - S$SQL: SELECT * FROM R WHERE x NOT IN (SELECT x FROM S) 笛卡儿积(Cartesian-product) $\\times$ 符号: $R \\times S$SQL: SELECT * FROM R, S 除(Divide) $\\div$ 符号: $R \\div S$SQL: SELECT DISTINCT r1.x FROM R AS r1 WHERE NOT EXISTS (SELECT S.y FROM S WHERE NOT EXISTS (SELECT * FROM R AS r2 WHERE r2.x = r1.x AND r2.y = S.y)) 连接(Join) $\\Join$ 符号: $R \\Join_{condition} S$SQL: SELECT * FROM R JOIN S ON condition 注: 其中$R$和$S$表示关系, $x$, $y$和$z$表示关系中的属性或列. 上述运算中, 其中一部分仅在一个关系上进行运算, 一般称为一元运算. 另一部分在一对关系上进行运算, 称为二元运算. 其中二元运算中大多数都是集合运算(并, 交, 差, 笛卡尔积), 连接是一种比较特别的关系运算, 上表中的连接更准确地来说是自然连接. 随着关系模型的广泛使用, 越来越多的连接运算被提出和应用. 下图(图片来自C.L. Moffatt的博文)通过可视化的方式直观地展示了各类常见连接的含义及其对应的SQL表示方法. SQL是一种基于关系代数的数据库查询语言, 目前已经成为了关系代数模型的标准接口, 它实现了绝大多数的关系运算, 但不是全部. 所有的SQL语句都可以转化为等价的关系代数表示. 从SQL到关系代数类似于编译器会首先将高级语言转化为中间表示(IR), 然后对IR进行优化, 最后转化为目标机器语言. Calcite会先将SQL语句解析为SqlNode树(实际上是一种抽象语法树(AST)), 之后Calcite会将SqlNode树转化为RelNode树(即Calcite中的IR)并进行查询优化. 以下是一个将SQL解析为SqlNode树, 再转化为RelNode树的案例. 关于Calcite中SQL解析的内容已经在本系列的第二篇文章中介绍了, 本文接下来的部分将会介绍从SqlNode到RelNode的转换. 为了方便表述, 本文接下来将某个关系运算的具体实现, 即具体的RelNode实现类, 称为关系算子或简称为算子. Calcite关系算子实现Calcite的核心是查询优化, 其背后也是基于关系代数实现的. 接下来本文将详细介绍Calcite中关系算子的实现. 实现关系算子的最终目的是进行等价转换从而实现查询优化和执行计划生成, 因此本文会同时介绍Calcite关系算子中与查询优化和执行计划相关的概念. RelNodeRelNode类型及继承关系RelNode是Calcite中关系算子的父类型, 代表了对关系的一种处理操作. 所有的关系算子都是它的子类型, 它的几个上层子类型的继承关系如下图所示. 实际上关系算子的最顶层类型是RelOptNode, 不过在Calcite的代码和文档中经常出现的是RelNode, 因此这里我们也用RelNode进行讲述. 我们常见的关系算子都是AbstractRelNode的子类型, 其中: SingleRel是一元算子的父类型, 其子类型包含常见的Project, Filter, Aggregate等. BiRel是Join算子的父类型. SetOp是集合算子的父类型, 其子类型包含Union, Intersect, Minus. AbstractRelNode属于逻辑算子, 与物理执行无关. EnumerableRel和BindableRel是Calcite中的物理算子, 可以转化为执行计划, 从底层数据源中获取数据. 具体来说EnumerableRel是通过代码生成的方式来生成执行计划的, 而BindableRel是通过解释执行的. 关于Calcite的执行计划会在后续的文章中详细介绍. Converter虽然也是RelNode的子类型, 但它实际上不代表一种关系运算, 而是不同RelNode之间的转换器. 通常来说是将逻辑算子转换为物理算子或执行计划, 在后续关于执行计划的文章中会进一步介绍Conveter. RelNode的子类型很多, 为了方便表述和理解, 本文将其在横向和纵向两个维度进行分类. 横向上存在各种不同种类的关系算子, 例如Project, Filter等; 纵向上存在不同属性的关系算子, 例如Project既有LogicalProject也有EnumerableProject. 由于Calcite支持多种数据源, 对于每种数据源都会有多个相同属性不同种类的关系算子. RelNode内部实现介绍完RelNode的种类, 我们再来看下RelNode的具体实现. 以下是RelNode中几个比较重要的方法, 本文重点要介绍的是与物理属性有关的两个方法, 关于查询优化相关的方法会在后续关于查询优化器的文章中介绍. getTraitSet()用于获取当前算子的所有物理属性集合, 可以看到在AbstractRelNode中包含一个RelTraitSet类型的属性, 它实现了AbstractList&lt;RelTrait&gt;用于保留所有物理属性; getConvention()用于获取当前算子的Calling Convertion(下文将详细介绍). 1234567891011121314151617181920212223242526272829303132333435363738394041public interface RelNode extends RelOptNode, Cloneable { // 获取当前算子的Calling Convertion, Calling Convertion的概念在下文介绍 Convention getConvention(); // 获取当前算子的物理属性 RelTraitSet getTraitSet(); // 获取RelOptTable如果当前节点是TableScan节点, 否则返回null RelOptTable getTable(); // 获取当前节点的子节点 RelNode getInput(int i); List&lt;RelNode&gt; getInputs(); // 向RelOptPlanner注册当前节点及其子节点 RelNode onRegister(RelOptPlanner planner); // 向RelOptPlanner注册专有优化规则 void register(RelOptPlanner planner); // 代价相关方法 double estimateRowCount(RelMetadataQuery mq); RelOptCost computeSelfCost(RelOptPlanner planner, RelMetadataQuery mq); // 遍历相关方法 RelNode accept(RelShuttle shuttle); RelNode accept(RexShuttle shuttle); void childrenAccept(RelVisitor visitor);}public abstract class AbstractRelNode implements RelNode { protected RelTraitSet traitSet; public final Convention getConvention() { return traitSet.getTrait(ConventionTraitDef.INSTANCE); } public RelTraitSet getTraitSet() { return traitSet; }} RelTraitRelTrait用于描述RelNode的物理属性, 它对于查询优化以及关系算子在逻辑和物理层面的转化都有重要影响. 举例来说, RelTrait可以用来描述RelNode的排序属性, 在查询优化时如果Sort算子的输入已经有序, 而且排序的目标列一致, 那么可以直接省略Sort算子以实现优化; 另外, RelTrait还可以用来描述RelNode的物理数据源, 这样就可将逻辑算子转换为指定的物理算子. RelTrait的继承关系如上图所示, 它的3个子类型的主要含义如下: RelCollation用于描述算子输出的排序属性. RelDistribution用于描述算子输出的分布属性, 如HASH_DISTRIBUTED, RANDOM_DISTRIBUTED等. Convention用于描述算子的底层数据处理引擎. 引入Convention可以让Calcite与底层的执行引擎解耦, 如果要适配新的执行引擎, 只需要扩展一个Convention实现即可, 如Flink中就有扩展的FlinkConvention. Calcite中已经实现的Convention如下图所示, 从名称中就可以看出是用于转换到什么执行引擎. RelTraitDefRelTraitDef代表一个RelTrait种类, 这里需要注意的是”种类”是指RelDistribution, RelCollation和Convention这三类, 而不是指具体的实现类. 也就是说JdbcConvention, EnumerableConvention以及其他Convention实现类的所对应的都是ConventionTraitDef, 在实现上它们的getTraitDef()方法都返回的是ConventionTraitDef.INSTANCE. RelTraitDef的实现类如下图所示. Calling Convention在RelNode相关的代码注释中经常会看到Calling Convention这个词, 从字面上理解它表示的是RelNode该如何被调用. 这个”调用”实际上应该理解为”转换”, 一般情况下是指从逻辑算子到物理算子的转换, 如从LogicalTableScan到EnumerableTableScan的转换. 从上文的介绍中也可以知道, 每个RelNode都会有一个RelTraitSet类型的变量来描述其当前的物理属性, 其中包含Convention. 对于逻辑算子, 其Convention一般是默认实现, 即Convention.Impl, 而物理算子则有其对应的Convention属性, 如EnumerableRel包含EnumerableConvention. 在Calcite中引入Calling Convention是为了方便对接多种不同的物理数据源, 不同属性间的算子转换由ConverterRule实现, 例如从LogicalTableScan到EnumerableTableScan的转换由EnumerableTableScanRule实现, 它是ConverterRule的一个实现, 顶层父类型是RelOptRule, 在后面关于查询优化器的文章中会有关于RelOptRule的详细介绍. RexNodeRexNode表示一个行表达式. 如果说RelNode表示的是如何对一个关系或表进行操作, 那么RexNode表示的是如何对表中的一行进行操作. 其实现关系如下. 其中: RexVariable代表变量, 一般是一个字段. 它的子类型中: RexDynamicParam代表SQL语句中的动态参数, 如SELECT name FROM users where id = ?中的?; RexLocalRef代表本地变量, 一般用于在一个节点内部相同字段相互之间的引用; RexInputRef代表字段引用, 用于上层节点对下层节点字段的引用; RexCorrelVariable代表一个字段, 用在Nested Loop Join中, 表示属于外表的关联字段. 比如SELECT u.name, o.price FROM user AS u JOIN order AS o on u.id = o.user_id中如果采用Nested Loop Join实现, 并且user为外表, 那么用RexCorrelVariable来表示u.id. RexLiteral代表常量. RexCall代表表达式, SQL语句中的所有运算都是一个表达式, 如id = 1, id = 1 and name = &apos;Jack&apos;. RexRangeRef代表多个列的集合. RexFieldAccess代表字段引用. Calcite关系算子树构建Calcite提供了SqlToRelConverter来实现从SqlNode到RelNode的转换, 实际上SqlToRelConverter的实现依赖于RelBuilder, 本节首先介绍如何基于RelBuilder构建关系算子, 之后分析SqlToRelConverter的实现原理. RelBuilderRelBuilder是Calcite中的关系算子生成器, 它提供的方法可用于快速生成绝大多数关系算子. 从整体上来看, RelBuilder所提供的方法可分为以下几类: 一类是生成RelNode的方法, 如scan(), filter(), project()等. 一类是生成RexNode的方法, 如field(), and(), equals()等. 还有其他一些辅助方法, 如生成GROUP BY字段的groupKey()方法, Calcite源代码中的RelBuilderExample包含了RelBuilder的详细示例, 本文不再赘述. SqlToRelConverterSqlToRelConverter用于将SqlValidator验证之后的SqlNode树转化为RelNode树, 其主要思想是通过遍历SqlNode树将其结点逐个转换为RelNode. 但是由于SqlNode种类众多, 其实现比较繁杂, 本文以一条简单的SQL语句为例, 来梳理从SqlNode到RelNode的整个转换流程. 我们以SELECT * FROM users;为例, 这条SQL语句的关系代数表示很容易通过肉眼看出, 在Calcite中的表示如下.12LogicalProject(id=[$0], name=[$1], age=[$2]) LogicalTableScan(table=[[s, users]]) 上述SQL语句的转换过程如下图所示: convertQuery是SqlToRelConverter的入口, 它的输入是SQL语句对应的SqlNode表示; convertQueryRecursive会根据SqlNode的SqlKind来调用对应的方法进行转换, 如SELECT类型调用convertSelect, INSERT类型调用convertInsert等. 很明显上述示例SQL语句是SELECT类型, 因此会调用convertSelect; convertSelect做完简单的准备工作后会直接调用convertSelectImpl, 它会对SQL语句的各个部分分别进行转换, 如对SELECT字段调用convertSelectList, 对FROM子句调用convertFrom, 对WHERE子句调用convertWhere, 对GROUP BY子句调用convertAgg, 对ORDER BY子句调用convertOrder. 在本例中会首先调用convertFrom转换FROM子句, 然后调用convertSelectList转换SELECT字段; convertFrom会调用conevrtIdentifier将FROM users转换为LogicalTableScan; convertSelectList会将*展开为具体的字段名, 最后调用RelBuilder.projectNamed生成LogicalProject. 总结关系代数是查询优化的基础, 本文介绍了Calcite中关系代数的实现. 可以看到, Calcite中关系算子的种类多样, 重点是要把握各个算子的抽象层次, 一般来说在扩展Calcite适配自定义数据源时, 只需实现对应的物理算子即可. Calcite提供了SqlToRelConverter来实现从SqlNode到RelNode的转换, 这一步得到的RelNode都是逻辑算子, 从逻辑算子到物理算子的转换将在RelOptPlanner中实现. 参考[1] Apache Calcite Docs - Algebra[2] Relational Operators in Apache Calcite[3] Visual Representation of SQL Joins 附录附录一: RelBuilder的创建在RelBuilderExample中, RelBuilder是通过Frameworks创建的, 去除依赖之后的代码如下. Frameworks会自动创建RelBuilder所需的参数. 123456789SchemaPlus rootSchema = Frameworks.createRootSchema(true);Schema hrSchema = new HrClusteredSchema();rootSchema.add(&quot;hr&quot;, hrSchema);FrameworkConfig config = Frameworks.newConfigBuilder() .parserConfig(SqlParser.Config.DEFAULT) .defaultSchema(rootSchema.getSubSchema(&quot;hr&quot;)) .build();RelBuilder builder = RelBuilder.create(config); 为了更好地理解RelBuilder的创建过程, 笔者将Frameworks的创建过程拆解出来了, 以下代码同样可以创建RelBuilder. 从中可以看出, RelBuilder的创建需要两个参数: RelOptCluster主要用于获取RexBuilder和RelDataTypeFactory, 这里笔者认为代码抽象得并不优雅, 实际上在构建RelNode的时候不需要用到RelOptPlanner. CatalogReader主要用于在创建TableScan时获取具体的表结构信息. 12345678910111213141516CalciteSchema rootSchema = CalciteSchema.createRootSchema(false, false);Schema hrSchema = new HrClusteredSchema();rootSchema.add(&quot;hr&quot;, hrSchema);RelBuilderFactory relBuilderFactory = RelFactories.LOGICAL_BUILDER;RelDataTypeFactory relDataTypeFactory = new JavaTypeFactoryImpl();VolcanoPlanner planner = new VolcanoPlanner();RelOptCluster cluster = RelOptCluster.create(planner, new RexBuilder(relDataTypeFactory));Prepare.CatalogReader catalogReader = new CalciteCatalogReader( rootSchema.getSubSchema(&quot;hr&quot;, false), Collections.singletonList(rootSchema.getName()), relDataTypeFactory, CalciteConnectionConfig.DEFAULT);RelBuilder relBuilder = relBuilderFactory.create(cluster, catalogReader);","link":"/apache-calcite-relational-algebra.html"},{"title":"Apache Calcite SQL验证","text":"本文已收录在合集Apche Calcite原理与实践中. 本文是Apache Calcite原理与实践系列的第三篇, 上一篇文章介绍了Calcite解析器的实现原理, 本文将介绍如何对解析器输出的SQL解析树进行语义分析, 如表名, 字段名, 函数名和数据类型的检查. 相对于解析器, SQL验证部分的内容扩展需求较少, 所以本文重点介绍Calcite中Schema相关的接口(用于提供元数据), 以及SQL验证相关的概念, 最后以SELECT语句为例, 介绍验证过程中的重要步骤. Calcite元数据接口SQL解析阶段只要SQL语句中没有语法错误便可解析成功, 而不会关注SQL语句的具体语义是否正确, 比如表是否存在, 字段是否存在等. SQL验证阶段就会检查SQL语句的语义是否正确, 这就需要依赖于外部提供的元数据信息. 为此, Calcite提供了一系列用于提供元数据信息的接口. 这里我们将第一篇文章中与SQL验证相关的部分复制过来, 以便对照讲述.1234567891011121314151617181920212223242526272829303132333435363738394041// 创建Schema, 一个Schema中包含多个表. Calcite中的Schema类似于RDBMS中的DatabaseSimpleTable userTable = SimpleTable.newBuilder(&quot;users&quot;) .addField(&quot;id&quot;, SqlTypeName.VARCHAR) .addField(&quot;name&quot;, SqlTypeName.VARCHAR) .addField(&quot;age&quot;, SqlTypeName.INTEGER) .withFilePath(&quot;/path/to/user.csv&quot;) .withRowCount(10) .build();SimpleTable orderTable = SimpleTable.newBuilder(&quot;orders&quot;) .addField(&quot;id&quot;, SqlTypeName.VARCHAR) .addField(&quot;user_id&quot;, SqlTypeName.VARCHAR) .addField(&quot;goods&quot;, SqlTypeName.VARCHAR) .addField(&quot;price&quot;, SqlTypeName.DECIMAL) .withFilePath(&quot;/path/to/order.csv&quot;) .withRowCount(10) .build();SimpleSchema schema = SimpleSchema.newBuilder(&quot;s&quot;) .addTable(userTable) .addTable(orderTable) .build(); CalciteSchema rootSchema = CalciteSchema.createRootSchema(false, false);rootSchema.add(schema.getSchemaName(), schema);RelDataTypeFactory typeFactory = new JavaTypeFactoryImpl();// 创建CatalogReader, 用于指示如何读取Schema信息Prepare.CatalogReader catalogReader = new CalciteCatalogReader( rootSchema, Collections.singletonList(schema.getSchemaName()), typeFactory, config);// 创建SqlValidator, 用于执行SQL验证SqlValidator.Config validatorConfig = SqlValidator.Config.DEFAULT .withLenientOperatorLookup(config.lenientOperatorLookup()) .withSqlConformance(config.conformance()) .withDefaultNullCollation(config.defaultNullCollation()) .withIdentifierExpansion(true);SqlValidator validator = SqlValidatorUtil.newValidator( SqlStdOperatorTable.instance(), catalogReader, typeFactory, validatorConfig);// 执行SQL验证SqlNode validateSqlNode = validator.validate(node); 从上述代码中可以看到, 有三个与元数据信息有关的重要接口: Schema(SimpleSchema继承自Schema), CalciteSchema和CatalogReader. 下面我们将分别介绍. Schema是用于描述数据库结构(如包含哪些表, 函数等)的一个接口. 在关系型数据库管理系统中通常有Catalog, Schema, Table这样的层级结构来管理表和数据库, 不过并非每种RDBMS都完全这么实现, 比如MySQL不支持Catalog, 并且用Database来替代Schema的位置. Calcite的Schema接口可以表示Catalog或Schema, 因为Schema接口支持嵌套, 这样就可以用来表示Schema嵌套在Catalog里这种层级结构. Schema接口的实现类如下图所示. 这里介绍几个重要的实现类: SchemaPlus接口是Schema接口的扩展, 增加了一些新的方法, 比如添加表. SchemaPlus不应该由用户创建. AbstractSchema是一个默认实现, 我们代码中的SimpleSchema就继承自该类. DelegatingSchema是一个简单的代理类, 将所有的操作交给内部的一个Schema实现. CalciteSchema接口是Schema接口的包装类, 可以嵌套多个Schema实体, 并提供了一些工具方法, 比如plus()方法可以将内部Schema包装为SchemaPlus后返回. 它有两个实现类, 在我们的代码中使用的是SimpleCalciteSchema. CatalogReader用于读取表的元数据信息, 在绝大多数情况下只需使用其默认实现CalciteCatalogReader即可, 如有特殊需求, 可继承CalciteCatalogReader进行实现. 这里重点介绍一下CalciteCatalogReader构造函数中需要的RelDataTypeFactory. 它是一个数据类型描述符的工厂, 定义了用于实例化和组合SQL, Java和集合类型的方法. 在我们代码的SimpleTable.getRowType()函数中会用到. 可以看到Calcite中用于提供元数据的接口还是有点绕的, 特别是Schema相关的接口. 不过好在在实践中我们通常只需要实现相应的Schema接口, 用于读取特定数据源的元数据信息即可. Calcite SQL验证相关概念在介绍SQL验证流程之前, 我们先介绍一下Calcite为实现SQL验证引入的一些概念. 在通用编程语言中都有作用域的概念, 只能使用当前作用域或父作用域内的函数或变量, 比如C语言的函数是一个作用域, 函数内部只能使用函数内定义的局部变量, 或定义在全局作用域内的全局变量或函数, 但是不能使用定义在其他函数内部的局部变量. SQL语言中同样有作用域, 在Calcite中称为Scope. 123456SELECT expr1FROM t1, t2, (SELECT expr2 FROM t3) AS q3WHERE c1 IN (SELECT expr3 FROM t4)ORDER BY expr4 我们以上述SQL语句来说明Calcite中关于Scope的概念. 在查询的各个位置可用的作用域如下: expr1只能看见t1, t2和q3, 也就是说expr1只能使用t1, t2, q3中存在的列名. expr2只能看见t3. expr3只能看见t4. expr4只能看见t1, t2, q3, 加上SELECT子句中定义的任何别名. 在Calcite中Scope由SqlValidateScope表示, 其类继承图如下. SQL语句需要从一个源中获取数据, Calcite将数据源抽象为命名空间Namespace. Namespace是一个抽象概念, 它既可以表示一个表, 也可以是视图或子查询. 上述SQL语句中有4个Namespace: t1, t2, (SELECT expr2 FROM t3) AS q3和(SELECT expr3 FROM t4). Calcite中使用SqlValidatorNamespace表示Namespace, 它的类继承图如下. Calcite SQL验证流程在SQL语句中, DDL语句是不需要验证的, DQL和DML语句都需要验证. 由于各类语句的验证流程在细节上存在差别, 这里以最常见的SELECT语句为例, 讲述其验证过程. 整个SELECT语句验证流程中的函数调用链如下.12345678910111213141516171819202122232425SqlValidator.validate() // SQL验证入口 SqlValidatorImpl.validate() // Calicte提供的SqlValidator默认实现 SqlValidatorImpl.validateScopedExpression() SqlValidatorImpl.performUnconditionalRewrites() // 对SQL语句进行重写 SqlValidatorImpl.registerQuery() // 注册Scope和Namespace SqlSelect.validate() // 开始验证SELECT语句 SqlValidatorImpl.validateQuery() SqlValidatorImpl.validateNamespace() AbstractNamespace.validate() SelectNamespace.validateImpl() SqlValidatorImpl.validateSelect() SqlValidatorImpl.validateFrom() SqlValidatorImpl.validateQuery() SqlValidatorImpl.validateNamespace() AbstractNamespace.validate() IdentifierNamespace.validateImpl() IdentifierNamespace.resolveImpl() DelegatingScope.resolveTable() EmptyScope.resolveTable() EmptyScope.resolve_() // 在这个函数中判断表名是否在Schema中 SqlValidatorImpl.validateWhereClause() SqlValidatorImpl.validateGroupClause() SqlValidatorImpl.validateHavingClause() SqlValidatorImpl.validateWindowClause() SqlValidatorImpl.validateSelectList() 在验证流程中有三个主要的函数: SqlValidatorImpl.performUnconditionalRewrites(), SqlValidatorImpl.registerQuery()和SqlSelect.validate(), 下文将对他们进行详细的说明. SQL重写SQL重写用于将解析阶段得到的解析树重写为统一的格式, 方便下一步的处理. 由于SqlValidatorImpl.performUnconditionalRewrites()函数的实现十分复杂, 这里也就不详细分析重写的过程了. 而是通过两个具体的例子来说明重写的效果. 第一个例子是带有ORDER BY的SELECT语句, 在解析阶段这类语句会被解析为SqlOrderBy节点, 不过SqlOrderBy是一个纯语法节点, 在重写阶段会转化为SqlSelect节点. 其实我们看一下这两个类的定义就能明白这种转化是如何进行的, 核心代码如下. 对于SELECT ... ORDER BY ...语句, 在解析阶段生成的SqlOrderBy节点中query变量保存了SELECT ...部分的解析结果, 它其实是一个SqlSelect实例, ORDER BY后面的字段列表保存在orderList中. 在重写之后, SqlOrderBy中的orderList被移动到SqlSelect的orderBy中.1234567891011121314151617public class SqlOrderBy extends SqlCall { public final SqlNode query; public final SqlNodeList orderList; ...}public class SqlSelect extends SqlCall { SqlNodeList keywordList; SqlNodeList selectList; SqlNode from; SqlNode where; SqlNodeList groupBy; SqlNode having; SqlNodeList windowDecls; SqlNodeList orderBy; ...} 第二个例子是DELETE语句, 在解析阶段这类语句会被解析为SqlDelete节点, 其关键属性如以下代码所示. 在解析阶段生成的SqlDelete中sourceSelect为null, 在重写阶段会生成sourceSelect用于指示如何从表中查询需要删除的记录. 比如DELET FROM users WHERE id &gt; 1对应的sourceSelect就是SELECT * FROM users WHERE id &gt; 1.123456public class SqlDelete extends SqlCall { SqlNode targetTable; // 目标表 SqlNode condition; // 过滤条件 SqlSelect sourceSelect; // 指示如何查询要删除的记录 ...} 通过以上例子可以看到重写阶段所作的工作就是对解析树进行一些轻微的调整, 一般情况下这一阶段也不需要任何改动, 只要大概了解其流程即可. 其实如果在解析阶段就生成标准的格式, 就不需要重写了, 只不过这样会让解析器的代码变得冗长, 这应该也是Calcite把一些重写工作放到验证阶段的原因. 注册Scope和Namespace在真正进行验证之前, 还需要调用SqlValidatorImpl.registerQuery()注册Scope和Namespace. 在这一步Calcite会先遍历一遍整个SQL语句, 为解析树的各个部分生成对应的Scope和Namespace. 解析的结果保存在SqlValidatorImpl的成员变量中, 以下是与解析结果相关的核心代码.123456789public class SqlValidatorImpl implements SqlValidatorWithHints { protected final Map&lt;SqlNode, SqlValidatorScope&gt; scopes = new IdentityHashMap&lt;&gt;(); private final Map&lt;IdPair&lt;SqlSelect, Clause&gt;, SqlValidatorScope&gt; clauseScopes = new HashMap&lt;&gt;(); protected final Map&lt;SqlNode, SqlValidatorNamespace&gt; namespaces = new IdentityHashMap&lt;&gt;(); ...} SQL语句验证注册完Scope和Namespace之后就可以调用SqlNode.validate()方法进行验证了, 这里Calcite也使用了Visitor模式. 如果读者不了解什么是Visitor模式, 可以参考笔者之前的博文. 简单来说, Visitor模式就是把实现逻辑集中到一个类中, 在这里就是SqlValidatorImpl. 我们可以看下SqlSelect.validate()的实现, 它并没有实现具体的逻辑, 而是调用了传入的SqlValidator的validateQuery方法, 也就是SqlValidatorImpl.validateQuery. 这样带来的一个好处是, 如果我们需要修改验证逻辑, 只需要对SqlValidatorImpl进行修改, 而不需要修改SqlNode的实现类.1234567891011121314public abstract class SqlNode implements Cloneable { public abstract void validate(SqlValidator validator, SqlValidatorScope scope); ...}public class SqlSelect extends SqlCall { public void validate(SqlValidator validator, SqlValidatorScope scope) { validator.validateQuery(this, scope, validator.getUnknownType()); }}public interface SqlValidatorNamespace { void validate(RelDataType targetRowType);} SqlValidatorImpl.validateQuery是SELECT语句验证的真正入口, 它主要对Scope和Namespace进行了验证, 整个调用逻辑比较复杂, 存在很多递归调用. 如果读者有兴趣可以根据上述调用链逐步调试, 这里不再详细分析. 总结本文梳理了Calcite SQL验证相关的概念和流程, 由于这部分的代码较为复杂, 存在大量的递归调用, 所以本文也没有详细的分析所有细节. 好在SQL验证这部分不太需要进行修改, 多数情况下只需要理解其整体流程即可. 如果要对扩展的SQL语法进行验证, 可以建立一个统一的基类, 让扩展的SQL语法节点都继承该类, 这样在验证阶段可以对这类节点单独进行验证, 并不一定要修改SqlValidatorImpl类, 从而简化工作.","link":"/apache-calcite-sql-validator.html"},{"title":"拜占庭将军问题 (The Byzantine Generals Problem)","text":"拜占庭将军问题(The Byzantine Generals Problem)提供了对分布式共识问题的一种情景化描述, 由Leslie Lamport等人在1982年首次发表. 本文首先以插图的形式描述拜占庭将军问题, 最后在理解拜占庭将军问题的基础上对现有的分布式共识算法进行分类. Leslie Lamport等人的论文提供了两种解决拜占庭将军问题的算法： 口信消息型解决方案(A solution with oral message); 签名消息型解决方案(A solution with signed message). 本文之后将详细讲述这两种算法. 事实上, 拜占庭将军问题是分布式系统领域最复杂的容错模型, 它描述了如何在存在恶意行为(如消息篡改或伪造)的情况下使分布式系统达成一致. 是我们理解分布式一致性协议和算法的重要基础. 拜占庭将军问题描述拜占庭将军问题描述了这样一个场景: 图1. 拜占庭将军问题 拜占庭帝国(Byzantine Empire)军队的几个师驻扎在敌城外, 每个师都由各自的将军指挥. 将军们只能通过信使相互沟通. 在观察敌情之后, 他们必须制定一个共同的行动计划, 如进攻(Attack)或者撤退(Retreat), 且只有当半数以上的将军共同发起进攻时才能取得胜利. 然而, 其中一些将军可能是叛徒, 试图阻止忠诚的将军达成一致的行动计划. 更糟糕的是, 负责消息传递的信使也可能是叛徒, 他们可能篡改或伪造消息, 也可能使得消息丢失. 为了更加深入的理解拜占庭将军问题, 我们以三将军问题为例进行说明. 当三个将军都忠诚时, 可以通过投票确定一致的行动方案, 图2展示了一种场景, 即General A, B通过观察敌军军情并结合自身情况判断可以发起攻击, 而General C通过观察敌军军情并结合自身情况判断应当撤退. 最终三个将军经过投票表决得到结果为进攻:撤退=2:1, 所以将一同发起进攻取得胜利. 对于三个将军, 每个将军都能执行两种决策(进攻或撤退)的情况下, 共存在6中不同的场景, 图2是其中一种, 对于其他5中场景可简单地推得, 通过投票三个将军都将达成一致的行动计划. 图2. 三个将军均为忠诚的场景 当三个将军中存在一个叛徒时, 将可能扰乱正常的作战计划. 图3展示了General C为叛徒的一种场景, 他给General A和General B发送了不同的消息, 在这种场景下General A通过投票得到进攻:撤退=1:2, 最终将作出撤退的行动计划; General B通过投票得到进攻:撤退=2:1, 最终将作出进攻的行动计划. 结果只有General B发起了进攻并战败. 图3. 二忠一叛的场景 事实上, 对于三个将军中存在一个叛徒的场景, 想要总能达到一致的行动方案是不可能的. 详细的证明可参看Leslie Lamport的论文. 此外, 论文中给出了一个更加普适的结论: 如果存在m个叛将, 那么至少需要3m+1个将军, 才能最终达到一致的行动方案. 解决方案Leslie Lamport在论文中给出了两种拜占庭将军问题的解决方案, 即口信消息型解决方案(A solution with oral message)和签名消息型解决方案(A solution with signed message). 口信消息型解决方案首先, 对于口信消息(Oral message)的定义如下: A1. 任何已经发送的消息都将被正确传达; A2. 消息的接收者知道是谁发送了消息; A3. 消息的缺席可以被检测. 基于口信消息的定义, 我们可以知道, 口信消息不能被篡改但是可以被伪造. 基于对图3场景的推导, 我们知道存在一个叛将时, 必须再增加3个忠将才能达到最终的行动一致. 为加深理解, 我们将利用3个忠将1个叛将的场景对口信消息型解决方案进行推导. 在口信消息型解决方案中, 首先发送消息的将军称为指挥官, 其余将军称为副官. 对于3忠1叛的场景需要进行两轮作战信息协商, 如果没有收到作战信息那么默认撤退. 图4是指挥官为忠将的场景, 在第一轮作战信息协商中, 指挥官向3位副官发送了进攻的消息; 在第二轮中, 三位副官再次进行作战信息协商, 由于General A, B为忠将, 因此他们根据指挥官的消息向另外两位副官发送了进攻的消息, 而General C为叛将, 为了扰乱作战计划, 他向另外两位副官发送了撤退的消息. 最终Commanding General, General A和B达成了一致的进攻计划, 可以取得胜利. 图4. 指挥官为忠将的场景 图5是指挥官为叛将的场景, 在第一轮作战信息协商中, 指挥官向General A, B发送了撤退的消息, 但是为了扰乱General C的决定向其发送了进攻的消息. 在第二轮中, 由于所有副官均为忠将, 因此都将来自指挥官的消息正确地发送给其余两位副官. 最终所有忠将都能达成一致撤退的计划. 图5. 指挥官为叛将的场景 如上所述, 对于口信消息型拜占庭将军问题, 如果叛将人数为m, 将军人数不少于3m+1, 那么最终能达成一致的行动计划. 值的注意的是, 在这个算法中, 叛将人数m是已知的, 且叛将人数m决定了递归的次数, 即叛将数m决定了进行作战信息协商的轮数, 如果存在m个叛将, 则需要进行m+1轮作战信息协商. 这也是上述存在1个叛将时需要进行两轮作战信息协商的原因. 签名消息型解决方案同样, 对签名消息的定义是在口信消息定义的基础上增加了如下两条: A4. 忠诚将军的签名无法伪造，而且对他签名消息的内容进行任何更改都会被发现; A5. 任何人都能验证将军签名的真伪.基于签名消息的定义, 我们可以知道, 签名消息无法被伪造或者篡改. 为了深入理解签名消息型解决方案, 我们同样以3三将军问题为例进行推导. 图6是忠将率先发起作战协商的场景, General A率先向General B, C发送了进攻消息, 一旦叛将General C篡改了来自General A的消息, 那么General B将将发现作战信息被General C篡改, General B将执行General A发送的消息. 图6. 忠将率先发起作战协商 图7是叛将率先发起作战协商的场景, 叛将General C率先发送了误导的作战信息, 那么General A, B将发现General C发送的作战信息不一致, 因此判定其为叛将. 可对其进行处理后再进行作战信息协商. 图7. 叛将率先发起作战协商 签名消息型解决方案可以处理任何数量叛将的场景. 论文简介Leslie Lamport等人的论文‘The Byzantine Generals Problem’的提纲如下: 1.Introduction: 介绍了拜占庭将军问题; 2.Impossibility Results: 通过反正法证明了, 三将军问题对于口信消息是无解的; 3.A solution with oral message: 介绍了口信消息型拜占庭将军问题的解决方案; 4.A solution with signed message: 介绍了签名消息型拜占庭将军问题的解决方案; 5.Missing communication paths: 讲述了在通信小时情况下的拜占庭将军问题; 6.Reliable systems: 讲述了如何通过拜占庭将军问题构建可靠的系统; 7.Conclution: 总结. 总 结在分布式系统领域, 拜占庭将军问题中的角色与计算机世界的对应关系如下: 将军, 对应计算机节点; 忠诚的将军, 对应运行良好的计算机节点; 叛变的将军, 被非法控制的计算机节点; 信使被杀, 通信故障使得消息丢失; 信使被间谍替换, 通信被攻击, 攻击者篡改或伪造信息. 如上文所述, 拜占庭将军问题提供了对分布式共识问题的一种情景化描述, 是分布式系统领域最复杂的模型. 此外, 它也为我们理解和分类现有的众多分布式一致性协议和算法提供了框架. 现有的分布式一致性协议和算法主要可分为两类: 一类是故障容错算法(Crash Fault Tolerance, CFT), 即非拜占庭容错算法, 解决的是分布式系统中存在故障, 但不存在恶意攻击的场景下的共识问题. 也就是说, 在该场景下可能存在消息丢失, 消息重复, 但不存在消息被篡改或伪造的场景. 一般用于局域网场景下的分布式系统, 如分布式数据库. 属于此类的常见算法有Paxos算法, Raft算法, ZAB协议等. 一类是拜占庭容错算法, 可以解决分布式系统中既存在故障, 又存在恶意攻击场景下的共识问题. 一般用于互联网场景下的分布式系统, 如在数字货币的区块链技术中. 属于此类的常见算法有PBFT算法, PoW算法.","link":"/byzantine-generals-problem.html"},{"title":"数据库事务系列-本地事务模型基础","text":"事务, 或者更严格地说数据库事务, 可能是数据库理论中最难理解的术语之一. 要真正理解事务的内涵和其外延, 最重要的是思考和理解以下问题: 为什么会出现事务? 事务是一种自然法则还是人为创造的? 什么是事务, 或者说事务的定义是什么? 事务有什么样的特性? 能满足什么样的应用场景? 事务是否存在劣势? 有没有弥补方法? 这篇博文分为三个部分: 第一部分介绍事务的定义; 第二部分介绍事务的ACID特性; 第三部分介绍与事务中隔离性相关的概念, 主要包括常见的4中隔离级别. 读者需要注意的是, 本文所描述的事务是指本地事务, 也就是指在单一数据节点中对单一数据库资源的访问控制, 分布式事务需要在本地事务的基础之上引入额外的协议和协调机制, 本文并不涉及. 此外, 本文所描述的事务是概念层面上的, 并不涉及具体的实现方法, 不同数据库的事务实现方法可能放在之后的博文中介绍. 总的来说, 关系型数据库都完美的支持了本文所描述的事务模型. 事务定义首先来看文章开头的第一个问题: 为什么会出现事务? 在现实生活中有很多行为需要实现一系列操作, 并且要求这一系列操作要么全部执行成功, 要么全部不执行, 不能存在其中几个操作执行成功, 其他操作执行失败的情况, 比如: 从A账户转1万块钱到B账户, 需要执行两个操作: 先在A账户扣除1万, 再给B账户加1万. 这两个操作要么全部执行成功要么全部不执行. 不能说在A账户扣除1万这个操作执行成功, 但是给B账户加1万这个操作执行失败. 销售一件商品时, 首先用户购物车商品删除, 然后在销售订单里增加订单信息, 再者需要将销售商品库存减1. 评论增加积分操作, 首先要在评论表里面增加一条评论信息, 再者需要在积分表里面增加相应的积分. 正是因为现实生活中存在很多行为需要实现一系列操作, 这些操作要么全部执行成功, 要么全部不执行这个需求, 才使得事务应运而生. 事务将应用程序的多个读, 写操作捆绑在一起成为一个逻辑单元, 即事务中的所有读写是一个执行的整体, 整个事务要么全部成功, 要么全部失败, 不存在部分成功或失败的场景. 到这里我们也可以知道, 事务并不是天然存在的东西, 它是被人为创造出来的, 其目的是简化应用层的编程模型. 在一个高可靠的数据存储环境中, 存在许多可能出错的场景. 通常可将其分为两类, 一类是故障性问题, 比如由软硬件故障或网络故障引起的程序中断; 另一类是由并发带来的竞争条件, 即如何进行并发控制. 事务的出现正是为了解决类似上述场景中可能出现的问题, 将所有的问题都屏蔽在数据库层面, 这样应用层就不需要担心部分失败的情况, 并且可以安全地重试. 我们可以给事务下一个正式的定义: 事务是指数据库中一个不可分割的逻辑工作单元, 它允许你将许多个操作表示为一个步骤. 事务执行的操作包括读取和写入数据记录. 通常意义上的事务针对的是多个对象, 即将多个读写操作聚合为一个逻辑执行单元. 并对这个逻辑单元提供可靠的执行保证, 体现在原子性(要么全部成功, 要么全部失败), 隔离性(防止产生竞争条件). 而存储引擎对单个对象操作的可靠执行保证几乎是必须的, 通常基于日志恢复来实现原子性, 基于对象锁来实现隔离性, 甚至某些数据库还提供了更加高级的原子操作, 如原子自增操作, 原子比较-设置操作. 值的注意的是, 虽然单对象操作有时也被称为”轻量级事务”, 但是它们并不是通常意义上的事务. 多对象事务在关系型数据库中几乎是必备的, 然而许多分布式数据存储系统并不支持多对象事务, 例如HBase仅支持单行事务和Region级别的跨行事务. 当然, 并非在分布式环境下难以实现多对象事务, 在分布式环境下也有分布式协议可以实现分布式事务, 这些分布式存储系统不支持多对象事务大多是基于性能的考虑. ACID特性了解了事务的含义, 我们再来看事务的特性. 数据库事务必须遵守原子性(Atomicity), 一致性(Consistency), 隔离性(Isolation)和持久性(Durability), 也就是我们常说的ACID特性. 虽然ACID特性早已被大家所熟知, 但是这四个词在计算机的不同领域有着不同的含义. 这一节将逐一剖析ACID中各个词的具体含义. 原子性: 事务所包含的一组操作是不可分的, 事务中所有的操作要么全部执行成功, 要么全都不执行. 事务中的操作不应当被部分应用, 每个事务要么成功提交(使事务内写操作产生的所有更改变得可见), 要么中止(回滚所有尚不可见的事务副作用). 中止后可以重试该事务. 原子性一词常出现在并发编程中, 如果某个线程执行一个原子操作, 这意味着其他线程无法看到该操作的中间结果. 它只能处于操作之前或操作之后的状态, 而不是两者之间的状态. 然而, ACID中的原子性与多个操作的并发性无关, 这实际上是由隔离性所定义的. ACID中的原子性目的在于屏蔽一个包含多个写操作的请求中可能出现的故障性问题. 例如在完成了一部分写入之后, 发生了故障. 如果没有原子性保证, 则必须由应用层记录哪些操作已经生效, 哪些没有生效, 这将使得应用层的业务逻辑格外复杂. 原子性则大大简化了这个问题: 只要事务没有成功提交, 应用程序可以确保没有发生任何更改, 所以可以安全地重试. 从中我们可以看出, ACID中原子性的特征是: 在出错时中止事务, 并将部分完成的写入丢弃. 也许在这里使用可中止性比原子性更加准确. 一致性: 事务只能将数据库从一个有效状态转移到另一个有效状态, 而不能打破与应用程序业务相关的各种约束, 例如: 银行账户之间无论如何转账其总额不变等现实约束; 性别最多只有男, 女, 跨性别者三种选项等完整性约束. 一致性同样在计算机的不同领域有不同的含义. ACID中的一致性主要是指对数据有特定的预期状态, 任何数据更改必须满足这些状态约束. 值的注意的是, 不同于原子性, 隔离性和持久性完全由数据库实现, 一致性虽然在一定程度上依赖数据库实现(原子性, 持久性, 隔离性也是为了保证一致性), 但其主要由应用程序实现, 应用程序有责任定义正确的事务来保持一致性. 隔离性: 多个并发执行的事务应当能够互不干扰地运行, 每个事务都像没有其他事务在同时执行一样. 隔离性定义了何时以及哪些对数据库的状态的更改可以对并发事务可见. 隔离性是ACID中最复杂的一个特性, 它定义了数据库中多个事务并发执行时的可见性规则(根据经验, 一旦涉及到并发, 问题似乎就变得复杂了). 为了更好的性能, 数据库通常提供了多种隔离级别, 每个隔离级别提供了不同程度的并发事务可见性保证, 本文将在隔离级别这一小节进行详细描述. 从技术角度讲, 每种隔离级别都需要不同的技术手段来保证, 通常来说涉及各种锁和MVCC机制, 后面两篇文章会重点解释隔离性. 持久性: 一旦事务被提交, 所有数据库状态的修改都必须被持久化到磁盘上, 即使之后发生故障性问题, 如断电, 系统故障或崩溃等也不受影响. 持久性通常意味着数据已经被写入非易失性存储, 如硬盘或SSD. 当然, 事实上并不存在完美的持久性, 如果所有磁盘都被销毁, 那么数据库也无能为力. 现在, 我们对ACID有个准确的认识, 并可以作如下总结: 原子性主要用于解决可能遇到的故障性问题, 如断电, 系统崩溃等; 隔离性主要用于解决并发问题, 并且存在多种隔离级别, 提供了不同程度的并发保证; 持久性几乎是数据库必备的特性, 如果没有持久性那么数据库也就没有了存在的必要; 一致性更多的是一种应用层的属性, 数据库很难检测到业务上的不一致, 而原子性, 隔离性, 持久性则是数据库本身的属性, 这些属性可以帮助应用层更好地保证一致性. 隔离级别事务隔离级别主要保证ACID中的I, 即隔离性. 上文中提到过, 数据库通常都提供了多种隔离级别, 以适用于不同的业务需求. 然而, 差强人意的是各个数据库之间所实现的隔离级别都存在微小的差异. 所以, 我们也没必要纠结于隔离级别的具体定义, 重点是要了解在并发事务下可能出现的异常, 及其相应的解决方案. 本文讲述常用的4中隔离级别以及它们能避免的异常, 在使用具体数据库时关注其可能存在的细微差别即可. 读未提交读未提交是最弱的一种隔离级别, 它几乎不提供任何保证. 从字面意思也可以看出, 它允许事务读取到其他并发事务未提交的更改, 这种现象称为脏读. 如下图所示, 开始时, 1号事务和2号事务看到的A都是A0, 接着1号事务将A0更新为A1, 再接着2号事务就读到A1新值, 然而1号事务将A1回滚回了A0. 这样1号事务就读到了一个不存在的值A1, 这就是脏读. 读已提交读已提交和读未提交是相对的, 读已提交表示事务只能读到其他并发事务已经提交的结果. 也就是说1号事务可以看到2号事务提交之后的结果, 但是看不到2号事务提交之前对任何数据的更改. 这样读已提交就避免了脏读, 但是还存在一个问题: 不可重复读. 上图中2号事务在1号事务更新完成之后提交之前读取A的值依然是A0, 避免了脏读; 但在1号事务提交之后再次读取时发现读到的值变成了A1, 出现了不同时间点对同一数据进行多次读取, 会读到不同的值的现象. 这种现象就称为不可重复读. 可重复读从字面意思中就可看出, 可重复读修复了读已提交隔离级别中存在的不可重复读问题. 如下图所示, 无论1号事务如何更新A, 2号事务在随后的进程中看到的A值都是事务开始第一次看到的A值, 即A0. 可重复读隔离级别虽然解决了不可重复读的问题, 但是还存在一个问题: 幻读. 上述三种隔离级别解决的都是对单个对象(或单行)的并发控制, 而幻读是在对多个对象(多行)进行操作时出现的问题. 以下图为例, 1号事务在事务过程中插入了一个大于B0的新值B2, 2号事务在插入操作前后读取B&gt;0的时候读到的值却不同. 可串行化可串性化是最严格的一种隔离级别, 要求有读写冲突的事务必须严格串行执行. 如下图所示, 2号事务要读取1号事务修改的记录A, 这就导致2号事务必须等待1号事务提交之后才能开启执行. 通过这种形式可以避免之前所提到脏读, 不可重复读和幻读. 虽说如此, 几乎所有数据库业务都不会开启这种隔离级别, 因为这会带来严重的锁冲突. 不同隔离级别为并发事务提供了不同的保证, 总结如下. 脏读 不可重复读 幻读 读未提交 允许 允许 允许 读已提交 - 允许 允许 可重复读 - - 允许 可串行化 - - - 总结上文已经回答了博文开头的前三个问题, 那么第四个问题呢, 事务有什么劣势? 其实答案已经隐含在前文中了. 事务的劣势就是它所带来的性能开销, 这也是数据库需要实现多种隔离级别的原因, 越弱的隔离级别其性能通常越好. 另外, 分布式事务的实现通常具有更高的复杂度和性能开销, 因此很多数据库都选择放弃了强一致性的分布式事务. 本文主要讲述了本地数据库事务的概念和其ACID特性, 重点分析了不同隔离级别所解决的事务并发问题. 正如文中所说, 不同地方对事务, ACID和隔离级别的定义都不尽相同, 我们也不必纠结与具体概念, 只要理解其本质内涵, 知道其能解决什么现实问题即可. 对于各个数据库在隔离级别方面的差异, 在使用具体数据库时再查阅即可. 参考[1] 纯干货 | 一篇讲透如何理解数据库并发控制[2] 开发者都应该了解的数据库隔离级别[3] Designing Data-Intensive Application[4] Database Internals: A Deep Dive into How Distributed Data Systems Work","link":"/local-transaction.html"},{"title":"Apache Calcite查询优化器之VolcanoPlanner","text":"本文已收录在合集Apche Calcite原理与实践中. 本文是Apache Calcite原理与实践系列的第七篇. 上一篇文章介绍了Calcite中的规则优化器HepPlanner, 本文将介绍成本优化器VolcanoPlanner. VolcanoPlanner是Volcano/Cascades风格的优化器, 支持基于成本的搜索, 并具有良好的扩展性. 本文首先介绍VolcanoPlanner中相关的概念和数据结构, 之后介绍现有的两种优化算法. 由于VolcanoPlanner的实现逻辑较为复杂, 包含了两种优化算法, 特别是Top-Down优化算法引入了较多的概念. 本文已经尽可能在厘清整体执行逻辑的基础上涵盖更多实现细节, 然而仍不可能面面俱到, 不过相信读者在理解了本文所讲述的内容之后, 可以轻松阅读源码理解本文未曾涉及的细节. VolcanoPlanner概念和数据结构RelSet和RelSubsetRelSet和RelSubset是VolcanoPlanner专用的数据结构, 由SQL得到的RelNode树需要转换为RelSet和RelSubset结构才能被VolcanoPlanner优化. RelSet是一组逻辑上等价的关系代数表达式集合, RelSubset(继承自AbstractRelNode)是一组逻辑上等价且物理属性相等的关系代数表达式集合. 一个RelSet中一般会包含多个逻辑上等价但物理属性不同的RelSubset. RelSet和RelSubset的核心成员如下. 123456789101112131415161718192021222324public class RelSubset extends AbstractRelNode { // 当前RelSubset下已知的最低成本 RelOptCost bestCost; // 当前RelSubset所属的RelSet final RelSet set; // 当前RelSubset下最低成本对应的表达式, 一般是物理表达式, 如EnumerableRel @Nullable RelNode best;}class RelSet { // 属于当前RelSet的所有RelNode(非RelSubset) final List&lt;RelNode&gt; rels = new ArrayList&lt;&gt;(); // 非RelSubset的RelNode类型, 且存在孩子节点是属于当前RelSet的RelSubset final List&lt;RelNode&gt; parents = new ArrayList&lt;&gt;(); // 当前RelSet所包含的所有RelSubset final List&lt;RelSubset&gt; subsets = new ArrayList&lt;&gt;(); @MonotonicNonNull RelSet equivalentSet; // 第一个加入该RelSet的RelNode, 由于最先注册的都是逻辑RelNode, // 因此该变量保存的都是LogicalXXX. @MonotonicNonNull RelNode rel;} 为了更好地理解RelNode树与RelSet和RelSubset结构的对应关系, 我们以一个具体的示例来进一步说明. 这里的示例表来自Calcite源码测试中常用的HrClusteredSchema, 其SQL语句对应的RelNode树及转换后的RelSet和RelSubset结构如下图所示. 图中出现的AbstractConverter会在下一小节进一步介绍, 这里可暂时忽略. RelNode树会首先被转化为对应的RelSet和RelSubset结构. 之后在优化过程中, 会根据给定的Convention和规则产生更多等价的RelSet和RelSubset, 如上图最右侧所示. 最终在其中选出代价最低的节点组成执行计划, 具体流程下文会进一步讲述. AbstractConverterAbstractConverter是一种特殊的节点, 用于对节点的物理属性进行转换, 它也是一种RelNode类型, 其继承关系如下图所示. VolcanoPlanner最终生成的是物理执行计划, 因此在优化过程中需要考虑各个节点的物理属性, 子节点的物理属性必须满足父节点的输入需要, 例如对于Merge Join而言, 输入的左右表必须按Join Key排序, 否则就需要在不满足排序属性的表之上增加一个排序算子. 为了实现这一点, 目前在Calcite中有两种方法: 在基于迭代的优化算法中, 如果子节点的物理属性不满足父节点输入需要, 便会在中间添加一个新的AbstractConverter, 它会跟ExpandConversionRule相匹配, 这样在后续规则应用阶段会触发该规则, 并调用RelTraitDef.convert()将子节点的物理属性进行转换以满足父节点输入需求. 在Top-Down优化算法中, 除根节点外, 父节点的物理属性可通过passThroughTraits()方法传递给子节点, 这样在子节点优化过程中产生的物理算子的物理属性如果不满足父节点需求, 便会调用对应Convention的enforce()方法进行物理属性的转换, 而不需要添加AbstractConverter. 同样, 为了更好地说明AbstractConverter到底是如何使用的, 我们还是用一个具体的例子来一探究竟. 这个例子十分简单, 对应的SQL语句是SELECT * FROM emps, 翻译成关系算子就是一个LogicalTableScan算子. 我们分几种情况来看AbstractConverter是如何起作用的: 当物理计划仅需满足EnumerableConvention.INSTANCE这一物理属性时, 在准备阶段会在RelSet中添加一个AbstractConverter(其traitSet属性为[EnumerableConvention]), 它是由VolcanoPlanner.ensureRootConverters()添加的, 用于保证根节点能转换到所需的物理属性. 不过在这个例子中, 所添加的AbstractConverter其实没有起到任何作用, 因为ConventionTraitDef.convert()只会对isGuaranteed()方法返回true的ConverterRule起作用, 而转换到EnumerableRel的ConverterRule其isGuaranteed()方法固定返回false. Convention这一物理属性的转换(即从LogicalTableScan到EnumerableTableScan的转换)实际是由注册到VolcanoPlanner的ConverterRule(即EnumerableRules.ENUMERABLE_TABLE_SCAN_RULE)来实现的. 现在我们来看一种稍微复杂一点的情况, 假设我们需要输出数据按第1列(从0开始计数)排序, 这样整个物理计划的根节点的物理属性就变成了[EnumerableConvention.INSTANCE, RelCollations.of(1)]. 此时由于由两个物理属性需要转换, 在准备阶段VolcanoPlanner.ensureRootConverters()就不会添加任何AbstractConverter(原因下文会进一步解释). 在优化阶段, 两种优化算法会有不同的行为: 对于迭代算法, 在生成EnumerableTableScan后注册该节点时, 会发现该节点无法满足RelCollationImpl [1]这一物理属性, 因此在调用RelSet.add()向RelSet添加该节点时会触发RelSet.getOrCreateSubset()-&gt;RelSet.addConverters()添加一个AbstractConverter, 后续在规则应用阶段展开ExpandConversionRule时会生成一个LogicalSort进行排序. 对于Top-Down算法, 由于enforce()代替了AbstractConverter的工作, 因此也就不需要再添加AbstractConverter节点了. 从上述例子中, 读者可能会对几个细节有所疑问: 为什么情况1中, 准备阶段在根节点所在RelSet中添加了一个AbstractConverter, 而情况2中没有. 这跟 ensureRootConverters()的执行逻辑有关, 它只在所需物理属性与根节点物理属性仅有一个不同时才会在根节点之上添加一个AbstractConverter. ensureRootConverters()的进一步解析可参考附录二. 可以看到在情况1中虽然添加了一个AbstractConverter来保证根节点的Convention属性转换, 但实际上这里的AbstractConverter并没有起作用. 在Calcite的代码中, Convention之间的转换都是通过ConverterRule来实现的. 而ConventionTraitDef.convert()是用来做多个Convention的连续转换的, 例如希望从A-&gt;C, 并且已经有了A-&gt;B, B-&gt;C两个ConverterRule, 且其isGuaranteed()方法返回true, 那么就可通过最短路径直接进行连续转换, 这在Apache Drill等框架中可能用到. 总结来说, 在以下几种情况下会添加AbstractConverter: 在准备阶段, 或者在优化阶段发生根节点合并的情况(见附录二), 会调用ensureRootConverters()尝试添加AbstractConverter; 在迭代优化算法中, 如何新产生节点的物理属性不符合父节点要求, 那么会添加一个AbstractConverter来进行物理属性的转换. Cost Model在Calcite中, 一个算子的物理执行成本由RelOptCost描述, VolcanoPlanner中默认使用的是VolcanoCost, 包含算子执行的CPU, IO成本以及中间结果的行数. 在分布式系统中, 算子执行的成本通常还包括数据在不同机器间的传输成本, 即网络成本. 对于这种情况可以继承RelOptCost实现一个新的成本模型, 并创建一个继承于RelOptCostFactory的工厂类用于构造自定义成本, 在构造VolcanoPlanner时传入该工厂类即可. RelOptCost的计算需要借助各种统计值, 如表的行数, 谓词的Selectivity等. 在Calcite中这种统计值都由Metadata表示, 下图列举了几种常见的类型, 完整的统计值类型都在BuiltInMetadata中. Metadata接口的子类型都是接口, 实际获取或计算统计值的逻辑都在Handler中, 它继承于MetadataHandler, 每个Metadata中都有一个Handler. MetadataHandler有两种实现: 一种是可用于计算具体的RelNode算子统计值的实现, 例如用于获取行数的RelMdRowCount, 具有计算各种类型RelNode算子行数的实现, 如: Double getRowCount(Union rel, RelMetadataQuery mq) Double getRowCount(Filter rel, RelMetadataQuery mq) 一种是各个Metadata子接口中的Handler接口, Handler接口中只有一个用于获取RelNode算子对应统计值的方法, 例如RowCount中的方法为: Double getRowCount(RelNode r, RelMetadataQuery mq). Handler是供RelMetadataQuery调用的接口, 它的实现类是动态生成的, 例如RowCount.Handler对应的生成类是GeneratedMetadata_RowCountHandler, 代码可见附录一, 其核心逻辑就是将传入的RelNode转化为具体的实现类型, 并调用RelMdRowCount中对应的方法. 由于MetadataHandler的具体实现较多, Calcite又提供了RelMetadataProvider来对MetadataHandler进行统一的创建和管理. 其实现类如下图所示, 其中: ReflectiveRelMetadataProvider用于辅助创建具体的RelMetadataProvider, 例如用于创建RelMdRowCount类型的RelMetadataProvider. DefaultRelMetadataProvider包含由ReflectiveRelMetadataProvider创建的RelMetadataProvider集合. JaninoRelMetadataProvider会在DefaultRelMetadataProvider的基础上, 通过动态代码生成来创建用于创建Handler实现类的RelMetadataProvider. RelMetadataQuery继承自RelMetadataQueryBase, 是Calcite中的的关系算子元数据查询接口标准, VolcanoPlanner是调用RelMetadataQuery中的方法进行成本计算的. RelMetadataQuery的创建在RelOptCluster中, 其核心代码如下, 可以看到RelMetadataQuery中包含的实际是JaninoRelMetadataProvider, 其中又持有DefaultRelMetadataProvider, 在RelMetadataQuery的具体方法中会调用JaninoRelMetadataProvider的方法动态生成一个Handler.12345678910111213141516171819202122232425262728293031323334353637383940414243public class RelOptCluster { RelOptCluster(RelOptPlanner planner, RelDataTypeFactory typeFactory, RexBuilder rexBuilder, AtomicInteger nextCorrel, Map&lt;String, RelNode&gt; mapCorrelToRel) { ... setMetadataProvider(DefaultRelMetadataProvider.INSTANCE); setMetadataQuerySupplier(RelMetadataQuery::instance); ... } public void setMetadataProvider( @UnknownInitialization RelOptCluster this, RelMetadataProvider metadataProvider) { this.metadataProvider = metadataProvider; this.metadataFactory = new org.apache.calcite.rel.metadata.MetadataFactoryImpl(metadataProvider); // 最终RelMetadataQuery中包含的是JaninoRelMetadataProvider RelMetadataQueryBase.THREAD_PROVIDERS .set(JaninoRelMetadataProvider.of(metadataProvider)); }}public class RelMetadataQuery extends RelMetadataQueryBase { protected RelMetadataQuery() { this(castNonNull(THREAD_PROVIDERS.get()), EMPTY.get()); } public /* @Nullable: CALCITE-4263 */ Double getRowCount(RelNode rel) { for (;;) { try { Double result = rowCountHandler.getRowCount(rel, this); return RelMdUtil.validateResult(castNonNull(result)); } catch (MetadataHandlerProvider.NoHandler e) { // 这里会调用JaninoRelMetadataProvider, // 基于其持有的DefaultRelMetadataProvider来动态生成一个RowCount.Handler实现类 rowCountHandler = revise(BuiltInMetadata.RowCount.Handler.class); } } }} 关于元数据获取这部分的实现逻辑比较绕, 既用到了反射, 又用到了动态代码生成. 之所以实现地这么复杂, 我认为有两个原因: 一是不希望手写if-else去为每个RelNode的实现类找到对应的计算统计值的方法. 二是为了有更好的扩展性, 成本计算中的统计值有比较强的扩展需求, 基于现在这套实现可以方便地进行扩展, 而不用修改现有代码. 有了上面的铺垫, 我们再来看下VolcanoPlanner中的成本计算过程, 其整体流程如下: 在VolcanoPlanner的propagateCostImprovements()中会触发成本计算, getCost()会递归调用RelMetadataQuery.getNonCumulativeCost()计算当前节点及其子节点的成本. RelMetadataQuery.getNonCumulativeCost()会计算当前节点的成本, 它会调用动态生成的类GeneratedMetadata_NonCumulativeCostHandler(具体代码见附录一)中的getNonCumulativeCost(). GeneratedMetadata_NonCumulativeCostHandler调用的是RelMdPercentageOriginalRows.getNonCumulativeCost(), 最终会调用RelNode.computeSelfCost()计算成本. RelNode的实现类有不同的computeSelfCost()实现, 例如TableScan会调用底层RelOptTable来获取行数, Calc会通过RelMetadataQuery.getRowCount()来获取行数. RuleDriverVolcanoPlanner支持不同的优化算法, RuleDriver是不同优化算法的抽象接口, 目前有两种实现, 其中: IterativeRuleDriver是基于迭代的优化算法; TopDownRuleDriver是Top-Down优化算法, 是目前State of the art的关系代数优化算法. 在RuleDriver内部, 由RuleQueue管理已经注册的规则. VolcanoPlanner优化流程VolcanoPlanner的优化流程较为复杂, 我们先来看看VolcanoPlanner中的核心成员, 理解这些核心成员中所存储的内容及其变化过程可帮助我们更好地理解VolcanoPlanner的优化过程.123456789101112131415161718192021222324public class VolcanoPlanner extends AbstractRelOptPlanner { // 以下成员来自来自AbstractRelOptPlanner // ---- // 用于保存VolcanoPlanner中已经注册的所有RelOptRule // 仅在addRule()方法中会添加新的映射 protected final Map&lt;String, RelOptRule&gt; mapDescToRule = new LinkedHashMap&lt;&gt;(); // 用于保存VolcanoPlanner待优化的RelNode树中所有的RelNode类型(包括RelNode和RelSubset) // 在两种情况下会向其中添加新的元素: // 一种是在初始化时, 在AbstractRelOptPlanner的构造函数中添加RelNode和RelSubset // 另一种是在调用changeTraits()或setRoot()时触发registerImpl(), 会调用registerClass()向其中添加每个RelNode的Class类型 private final Set&lt;Class&lt;? extends RelNode&gt;&gt; classes = new HashSet&lt;&gt;(); // 以下成员来自VolcanoPlanner // ---- // 用于保存VolcanoPlanner中所有的RelNode到对应的规则Pattern的映射 // 在两种情况下会向其中添加新的映射: // 一是在addRule()方法中 // 二是在registerClass()如果注册了一个新的类型会调用onNewClass(), 添加新的映射 private final Multimap&lt;Class&lt;? extends RelNode&gt;, RelOptRuleOperand&gt; classOperands = LinkedListMultimap.create();} VolcanoPlanner中存在两种优化算法, 分别是基于迭代的优化算法和Cascades风格的Top-Down优化算法. 这两种优化算法的准备工作都是相同的, 下面我们先介绍几个准备流程, 之后分别介绍两种优化算法的执行流程. 准备流程准备流程相当于数据结构的初始化, 后续的算法都是在准备阶段产出的数据结构上进行的. 我们可以从RuleSetProgram中看到使用VolcanoPlanner的步骤.123456789101112131415161718192021222324252627282930313233/** Program backed by a {@link RuleSet}. */static class RuleSetProgram implements Program { final RuleSet ruleSet; private RuleSetProgram(RuleSet ruleSet) { this.ruleSet = ruleSet; } @Override public RelNode run(RelOptPlanner planner, RelNode rel, RelTraitSet requiredOutputTraits, List&lt;RelOptMaterialization&gt; materializations, List&lt;RelOptLattice&gt; lattices) { planner.clear(); // 注册优化规则 for (RelOptRule rule : ruleSet) { planner.addRule(rule); } for (RelOptMaterialization materialization : materializations) { planner.addMaterialization(materialization); } for (RelOptLattice lattice : lattices) { planner.addLattice(lattice); } // 设置目标RelTrait if (!rel.getTraitSet().equals(requiredOutputTraits)) { rel = planner.changeTraits(rel, requiredOutputTraits); } // 设置根节点 planner.setRoot(rel); return planner.findBestExp(); }} 注册优化规则Calcite中无论是HepPlanner还是VolcanoPlanner, 在优化过程中都使用了规则. 不同之处在于, HepPlanner只是”无脑”按一定的顺序不断遍历整个RelNode树, 一旦发现某个子树节点符合规则所定义的Pattern就对其进行转换, 直到RelNode树在给定的规则集下不再发生变化或遍历次数达到上限; 而VolcanoPlanner的目标是搜索成本最小的计划, 它可以在多个不同的子计划中选出代价最小的, 更重要的是它可以支持生成指定Calling Convention的执行计划. 因此, 使用VolcanoPlanner的第一步也是挑选并添加所需规则. 这一步的实现在VolcanoPlanner中的addRule()方法中, 其代码如下.123456789101112131415161718192021222324252627282930313233343536373839public boolean addRule(RelOptRule rule) { if (locked) { return false; } // 这里调用父类的addRule()方法, 用于判断当前rule是否已经注册, // 如未注册则加入到mapDescToRule中 if (!super.addRule(rule)) { return false; } // Each of this rule&apos;s operands is an &apos;entry point&apos; for a rule call. // Register each operand against all concrete sub-classes that could match // it. for (RelOptRuleOperand operand : rule.getOperands()) { for (Class&lt;? extends RelNode&gt; subClass : subClasses(operand.getMatchedClass())) { // 在VolcanoPlanner中TransformationRule不支持匹配PhysicalNode, // 具体可见TransformationRule注释 if (PhysicalNode.class.isAssignableFrom(subClass) &amp;&amp; rule instanceof TransformationRule) { continue; } classOperands.put(subClass, operand); } } // If this is a converter rule, check that it operates on one of the // kinds of trait we are interested in, and if so, register the rule // with the trait. if (rule instanceof ConverterRule) { ConverterRule converterRule = (ConverterRule) rule; final RelTrait ruleTrait = converterRule.getInTrait(); final RelTraitDef ruleTraitDef = ruleTrait.getTraitDef(); if (traitDefs.contains(ruleTraitDef)) { ruleTraitDef.registerConverterRule(this, converterRule); } } return true;} 上述代码中需要注意的是, 如果在调用addRule()之前还没有调用过setRoot(), 那么在classes中并没有记录, 因此addRule()中并不会向classOperands添加记录. 后续在注册RelNode树时会向classOperands中添加RelNode匹配的规则Pattern. 注册RelNode在VolcanoPlanner中, 其优化过程是基于RelSet和RelSubset展开的, 因此在优化前需要将待优化的RelNode树转化为RelSet和RelSubset形式的表达. 以下是在准备阶段, RelNode树注册为RelSet和RelSubset的案例. 除了在准备阶段会将整棵待优化的RelNode树进行注册, 后续如果有规则触发后产生了新的RelNode类型, 也会进行注册. RelNode注册的主要实现逻辑在registerImpl()函数中, 其核心流程如下图所示. RelNode注册过程中的几个要点是: RelNode.onRegister()会为当前节点的每个子节点调用VolcanoPlanner.ensureRegistered()进行注册. registerClass()会将当前节点的类型加入classes中, 并调用onNewClass()从已经注册的规则中找到与当前节点匹配的规则, 并将其Pattern记录到classOperands中. addRelToSet()会为当前的RelNode创建一个对应的RelSubset(如果没有RelTrait相同的RelSubset的话), 或将当前的RelNode加入到已有的RelSubset中(如果已有RelTrait相同的RelSubset). 同时还会调用propagateCostImprovements()计算当前节点的Cost, 并将其传播给父节点. fileRules()会根据classOperands中注册的规则Pattern, 找到与当前节点匹配的规则, 并将其放入规则队列中, 后续算法会根据这一队列进行优化. 总结来说, 注册RelNode这个过程是准备阶段的核心, 在这一过程中, 会将与原始的待优化RelNode树匹配的规则放入相应的队列中. 后续的算法, 无论是基于迭代还是Top-Down优化算法都是基于队列中的规则展开的. 需要说明的是registerImpl()是一个私有方法, 注册RelNode的过程并不需要用户显式调用. 在设置目标RelTrait和设置根节点时, 都会间接调用registerImpl()注册RelNode, 下文会进一步介绍. 设置目标RelTrait由于VolcanoPlanner生成的是物理计划, 因此需要设置目标输出计划的物理属性, 一般来说Convention属性是必须要设置的, 在Calcite中如果要生成执行计划, 可将Convention设置为EnumerableConvention或BindableConvention. 一般地, 在使用VolcanoPlanner时只需为根节点调用changeTraits()即可. changeTraits()的实现逻辑比较简单, 首先调用ensureRegistered()确保当前节点已经注册, 最后根据需要的属性, 返回已有的RelSubset或创建一个新的RelSubset.123456789101112@Override public RelNode changeTraits(final RelNode rel, RelTraitSet toTraits) { assert !rel.getTraitSet().equals(toTraits); assert toTraits.allSimple(); // 确保当前RelNode已注册 RelSubset rel2 = ensureRegistered(rel, null); if (rel2.getTraitSet().equals(toTraits)) { return rel2; } return rel2.set.getOrCreateSubset(rel.getCluster(), toTraits, true);} 设置根节点在执行优化前的最后一步就是调用setRoot()设置根节点. 其实现逻辑也很简单, 首先会调用registerImpl()确保根节点已经注册; 其次由于根节点没有父节点, 因此会调用ensureRootConverters()尝试添加AbstractConverter用于保证输出的物理属性符合要求.123456789@Override public void setRoot(RelNode rel) { this.root = registerImpl(rel, null); if (this.originalRoot == null) { this.originalRoot = rel; } rootConvention = this.root.getConvention(); ensureRootConverters();} 优化算法在准备工作完成后, 即可调用findBestExp()执行优化算法, 搜索代价最小的执行计划. VolcanoPlanner中提供了两种优化算法, 默认使用的是基于迭代的优化算法, 可以调用VolcanoPlanner.setTopDownOpt(true)启用Top-Down优化算法. 基于迭代的优化算法基于迭代的优化算法实现比较简单, 所有与当前RelNode树及其子树匹配的规则都已经在准备阶段被放入了IterativeRuleQueue. 在优化阶段, 不断从队列中取出一个规则用于执行, 直到队列为空. 整体执行流程如下图所示: IterativeRuleQueue有两个队列, preQueue用于存放SubstitutionRule(即应用后一定能产生正向收益的规则, 如CalcRemoveRule, ProjectJoinRemoveRule等), queue用于存放其他规则. 在算法执行时会优先弹出preQueue中的规则. 在规则执行完成之后, 会调用VolcanoPlanner.ensureRegistered()注册规则产生的新节点, 在此过程中如果发现新节点与某个注册规则匹配, 即将匹配的规则再次加入到IterativeRuleQueue中. 为了更好地理解上述过程, 我们以一个具体的例子来进一步说明. 还是使用前文中双表Join的案例, 不过这里把输入的优化规则做了一些改动, 把原来的EnumerableRules.ENUMERABLE_JOIN_RULE改成了EnumerableRules.ENUMERABLE_MERGE_JOIN_RULE, 这样Join的物理算子就会从EnumerableHashJoin变成EnumerableMergeJoin. 整个优化过程就是不断从队列中取出规则并应用, 这些规则大部分是ConverterRule, 应用之后会产生一个物理算子. 唯一不同的是EnumerableMergeJoinRule, 它产生的算子EnumerableMergeJoin需要输入节点按Join Key排序, 因此需要添加一个AbstractConverter, 及与之匹配的ExpandConversionRule, 之后该规则触发时便会添加一个LogicalSort来满足排序属性. 读者可调试运行IterativeMergeJoinExample以获得更直观的感受. Top-Down优化算法Top-Down优化算法借鉴于Columbia的实现, 其本质上是一种自顶向下的记忆化动态规划算法, 由于递归解法效率低下, 因此Columbia定义了多种Task, 并利用栈消除递归, 优化的整体过程就是不断从栈顶取出需要执行的Task(在Task中可能生成新的Task放入栈中), 直到栈为空. 概念及数据结构Columbia衍生自Volcano/Cascades, 我们先介绍一下Calcite中与之对应的概念和具体实现. Volcano/Cascades概念 Calcite VolcanoPlanner实现 Operator(Logical/Physical) RelNode(没有LogicalRelNode基类, 但仍可区分Logical/Physical) Rule(Transformation/Implementation) RelOptRule(TransformationRule/ConverterRule) Enforcer rule ConverterRule Group(逻辑等价类) RelSet RelSubset Pattern 通过RelOptRuleOperand来实现 Property(Logical/Physical) RelTrait Calcite中Task的定义也基本可与Columbia对应, 为了方便实现做了一些轻微调整. Columbia Calcite O_GROUP OptimizeGroup O_INPUTS OptimizeInputsOptimizeInput1 O_EXPR OptimizeMExpr E_GROUP ExploreInput APPLY_RULE ApplyRules OptimizeGroup用于优化RelSubset. 它会先优化当前RelSubset所属的RelSet中的物理节点(这一过程为Implement), 即生成OptimizeInputs/OptimizeInput1; 之后会优化所属RelSet中的逻辑节点(这一过程为Explore), 即生成OptimizeMExpr. 这里先优化物理节点是为了可以快速获得一个Upper Bound以便进行剪枝. OptimizeInputs/OptimizeInput1会为每个子节点创建OptimizeGroup, 并生成CheckInput为子节点设置优化后的成本. OptimizeInput1是在只有一个输入节点情况下的简化版本. OptimizeMExpr用于优化逻辑节点, 它会先Explore每个子节点, 即生成ExploreInput; 然后为当前节点应用规则, 即生成ApplyRules. ExploreInput为当前RelSubset中的每个逻辑节点生成OptimizeMExpr. ApplyRules会为当前节点找到所有的VolcanoRuleMatch(在Explore节点仅会应用TransformationRule), 并为每个VolcanoRuleMatch生成一个ApplyRule. ApplyRule会真正应用一个规则, 如果在规则应用过程中生成了新的节点, 会调用VolcanoPlanner.addRelToSet()加入RelSubset, 同时会触发TopDownRuleDriver.onProduce(), 其流程与OptimizeGroup类似, 对于物理节点会生成OptimizeInputs/OptimizeInput1, 而对于逻辑节点会生成OptimizeMExpr. Task执行流程各个Task之间的调用关系如下图所示, 其中蓝色阴影框中是Columbia中定义的Task, 主要的优化流程都在这些Task中, 其他框是Calcite实现的Task, 用于执行一些辅助工作. 另外, 图中的红色箭头表示进入到下一层节点. 可以看到整个调用流程还是比较复杂的, 我们可以从几个闭环来厘清整体思路: 从OptimizeGroup到OptimizeInputs/OptimizeInput1这个环是整个Implement的过程, 它会不断递归直到已经优化过的节点或叶节点. 对于已经优化过的节点或者叶节点, 则会进入另一个环. 从OptimizeMExpr到ExploreInput这个环是整个Explore过程, 它也会不断递归, 对整个子树应用匹配的规则. 这里需要注意的一点是, 如果OptimizeMExpr是OptimizeGroup生成的, 那么属于Implement过程, 在此过程中会应用所有匹配的规则; 如果OptimizeMExpr是ExploreInput生成的, 那么属于Explore过程, 在此过程中仅仅会优化逻辑节点, 也就是仅会应用TransformationRule. 剪枝的实现目前, Calcite的实现仅支持对物理节点进行剪枝, 而Columbia对于逻辑节点会根据逻辑属性计算一个Lower Bound. 剪枝的过程主要实现在OptimizeInputs中: 对于bestCost不为无限大的RelSubset(即物理节点), 首先根据Upper Bound(在OptimizeGroup时传入)计算其输入节点的Upper Bound之和upperForInput, 再调用RelMetadataQuery计算所有输入节点的Lower Bound之和lowerBoundSum, 如果lowerBoundSum &gt; upperForInput则可以不再优化当前节点的子节点, 直接跳过. OptimizeInputs优化完一个节点后, CheckInput会用实际的成本替代之前的Lower Bound. 为了更形象地描述剪枝的过程, 我们举一个具体的例子, 读者可运行PruningJoinExample观察更多细节. 还是上文的表达式, 在优化rel#21:RelSubset#4.ENUMERABLE.[]这个Group时: 已知的最优成本是{17.34517744447956 rows, 9.0 cpu, 0.0 io}(即整个Group的Upper Bound), 这个最优成本来自于rel#26:EnumerableHashJoin.ENUMERABLE.[0](RelSubset#24,RelSubset#25). 在优化rel#33:EnumerableMergeJoin.ENUMERABLE.[[1], [5]](RelSubset#31,RelSubset#25)这个节点时, 由于其本身的成本为{8.8 rows, 0.0 cpu, 0.0 io}. 因此对于该节点而言, 其upperForInput为{8.54517744447956 rows, 9.0 cpu, 0.0 io}, 而此时它的两个子节点的成本(Lower Bound)rel#31:RelSubset#1.ENUMERABLE.[1]为{8.0 rows, 182.445678223346 cpu, 0.0 io}, rel#25:RelSubset#0.ENUMERABLE.[0]为{3.0 rows, 4.0 cpu, 0.0 io}, 其和大于upperForInput, 因此rel#33:EnumerableMergeJoin.ENUMERABLE.[[1]可以直接被剪枝. 物理属性的Pass-through和Derive由于VolcanoPlanner输出的是优化后的物理表达式, 因此在优化过程中必须考虑节点的物理属性. 在基于迭代的优化算法中, 父子节点之间无法传递物理属性, 因此如果父子节点之间的物理属性不匹配, 便会在其中放置一个AbstractConvertor, 之后在优化中会应用与之匹配的规则ExpandConversionRule来调用RelTraitDef.convert()实现转换. 在Top-Down优化算法中, 父子节点之间可以传递物理属性, 将父节点的物理属性传递给子节点称为Pass-through, 而将子节点的物理属性汇集到父节点称为Derive. Pass-through和Derive的实现接口都在PhysicalNode中, 因为只有物理节点才需要关心物理属性. 对于Pass-through可以按需实现passThrough(RelTraitSet)或passThroughTraits(RelTraitSet). 注意这两个函数的返回类型不同, 供外部调用的是前者, 而用户一般只需要实现passThroughTraits(RelTraitSet)即可. 由于一个节点可能有多个子节点, 在Derive时可能存在多种模式, 如LEFT_FIRST(表示在某个物理属性冲突时以最左侧的子节点为准), RIGHT_FIRST, BOTH和OMAKASE(表示由用户自行决定如何处理各个子节点中冲突的物理属性), 这些模式都定义在DeriveMode中. 在OMAKASE模式下需要实现derive(List), 否则可实现derive(RelTraitSet, int)或deriveTraits(RelTraitSet, int), 一般情况下只需实现后者即可. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public interface PhysicalNode extends RelNode { default @Nullable RelNode passThrough(RelTraitSet required) { Pair&lt;RelTraitSet, List&lt;RelTraitSet&gt;&gt; p = passThroughTraits(required); if (p == null) { return null; } int size = getInputs().size(); assert size == p.right.size(); List&lt;RelNode&gt; list = new ArrayList&lt;&gt;(size); for (int i = 0; i &lt; size; i++) { RelNode n = RelOptRule.convert(getInput(i), p.right.get(i)); list.add(n); } return copy(p.left, list); } default @Nullable Pair&lt;RelTraitSet, List&lt;RelTraitSet&gt;&gt; passThroughTraits( RelTraitSet required) { throw new RuntimeException(getClass().getName() + &quot;#passThroughTraits() is not implemented.&quot;); } default @Nullable RelNode derive(RelTraitSet childTraits, int childId) { Pair&lt;RelTraitSet, List&lt;RelTraitSet&gt;&gt; p = deriveTraits(childTraits, childId); if (p == null) { return null; } int size = getInputs().size(); assert size == p.right.size(); List&lt;RelNode&gt; list = new ArrayList&lt;&gt;(size); for (int i = 0; i &lt; size; i++) { RelNode node = getInput(i); node = RelOptRule.convert(node, p.right.get(i)); list.add(node); } return copy(p.left, list); } default @Nullable Pair&lt;RelTraitSet, List&lt;RelTraitSet&gt;&gt; deriveTraits( RelTraitSet childTraits, int childId) { throw new RuntimeException(getClass().getName() + &quot;#deriveTraits() is not implemented.&quot;); } default List&lt;RelNode&gt; derive(List&lt;List&lt;RelTraitSet&gt;&gt; inputTraits) { throw new RuntimeException(getClass().getName() + &quot;#derive() is not implemented.&quot;); } default DeriveMode getDeriveMode() { return DeriveMode.LEFT_FIRST; }} Pass-through为了更好地理解Pass-through的原理, 我们先以两个具体的例子来说明物理节点该如何实现Pass-through. 之后再介绍在优化过程中是如何触发物理属性的Pass-through的. 对于像Project, Filter这类简单算子, 一般对于不同的物理属性没有什么不同的处理逻辑, 因此在Pass-through的时候直接传递父节点的物理属性即可. 以EnumerableFilter为例, 其passThroughTraits()实现如下, 仅仅是接收了来自父节点的RelCollation.123456789@Override public @Nullable Pair&lt;RelTraitSet, List&lt;RelTraitSet&gt;&gt; passThroughTraits(RelTraitSet required) { RelCollation collation = required.getCollation(); if (collation == null || collation == RelCollations.EMPTY) { return null; } RelTraitSet traits = traitSet.replace(collation); return Pair.of(traits, ImmutableList.of(traits));} 对于像Join这样的算子, 就需要一些额外的处理了. 以EnumerableHashJoin为例, 由于其结果不会改变Probe side的顺序, 因此如果所需的顺序恰好是Probe side的顺序, 那么可以直接将其穿透到Probe side节点. 其代码实现如下, 仅当Join类型是Inner或Left join, 且排序键在左侧节点时才穿透排序属性.123456789101112131415161718192021222324static @Nullable Pair&lt;RelTraitSet, List&lt;RelTraitSet&gt;&gt; passThroughTraitsForJoin( RelTraitSet required, JoinRelType joinType, int leftInputFieldCount, RelTraitSet joinTraitSet) { RelCollation collation = required.getCollation(); if (collation == null || collation == RelCollations.EMPTY || joinType == JoinRelType.FULL || joinType == JoinRelType.RIGHT) { return null; } for (RelFieldCollation fc : collation.getFieldCollations()) { // If field collation belongs to right input: cannot push down collation. if (fc.getFieldIndex() &gt;= leftInputFieldCount) { return null; } } RelTraitSet passthroughTraitSet = joinTraitSet.replace(collation); return Pair.of(passthroughTraitSet, ImmutableList.of( passthroughTraitSet, passthroughTraitSet.replace(RelCollations.EMPTY)));} 在整个优化过程中, 有两个地方会触发Pass-through: 一个是在getOptimizeInputTask()生成OptimizeInputs/OptimizeInput1时, 如果节点与所在的RelSubset的物理属性不匹配, 会调用TopDownRuleDriver.convert()进行转换, 其中便会触发Pass-through. 另一个是在新节点生成时, 会调用TopDownRuleDriver.onProduce(), 如果是物理节点也会调用getOptimizeInputTask()触发Pass-through. Derive对于像Project, Filter这类简单算子, 其Derive过程同样比较简单, 直接从子节点收集物理属性向上传递即可, 这里不再赘述. 而对于EnumerableHashJoin算子, 由于Join之后只会保留Probe side的顺序, 因此只会向上传递Probe side的排序属性. 在每个OptimizeInputs/OptimizeInput1完成后都会生成一个DeriveTrait任务, 它会调用TopDownRuleDriver.derive()进行物理属性的Derive. 结束流程经过上述优化之后, 正常情况下相应的RelSubset都产生了一个成本最低的物理表达式. 结束流程就是从根节点开始进行一次深度优先遍历, 将整个执行计划还原为RelNode(非RelSubset)结构. 总结本文详细介绍了Calcite中的成本优化器VolcanoPlanner, 在1.24之后的版本中已经支持Cascades风格的Top-Down优化算法. 可以看到VolcanoPlanner的优化过程是比较复杂的, 引入了不少新的概念, 涉及诸多细节处理, 代码实现上也比较绕, 在阅读源码时可以先从具体案例(例如本文给出的几个可运行示例)出发, 多运行调试, 把握处理要点, 切勿太早陷入过多细节. 以笔者的经验来看, 很多细节只能反复调试才能理解其原理, 遇到难以理解到的逻辑可从宏观角度出发, 反推在什么样的结构中才有可能遇到这种情况, 然后构造一个表达式调试观察其优化过程. 参考[1] Custom traits in Apache Calcite[2] What is Cost-based Optimization?[3] Memoization in Cost-based Optimizers[4] Metadata Management in Apache Calcite[5] Efficiency in the Columbia Database Query Optimizer[4] Calcite中新增的Top-down优化器[5] SQL查询优化原理与Volcano Optimizer介绍[6] 揭秘TiDB新优化器: Cascades Planner原理解析 附录附录一: 代码生成Calcite生成的NonCumulativeCost.Handler实现类. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package org.apache.calcite.rel.metadata.janino;public final class GeneratedMetadata_NonCumulativeCostHandler implements org.apache.calcite.rel.metadata.BuiltInMetadata.NonCumulativeCost.Handler { private final Object methodKey0 = new org.apache.calcite.rel.metadata.janino.DescriptiveCacheKey(&quot;RelOptCost Handler.getNonCumulativeCost()&quot;); public final org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows$RelMdNonCumulativeCost provider0; public GeneratedMetadata_NonCumulativeCostHandler( org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows$RelMdNonCumulativeCost provider0) { this.provider0 = provider0; } public org.apache.calcite.rel.metadata.MetadataDef getDef() { return provider0.getDef(); } public org.apache.calcite.plan.RelOptCost getNonCumulativeCost( org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq) { while (r instanceof org.apache.calcite.rel.metadata.DelegatingMetadataRel) { r = ((org.apache.calcite.rel.metadata.DelegatingMetadataRel) r).getMetadataDelegateRel(); } final Object key; key = methodKey0; final Object v = mq.map.get(r, key); if (v != null) { if (v == org.apache.calcite.rel.metadata.NullSentinel.ACTIVE) { throw new org.apache.calcite.rel.metadata.CyclicMetadataException(); } if (v == org.apache.calcite.rel.metadata.NullSentinel.INSTANCE) { return null; } return (org.apache.calcite.plan.RelOptCost) v; } mq.map.put(r, key,org.apache.calcite.rel.metadata.NullSentinel.ACTIVE); try { final org.apache.calcite.plan.RelOptCost x = getNonCumulativeCost_(r, mq); mq.map.put(r, key, org.apache.calcite.rel.metadata.NullSentinel.mask(x)); return x; } catch (java.lang.Exception e) { mq.map.row(r).clear(); throw e; } } private org.apache.calcite.plan.RelOptCost getNonCumulativeCost_( org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq) { if (r instanceof org.apache.calcite.rel.RelNode) { return provider0.getNonCumulativeCost((org.apache.calcite.rel.RelNode) r, mq); } else { throw new java.lang.IllegalArgumentException(&quot;No handler for method [public abstract org.apache.calcite.plan.RelOptCost org.apache.calcite.rel.metadata.BuiltInMetadata$NonCumulativeCost$Handler.getNonCumulativeCost(org.apache.calcite.rel.RelNode,org.apache.calcite.rel.metadata.RelMetadataQuery)] applied to argument of type [&quot; + r.getClass() + &quot;]; we recommend you create a catch-all (RelNode) handler&quot;); } }} Calcite生成的NonCumulativeCost.Handler实现类. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package org.apache.calcite.rel.metadata.janino;public final class GeneratedMetadata_NonCumulativeCostHandler implements org.apache.calcite.rel.metadata.BuiltInMetadata.NonCumulativeCost.Handler { private final Object methodKey0 = new org.apache.calcite.rel.metadata.janino.DescriptiveCacheKey(&quot;RelOptCost Handler.getNonCumulativeCost()&quot;); public final org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows$RelMdNonCumulativeCost provider0; public GeneratedMetadata_NonCumulativeCostHandler( org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows$RelMdNonCumulativeCost provider0) { this.provider0 = provider0; } public org.apache.calcite.rel.metadata.MetadataDef getDef() { return provider0.getDef(); } public org.apache.calcite.plan.RelOptCost getNonCumulativeCost( org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq) { while (r instanceof org.apache.calcite.rel.metadata.DelegatingMetadataRel) { r = ((org.apache.calcite.rel.metadata.DelegatingMetadataRel) r).getMetadataDelegateRel(); } final Object key; key = methodKey0; final Object v = mq.map.get(r, key); if (v != null) { if (v == org.apache.calcite.rel.metadata.NullSentinel.ACTIVE) { throw new org.apache.calcite.rel.metadata.CyclicMetadataException(); } if (v == org.apache.calcite.rel.metadata.NullSentinel.INSTANCE) { return null; } return (org.apache.calcite.plan.RelOptCost) v; } mq.map.put(r, key,org.apache.calcite.rel.metadata.NullSentinel.ACTIVE); try { final org.apache.calcite.plan.RelOptCost x = getNonCumulativeCost_(r, mq); mq.map.put(r, key, org.apache.calcite.rel.metadata.NullSentinel.mask(x)); return x; } catch (java.lang.Exception e) { mq.map.row(r).clear(); throw e; } } private org.apache.calcite.plan.RelOptCost getNonCumulativeCost_( org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq) { if (r instanceof org.apache.calcite.rel.RelNode) { return provider0.getNonCumulativeCost((org.apache.calcite.rel.RelNode) r, mq); } else { throw new java.lang.IllegalArgumentException(&quot;No handler for method [public abstract org.apache.calcite.plan.RelOptCost org.apache.calcite.rel.metadata.BuiltInMetadata$NonCumulativeCost$Handler.getNonCumulativeCost(org.apache.calcite.rel.RelNode,org.apache.calcite.rel.metadata.RelMetadataQuery)] applied to argument of type [&quot; + r.getClass() + &quot;]; we recommend you create a catch-all (RelNode) handler&quot;); } }} Calcite生成的LowerBoundCost.Handler实现类. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package org.apache.calcite.rel.metadata.janino;public final class GeneratedMetadata_LowerBoundCostHandler implements org.apache.calcite.rel.metadata.BuiltInMetadata.LowerBoundCost.Handler { private final Object methodKey0 = new org.apache.calcite.rel.metadata.janino.DescriptiveCacheKey(&quot;RelOptCost Handler.getLowerBoundCost(RelNode, RelMetadataQuery, VolcanoPlanner)&quot;); public final org.apache.calcite.rel.metadata.RelMdLowerBoundCost provider0; public GeneratedMetadata_LowerBoundCostHandler( org.apache.calcite.rel.metadata.RelMdLowerBoundCost provider0) { this.provider0 = provider0; } public org.apache.calcite.rel.metadata.MetadataDef getDef() { return provider0.getDef(); } public org.apache.calcite.plan.RelOptCost getLowerBoundCost( org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq, org.apache.calcite.plan.volcano.VolcanoPlanner a2) { while (r instanceof org.apache.calcite.rel.metadata.DelegatingMetadataRel) { r = ((org.apache.calcite.rel.metadata.DelegatingMetadataRel) r).getMetadataDelegateRel(); } final Object key; key = org.apache.calcite.runtime.FlatLists.of(methodKey0, org.apache.calcite.rel.metadata.NullSentinel.mask(a2)); final Object v = mq.map.get(r, key); if (v != null) { if (v == org.apache.calcite.rel.metadata.NullSentinel.ACTIVE) { throw new org.apache.calcite.rel.metadata.CyclicMetadataException(); } if (v == org.apache.calcite.rel.metadata.NullSentinel.INSTANCE) { return null; } return (org.apache.calcite.plan.RelOptCost) v; } mq.map.put(r, key,org.apache.calcite.rel.metadata.NullSentinel.ACTIVE); try { final org.apache.calcite.plan.RelOptCost x = getLowerBoundCost_(r, mq, a2); mq.map.put(r, key, org.apache.calcite.rel.metadata.NullSentinel.mask(x)); return x; } catch (java.lang.Exception e) { mq.map.row(r).clear(); throw e; } } private org.apache.calcite.plan.RelOptCost getLowerBoundCost_( org.apache.calcite.rel.RelNode r, org.apache.calcite.rel.metadata.RelMetadataQuery mq, org.apache.calcite.plan.volcano.VolcanoPlanner a2) { if (r instanceof org.apache.calcite.plan.volcano.RelSubset) { return provider0.getLowerBoundCost((org.apache.calcite.plan.volcano.RelSubset) r, mq, a2); } else if (r instanceof org.apache.calcite.rel.RelNode) { return provider0.getLowerBoundCost((org.apache.calcite.rel.RelNode) r, mq, a2); } else { throw new java.lang.IllegalArgumentException(&quot;No handler for method [public abstract org.apache.calcite.plan.RelOptCost org.apache.calcite.rel.metadata.BuiltInMetadata$LowerBoundCost$Handler.getLowerBoundCost(org.apache.calcite.rel.RelNode,org.apache.calcite.rel.metadata.RelMetadataQuery,org.apache.calcite.plan.volcano.VolcanoPlanner)] applied to argument of type [&quot; + r.getClass() + &quot;]; we recommend you create a catch-all (RelNode) handler&quot;); } }} 附录二: ensureRootConverters()解析ensureRootConverters()会在两个地方被调用: 一是在优化开始前, 在VolcanoPlanner.setRoot()和VolcanoPlanner.findBestExp()中调用, 这里虽然调用了两次, 实际第二次只是确认, 没有产生实际效果. 二是在优化过程中, 如果如果出现了和根节点所在RelSet等价的其他RelSet, 那么两个RelSet就会进行合并, 此时必然涉及到根节点的变动, 因此这时也需要再调用一次nsureRootConverters(). 为了更好地理解这一过程, 笔者提供了一个可运行的案例RelSetMergeExample, 读者可运行调试. 下面我们看下ensureRootConverters()的实现代码.12345678910111213141516171819202122232425262728293031/** Ensures that the subset that is the root relational expression contains * converters to all other subsets in its equivalence set. * * &lt;p&gt;Thus the planner tries to find cheap implementations of those other * subsets, which can then be converted to the root. This is the only place * in the plan where explicit converters are required; elsewhere, a consumer * will be asking for the result in a particular convention, but the root has * no consumers. * 这里的意思是说, 根节点没有父节点来保证所需的物理属性, 因此需要添加一个AbstractConverter */@RequiresNonNull(&quot;root&quot;)void ensureRootConverters() { final Set&lt;RelSubset&gt; subsets = new HashSet&lt;&gt;(); // 这里只会在根节点被合并的情况下起作用 for (RelNode rel : root.getRels()) { if (rel instanceof AbstractConverter) { subsets.add((RelSubset) ((AbstractConverter) rel).getInput()); } } for (RelSubset subset : root.set.subsets) { final ImmutableList&lt;RelTrait&gt; difference = root.getTraitSet().difference(subset.getTraitSet()); // 这里只有在1个RelTrait不同时才会添加AbstractConverter if (difference.size() == 1 &amp;&amp; subsets.add(subset)) { register( new AbstractConverter(subset.getCluster(), subset, difference.get(0).getTraitDef(), root.getTraitSet()), root); } }} 这里再进一步解释一下为什么只有在1个RelTrait不同时才会添加AbstractConverter. 其实我们看一下AbstractConverter是如何应用的就明白了, 在ExpandConversionRule中实际是通过VolcanoPlanner.changeTraitsUsingConverters()来实现物理属性转换的, 而它最终返回的只有一个值, 就是最后一个属性转换后对应的节点. 如果根节点需要转换多种物理属性, 可以多次调用VolcanoPlanner进行多重转换.","link":"/apache-calcite-volcanoplanner.html"},{"title":"Flink源码 - RPC原理与实现","text":"本文已收录在合集Apche Flink原理与实践中. Flink作为一个分布式计算系统, 其组件间的通信是由RPC实现的. 为方便使用, Flink抽象了一套RPC框架, 并提供了基于Akka的实现. 本文首先介绍Flink RPC框架的整体设计, 之后介绍其基于Akka的实现. 理解Flink的RPC实现是理解其底层组件之间通信原理的基础. 由于Akka将许可协议从Apache 2.0改为了Business Source License (BSL) v1.1, 为了避免商业风险, FLINK-32468暂时将Akka替换为了Apache Pekko(Fork自Akka 2.6.x), 已经在Flink 1.18中发布, 由于Pekko源自Akka, 因此本文在介绍时不作区分. 远期看社区有计划将Akka替换为gRPC(FLINK-29281). Flink抽象了一套RPC接口, 基于这套接口可以有不同的底层实现, 例如当前基于Akka的实现, 以及计划中基于gRPC的实现. Flink的RPC实现都在独立的flink-rpc模块下, 如果有需要完全可以引用相关依赖并在其他项目中使用. 使用案例在进一步介绍Flink RPC接口之前, 我们先通过一个简单的RPC远程调用案例来展示一下Flink RPC接口的使用方法. 完整代码可见此处. 定义服务接口.123public interface HelloGateway extends RpcGateway { String hello();} 定义服务端组件.12345678910public class HelloRpcEndpoint extends RpcEndpoint implements HelloGateway { public HelloRpcEndpoint(RpcService rpcService) { super(rpcService); } @Override public String hello() { return &quot;Hello&quot;; }} 启动服务端组件.123456789rpcService1 = PekkoRpcServiceUtils.createRemoteRpcService( new Configuration(), &quot;localhost&quot;, &quot;0&quot;, null, Optional.empty());HelloRpcEndpoint helloRpcEndpoint = new HelloRpcEndpoint(rpcService1);helloRpcEndpoint.start(); 客户端进行远程调用.123456789101112rpcService2 = PekkoRpcServiceUtils.createRemoteRpcService( new Configuration(), &quot;localhost&quot;, &quot;0&quot;, null, Optional.empty());HelloGateway helloGateway = rpcService2.connect( helloRpcEndpoint.getAddress(), HelloGateway.class).get();String result = helloGateway.hello();System.out.println(result); 从上述案例可以看到, Flink的RPC框架提供了非常高级的抽象, 客户端拿到服务接口后的远程调用就跟本地方法调用一样. 需要说明的是, 本例中的客户端和服务端区分只是为了方便描述, 实际上在Flink中JabManager中的各个组件和TaskManager在通信角色上是对等的, 并没有严格的客户端和服务端区分, 或者说客户端和服务端的角色会发生转变, 谁发送请求谁就是客户端. 接口详解RpcEndpoint, RpcService, RpcGateway和RpcServer是Flink RPC框架中的核心抽象接口. RpcEndpoint代表一个可提供RPC服务的组件, 它持有其所属的RpcService引用, 在Flink中其实现类如下. JobManager中的ResourceManager, Dispatcher和JobMaster组件以及TaskManager中的TaskExecutor都是RpcEndpoint组件. RpcService是RPC服务的底层实现, 一个RpcService可服务于多个RpcEndpoint, 例如JobManager中的ResourceManager, Dispatcher和JobMaster就共用一个RpcService. RpcService可通过connect()连接到另一个RpcService. RpcGateway是所有RPC服务的抽象接口, 用户可继承RpcGateway并扩展更多接口. 通常情况下, RpcEndpoint会继承RpcGateway并实现具体的方法. RpcServer也继承了RpcGateway, 它可以看做是RpcGateway的代理, 实际上RpcService.connect()方法返回的都是RpcServer, 在调用RpcGateway的实现方法时, 都由RpcServer代理实现, 这样即可根据调用方与被调用方的关系选择远程调用或本地调用. 这里读者可能会有一些疑惑, 有了RpcGateway为什么还需要RpcServer? 这是由于RpcGateway只是一个普通的Java接口, 其方法都是本地实现, 如果RpcService.connect()返回的是RpcGateway实例, 那么并不能实现RPC调用. RpcServer通过动态代理包装了RpcGateway的方法, 从而实现RPC调用. 上述接口在Flink中使用时的联动关系如下图所示. RpcSystem是一个工厂接口, 具体实现由RpcSystemLoader加载得到. 它创建的RpcServiceBuilder可启动一个RpcService; RpcEndpoint会持有一个RpcService, 在初始化时会根据传入的RpcService启动一个RpcServer用于为当前RpcEndpoint提供RPC服务; 通过RpcService可连接到本地或远程RpcService中对应的RpcServer服务. 基于Akka的RPC实现Akka简介Akka是Actor模型的一种实现, Actor模型是一种通用的消息传递编程模型, 在分布式系统构建中被广泛运用. Actor的核心思想是独立维护隔离状态, 并基于消息传递实现异步通信. Actor通常包含以下特征: 每个Actor持有一个邮箱(即队列), 用于存储消息; 每个Actor可以发送消息至任何Actor; 每个Actor可以通过处理消息来更新内部状态, 对于外部而言, Actor的状态是隔离的. 在Akka中, 用户可继承Actor, 从而实现自定义的消息处理逻辑. Actor实现Flink RPC基于Akka的实现中, 扩展了多种类型的Actor, 其继承关系如下图所示. 其中: SupervisorActor用于启动PekkoRpcActor, 并监控其何时终止. PekkoRpcActor用于真正处理RPC调用. DeadLettersActor用于处理DeadLetter(即发送给已经停止的Actor的消息). 基于Actor的RPC调用流程上文已经说到, RpcService.connect()方法返回的都是RpcServer, PekkoInvocationHandler是基于Akka的实现. 对于RpcGateway的调用都会被代理到PekkoInvocationHandler.invoke()中, PekkoInvocationHandler在创建时就持有一个Actor的引用, 在调用时即可与该Actor通信, PekkoRpcActor会通过动态代理调用RpcEnpoint中的具体实现方法, 并将结果返回. 总结本文通过一个示例直观的介绍了Flink RPC的使用方法. 之后介绍了几个核心接口的含义, 以及基于Akka的实现. 从整体上看, Flink的RPC框架是比较优雅的实现, 其中使用了多种设计模式, 除了理解Flink的底层通信方式, 也值得我们从中学习框架的设计方法. 参考[1] FLIP-6: Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc.[2] [FLINK-4346] Implement basic RPC abstraction[3] Spark与Flink中的RPC实现[4] 浅谈Actor模型","link":"/flink-code-rpc.html"},{"title":"Flink SQL最佳实践 - HBase SQL Connector应用","text":"本文已收录在合集Apche Flink原理与实践中. HBase作为谷歌BigTable的开源实现, 是构建在HDFS上的分布式键值数据库. 由于具有极高的读写性能, HBase已经在实时计算领域得到了广泛运用. 随着Flink在实时计算领域的普及, Flink与HBase的结合应用也是趋势使然. 从Flink 1.9开始就在SQL API层面增加了对HBase Connector的支持. 本文通过一个实际的应用案例对Flink SQL与HBase的结合应用进行实践, 整个过程只需要在SQL Client中编写SQL语句, 不需要写任何Java或Scala代码. 本文案例主要包含两种场景. 场景一是将Flink SQL的计算结果实时写入HBase; 场景二是在Flink SQL中利用HBase table进行Lookup Join. 本文的实践案例需要依赖Kafka SQL Connecter和HBase SQL Connecter, 将相关Jar包下载后置于$FLINK_HOME/lib目录下并重启Flink集群即可完成配置. 应用案例本文的案例考虑如下一种场景: 存在一种出租车订单信息流, 流中的每条记录都包含订单id(order_id), 订单金额(price), 交易货币(currency), 交易时间(time). 其中交易货币支持各种类型的货币, 比如美元(USD), 日元(JPY), 欧元(EUR), 英镑(GBP). 现在有两个需求: 实时计算每种货币的订单数量及订单总金额, 并写入数据库供前端业务进行实时展示; 给定一个汇率表, 记录了各种货币转换为人民币(CNY)的汇率, 通过关联该汇率表, 实时将每条记录成交金额转化为人民币. 这两个需求即分别对应文章开头所说的两种应用场景. 数据准备出租车订单信息流为方便复现本文案例, 在操作过程中我们并没有使用真实数据, 而是使用Python脚本生成模拟数据, 并通过Linux管道命令将数据写入Kafka. 由于Linux管道是批量刷写的, 所以进入Kafka的数据可能是按小批次写入的, 并不是逐条实时写入的. 通过如下命令创建Kafka topic orders, 如已经存在可先运行删除命令.12345678910111213# 删除orders topickafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --delete \\ --topic orders# 创建orders topickafka-topics.sh \\ --create \\ --bootstrap-server localhost:9092 \\ --replication-factor 1 \\ --partitions 1 \\ --topic orders 生成出租车订单的Python脚本如下, 其文件名为txai_fare.py.1234567891011121314151617181920212223242526import randomimport timerates = [ [&apos;USD&apos;, 2, 20], [&apos;JPY&apos;, 200, 2000], [&apos;EUR&apos;, 2, 20], [&apos;GBP&apos;, 2, 20], [&apos;CNY&apos;, 10, 200]]def main(): order_id = 1 while True: currency = random.choice(rates) price = random.uniform(currency[1], currency[2]) fare = &quot;%d,%.2f,%s,%s&quot; % (order_id, price, currency[0], time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;, time.localtime())) print(fare) time.sleep(0.05) order_id += 1if __name__ == &apos;__main__&apos;: main() 运行如下命令即可通过Linux管道将taxi_fare.py脚本生成的模拟数据写入orders topic.1234python taxi_fare.py \\ | kafka-console-producer.sh \\ --bootstrap-server localhost:9092 \\ --topic orders 汇率表汇率表存储在HBase中, 实际上是一个维表. 首先通过HBase shell创建HBase rates table. 创建命令如下.1create &apos;rates&apos;, {NAME =&gt; &apos;f&apos;} 接着通过SQL Client创建Flink HBase table. 在控制台运行sql-client.sh embedded命令即可通过默认配置启动SQL Client. 建表语句如下.123456789CREATE TABLE rates ( currency STRING, f ROW&lt;rate DECIMAL(32, 2)&gt;) WITH ( &apos;connector&apos; = &apos;hbase-1.4&apos;, &apos;table-name&apos; = &apos;rates&apos;, &apos;zookeeper.quorum&apos; = &apos;localhost:2181&apos;, &apos;zookeeper.znode.parent&apos; = &apos;/hbase&apos;); 建表成功后通过如下语句写入几条示例数据用于实验.123456INSERT INTO rates VALUES (&apos;USD&apos;, ROW(6.45)), (&apos;JPY&apos;, ROW(0.06)), (&apos;EUR&apos;, ROW(7.91)), (&apos;GBP&apos;, ROW(8.78)); 为了验证写入是否成功, 可用过如下SELECT语句进行验证.1SELECT * FROM rates; 需要说明的是, 这里使用Flink SQL Client写入示例数据只是为了方便, 实际上汇率表的内容可能是由其它程序通过HBase客户端API写入的. Flink SQL与HBase结合应用场景一HBase由于出色的写入性能, 常被用于实时大屏. 其构建流程如下图所示. 实时数据通常接入到Kafka, 然后由Flink对Kafka中的数据进行实时处理, 最终写入HBase供实时大屏进行展示. 上文提到的第一个需求其实就是一个实时大屏应用. 在数据准备阶段我们已经实现了不断生成出租车订单信息并写入Kafka, 这里我们就可以通过Flink SQL实时读取Kafka中的数据进行计算, 并将结果写入HBase. 下面我们具体讲述如何解决第一个需求. 在SQL Client中输入如下DDL创建Kafka orders表, 表示出租车订单信息流.123456789101112131415CREATE TABLE orders ( order_id STRING, price DECIMAL(32,2), currency STRING, order_time TIMESTAMP(3), proc_time AS PROCTIME(), WATERMARK FOR order_time AS order_time) WITH ( &apos;connector&apos; = &apos;kafka&apos;, &apos;topic&apos; = &apos;orders&apos;, &apos;scan.startup.mode&apos; = &apos;earliest-offset&apos;, &apos;properties.zookeeper.connect&apos; = &apos;localhost:2181&apos;, &apos;properties.bootstrap.servers&apos; = &apos;localhost:9092&apos;, &apos;format&apos; = &apos;csv&apos;); 在HBase shell中创建计算结果表order_agg, 用于存储每种金额成交的订单数量和订单总金额.1create &apos;order_agg&apos;, {NAME =&gt; &apos;f&apos;} 在SQL Client中创建HBase order_agg table.123456789CREATE TABLE order_agg ( currency STRING, f ROW&lt;order_num BIGINT, total_price DECIMAL(32, 2)&gt;) WITH ( &apos;connector&apos; = &apos;hbase-1.4&apos;, &apos;table-name&apos; = &apos;order_agg&apos;, &apos;zookeeper.quorum&apos; = &apos;localhost:2181&apos;, &apos;zookeeper.znode.parent&apos; = &apos;/hbase&apos;); 通过如下语句执行计算, 并将结果写入HBase.123456789101112INSERT INTO order_agg SELECT currency, ROW(order_num, total_price) FROM ( SELECT currency, count(*) AS order_num, sum(price) AS total_price FROM orders GROUP BY currency ); 完成上述步骤后即可在HBase的order_agg表查询到实时的更新结果, 在实际应用中会通过前端界面展示实时计算的结果, 本文的重点不在于如何展示, 所以省略了这一步骤. 场景二在流式数据的join中, 维度表的join一定是绕不开的. 为此, Flink SQL实现了Lookup Join. 而HBase作为分布式键值数据库, 相对于MySQL等关系型数据库有更好的并发性能和查询效率, 因此将维表存储在HBase中是更加合适的. 下面我们将通过Lookup Join解决上文提到的第二个需求. Flink Kafka table orders和Flink HBase table rates在上文中已经创建, 创建方式不再赘述. 在场景二中进行Lookup Join的目的是为orders表中的每条订单记录关联其对应货币的汇率并将交易金额转化为标准的人命币金额. 可通过如下DQL语句实现.12345678SELECT order_id, price * rate as cny_price, order_timeFROM orders as O JOIN rates FOR SYSTEM_TIME AS OF O.proc_time AS R ON O.currency = R.currency; 上述DQL语句只是将结果在SQL Client中进行展示, 在实际应用中可能需要将相关记录写入HBase, 对此读者可以参考场景一的例子创建相应的表, 并将结果写入HBase. 本文对此不在赘述. 参考[1] [技术生态篇]最佳实践: Flink流式导入HBase[2] Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL","link":"/flink-sql-hbase.html"},{"title":"Flink源码 - 从Kafka Connector看Source接口重构","text":"本文已收录在合集Apche Flink原理与实践中. Flink原始的Source接口(SourceFunction)随着Flink在数据集成和流批一体上的不断发展, 暴露出了越来越多的问题. 为了实现更优雅的数据接入, 社区提出了FLIP-27来重构Source接口. 新的Source接口已经在Flink 1.12中得到实现, 该接口将成为Flink数据接入的新标准. 虽然FLIP-27为流式数据的读取抽象了优雅的接口, 但是这些接口的实现和交互逻辑较为复杂, 如果不能准确理解其实现原理, 就很难写出正确的Connector. 本文以Kafka Connector为例, 详细介绍FLIP-27 Source接口的实现原理. Source接口重构动机随着Flink的快速发展, 旧的Source接口(SourceFunction)出现了诸多问题(关于重构动机的更详细介绍可参考漫谈Flink Source接口重构): 无法实现流处理和批处理的统一: 原来的SourceFunction针对流处理设计, 难以适配到批处理. 从社区的发展来看, Flink对流批一体是有很大”野心”的, 流批一体的Source接口是重要的推进步骤. 数据源的发现和数据读取耦合在一起: 这一问题使得对一些需要进行动态分区发现的数据源的Connector实现异常复杂, 如Kafka Connector. 分区(Partition/Shard/Split)并没有抽象的接口: 这一问题使得难以实现一些数据源无关的通用特性, 例如Event Time对齐, Watermark对齐等. 需要显示使用Checkpoint Lock: 由于Checkpoint Lock是非公平锁, SourceFunction可能长期持有而导致Checkpoint阻塞. 另外显示加锁的方式也不符合Flink算子Actor/Mailbox风格的线程模型设计. 没有统一的线程模型: 为了避免阻塞主线程, Source通常需要实现IO线程来进行数据读取. 目前由于缺乏统一的线程管理, 提高了实现新的Source的门槛. 从结果来看, FLIP-27以及之后的FLIP-182和FLIP-217已经基本解决了上述问题. File, Kafka, Pulsar等常见数据源的Connector也已经基于新的接口进行了重构. 整体执行原理在介绍具体的接口实现之前, 我们先来看下新的Source实现在Runtime层面的整体运行逻辑. 在JobMaster中, 引入了一个新的组件SourceCoordinator, 其包含的SourceCoordinatorContext 负责通过RPC与Task中的SourceOperator通信. SplitEnumerator是一个抽象接口, 用户可针对不同的数据源做具体的实现, 主要用于发现Split并分配给指定的Reader. SplitEnumerator与SourceCoordinator共享一个SourceCoordinatorContext, 并通过它进行交互. 在TaskManager中, 引入了一个新的SourceOperator, 负责与SourceCoordinator交互, 并在内部调用SourceReader获取数据. SourceReader同样是一个抽象接口, 用户需要根据数据源做具体实现. SourceCoordinatorContext与SourceOperator之间通过OperatorEvent进行交互, 具体来说: SourceOperator主要会向SourceCoordinatorContext发送: RequestSplitEvent来向SourceCoordinator请求需要读取的Split, 在Kafka Connector中, SourceCoordinator总是主动向SourceOperator分配Split. ReaderRegistrationEvent来向SourceCoordinator注册当前Reader. ReportedWatermarkEvent来向SourceCoordinator报告当前Reader的Watermark, 以便SourceCoordinator决定是否需要进行Watermark对齐. SourceCoordinatorContext主要会向SourceOperator发送: AddSplitEvent来向SourceOperator分配Split. WatermarkAlignmentEvent来通知SourceOperator要进行Watermark对齐. NoMoreSplitsEvent来通知SourceOperator没有更多的Split了. 从宏观角度来看, FLIP-27最重要的变动就是将数据源的发现和数据的读取拆分开来, 作为两个独立的组件分别运行在JobMaster和Task中, 彼此之间通过RPC进行通信. 在这个整体框架下, 很多细节问题就变得很好解决了, 比如Watermark的对齐可以通过全局的SourceCoordinator方便地实现. 核心接口在FLIP-27中, 最核心的两个接口是SplitEnumerator和SourceReader. 其中: SplitEnumerator用于发现Splits并将其分配给SourceReader. SourceReader用于从给定的Splits中读取数据. 除上述两个接口是公共接口外, 其他几个公共接口如下, 在实现Connector时需要频繁使用这些接口: Source: 实际上是一个工厂类, 用于创建SplitEnumerator和SourceReader. SourceSplit: 分区的抽象接口, 用于表示一个Partition/Shard/Split. SplitEnumeratorContext: 用于为SplitEnumerator提供上下文信息, 保存已分配的分区, 并向SourceReader发送事件消息(如分配一个新的分区). SplitAssignment: 用于保存分配给各个子任务的分区. SourceReaderContext: 用于SourceReader和SplitEnumerator之间的通信. SplitReader: 从一个或多个分区中读取数据. SplitFetcherManager: 用于管理SplitFetcher, SplitFetcher调用SplitReader进行数据读取, 运行在SplitFetcherManager的线程池中. SourceOutput: Collector样式的接口, 用于获取SourceReader发出的记录和时间戳. WatermarkOutput: 用于发送Watermark, 并指示Source是否空闲. Watermark: 这是在org.apache.flink.api.common.eventtime包中新建的Watermark类. 这个类最终将替换org.apache.flink.stream.api.Watermark. 这个更改允许flink-core独立于其他模块. 因为最终Watermark的生成将会被放到Source中. (关于为什么要将Watermark的生成放到Source中可参考笔者之前的博文Flink最佳实践 - Watermark原理及实践问题解析) KafkaSource执行流程上文介绍了FLIP-27中引入的几个新接口, 接下来本文将以KafkaSource为例, 介绍这些接口的执行和交互流程. 如果不做特殊说明, 本文在描述时将以Kafka Connector的具体实现类为准, 其中某些方法可能实现在父类中, 在阅读源码时需要稍加注意. Split分配流程Split分配流程由KafkaSourceEnumerator实现, 它是SplitEnumerator的实现类, 运行在JobMaster中. 整个Split的分配流程如下图所示: 在JobMaster中会启动一个SourceCoordinator, 在它的start()方法中会调用KafkaSourceEnumerator的start()方法来启动Split分配. KafkaSourceEnumerator的start()方法会根据用户配置, 选择定时或一次调用getSubscribedTopicPartitions()和checkPartitionChanges(). 其中: getSubscribedTopicPartitions()通过Kafka客户端获取指定Topic的所有Partition, 并将结果传递给checkPartitionChanges(); checkPartitionChanges()会调用getPartitionChange()来根据已经分配的Partition判断哪些Partition是新增的, 哪些是删除的(实际上删除的Partition在目前的实现中没有做任何处理). 当获取到新增和删除的Partition之后, checkPartitionChanges()会调用initializePartitionSplits()和handlePartitionSplitChanges(), 其中: initializePartitionSplits()用于初始化各个Partition的起始和结束Offset. 结果会传递给handlePartitionSplitChanges(); handlePartitionSplitChanges()会先调用addPartitionSplitChangeToPendingAssignments()来计算各个Partition所属的SourceReader, 之后调用assignPendingPartitionSplits()来向SourceOperator发送消息分配Partition. 真正分配Partition的消息由SourceCoordinatorContext的assignSplits()方法通过发送AddSplitEvent来实现. TaskManager接收到消息后会交由SourceOperator进行处理, 详细的处理过程会在下文分析. 可以看到KafkaSourceEnumerator的Partition分配流程还是比较复杂的, 不过在把握整体流程之后再阅读各个函数的代码, 其实也不难理解. 数据读取流程当KafkaSourceReader接收到来自KafkaSourceEnumerator分配的Partition之后, 就会开始真正进行数据读取了. 数据读取的整体流程还是比较简单的, KafkaSourceReader运行在Task主线程中, 非阻塞地从FutureCompletingBlockingQueue中获取数据, 如果有数据就使用KafkaRecordEmitter向下游发送数据. SplitFetcher是真正的IO线程, 通过KafkaPartitionSplitReader从Kafka读取数据后放入FutureCompletingBlockingQueue. 数据读取流程的核心在于IO线程模型的实现, 可通过SplitFetcherManager进行控制. 多数实现都是以下线程模型中的一种: Sequential Single Split, 即单个IO线程依次顺序读取各个Split, 这种模型一般用在文件, 数据库查询等有界数据场景. Multi-split Multiplexed, 即单个IO线程复用以读取多个Split, 一般用在支持多路复用客户端的组件上, 如Kafka. Multi-split Multi-threaded, 即一个IO线程读取一个Split, 一般用在Amazon Kinesis, 阿里云SLS等产品上. SplitFetcher生命周期通过上文的分析我们已经知道, SplitFetcher是真正从数据源读取数据的任务, 它继承自Runnable并运行在SplitFetcherManager的线程池中. SplitFetcher任务的状态在运行过程中会不断发生变化, 笔者将SplitFetcher的生命周期总结为以下四个状态(需要注意的是这几个状态是笔者总结的逻辑上的状态, 并不与Java线程的状态完全对应): Running: 当SplitFetcherManager创建并向线程池提交SplitFetcher任务之后就进入了Runnning状态. Running状态的SplitFetcher可能是在真正地读取数据(对应的JVM线程状态为RUNNABLE, WAITING或TIME_WAITING), 也可能在等待Split的分配(如刚启动时的场景, 对应的JVM线程状态为WAITING). Pause: 为了在多个Split之间进行Watermark对齐, SplitFetcher提供了三个接口: pauseOrResumeSplits(): 会调用SplitReader.pauseOrResumeSplits(), 适用于KafkaConsumer这类提供了暂停或继续读取指定Split的客户端接口. pause(): 通过设置SplitFetcher的paused标志位为true来暂停SplitFetcher任务. resume(): 通过设置SplitFetcher的paused标志位为false, 并调用resumed.signal()来继续执行SplitFetcher任务. Idle: 如果SplitFetcher完成了所有Split的数据读取, 就会进入Idle状态. 对应的JVM线程状态为WAITING. Closed: 进入Idle状态的SplitFetcher将会被关闭而进入Closed状态. 从JVM线程的角度来看, SplitFetcher线程进入WAITING/TIME_WAITING状态的可能原因有两个: 在读取数据时, 即在SplitReader.fetch()中等待, 如KafkaConsumer.poll()会因为暂时获取不到数据而等待. assignedSplits和taskQueue均为空, 等待需要等待任务. SplitFetcher的生命周期受SourceReader的控制, 其控制逻辑如下图所示. 其中紫色阴影块会调用SplitFetcher对其生命周期进行控制. 上图展示了SourceReader是如何通过SplitFetcherManager来控制SplitFetcher任务的. 接下来我们再来分析下SplitFetcher内部的执行逻辑. 其整体执行流程如下图所示: 在SplitFetcher的run()方法中会通过一个while循环不断运行runOnce()方法. SourceReader在pollNext()方法中会调用maybeShutdownFinishedFetchers()来判断对应的SplitFetcher是否处于Idle状态, 如果是则调用SplitFetcher.shutdown()来关闭该SplitFetcher(设置closed为true), 之后runOnce()将进入步骤3.1. 如果各个Split之间的Watermark差距过大, SourceReader会调用SplitReader的pauseOrResumeSplits()或pause()来暂停当前任务, 之后runOnce()将进入步骤4.1. 后续会调用pauseOrResumeSplits()或resume()来继续任务. 如果当前没有被分配的Split, 也没有需要执行的任务(如添加Split), 那么SplitFetcher将进入WAITING状态, 直到addSplits(), pauseOrResumeSplits()或enqueueTask()(在KafkaSource中提交Offset时将调用次方法)被调用. SpliteFetcher是真正实现数据读取的任务, 理解它的生命周期和执行流程是理解新版Source接口的核心之一. Watermark对齐流程新Source接口中一个十分重要的特性就是Watermark对齐, 用于解决Event Time倾斜问题. 在Flink 1.15及之后的版本中可以通过如下方式指定对齐参数.123456WatermarkStrategy .&lt;Tuple2&lt;Long, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withWatermarkAlignment( &quot;alignment-group-1&quot;, // watermarkGroup: Watermark对齐的group名称 Duration.ofSeconds(20), // maxAllowedWatermarkDrift: 最大Watermark偏移 Duration.ofSeconds(1)); // updateInterval: SourceOperator上报Watermark及SourceCoordinator下发Watermark对齐的时间间隔 FLIP-296对SQL层Watermark相关的支持进行了完善, 支持在WITH参数中指定上述参数, 示例代码如下, 这一特性在Flink 1.18中发布.12345678910CREATE TABLE user_actions ( ... user_action_time TIMESTAMP(3), WATERMARK FOR user_action_time AS user_action_time - INTERVAL &apos;5&apos; SECOND) WITH ( &apos;scan.watermark.alignment.group&apos;=&apos;alignment-group-1&apos;, &apos;scan.watermark.alignment.max-drift&apos;=&apos;1min&apos;, &apos;scan.watermark.alignment.update-interval&apos;=&apos;1s&apos;, ...); Watermark的对齐需要SourceCoordinator和SourceOperator配合进行. 整体的交互流程如下图所示: SourceOperator会定期上报Watermark(时间间隔由updateInterval参数指定), 在实现上Mailbox线程最终会调用emitNext(), 如果SourceOperator不处于READING状态, 就会调用emitNextNotReading(), 它会在初始化时通过ProcessingTimeService启动一个定时任务, 定期调用emitLatestWatermark()向SourceCoordinator发送ReportWatermarkEvent事件, 最终SourceCoordinator.handleEventFromOperator()会处理该事件. SourceCoordinator会定期下发Watermark对齐事件(时间间隔由updateInterval参数指定), 在实现上其构造函数中会利用SourceCoordinator启动一个定时任务, 定期调用announceCombinedWatermark()向SourceOperator发送WatermarkAlignmentEvent, 最终SourceOperator.handleOperatorEvent()会处理该事件. 上文介绍了Watermark对齐的整体交互流程. 在具体实现上, SourceCoordinator端的实现比较简单, 就是将各个SourceOperator上报的Watermark聚合, 选出最小的Watermark值, 将其加上maxAllowedWatermarkDrift之后定期下发给SourceOperator, 作为允许读取的最大Watermark. SourceOperator端的Watermark对齐流程相对复杂一些. FLIP-182提出了SourceOperator级别的Watermark对齐, 可以支持一个SourceOperator仅读取一个Split的场景, 在Flink 1.15及之后的版本中可用. FLIP-217提出了Split级别的Watermark对齐, 支持一个SourceOperator读取多个Split的场景, 在Flink 1.17及之后的版本中可用. 整体的对齐流程如下图所示. SourceOperator级别的对齐在checkWatermarkAlignment()中实现, 其核心代码如下. 如果当前SourceOperator处于READING状态, 且需要对齐则重置waitingForAlignmentFuture变量, Mailbox主线程会调用getAvailableFuture()来获取SourceOperator的可用性, 如果此时waitingForAlignmentFuture不处于结束状态则代表当前SourceOperator需要阻塞等待Watermark对齐; 如果当前SourceOperator处于WAITING_FOR_ALIGNMENT状态, 且不需要对齐则结束waitingForAlignmentFuture, SourceOperator重新回到运行状态.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Internalpublic class SourceOperator&lt;OUT, SplitT extends SourceSplit&gt; extends AbstractStreamOperator&lt;OUT&gt; implements OperatorEventHandler, PushingAsyncDataInput&lt;OUT&gt;, TimestampsAndWatermarks.WatermarkUpdateListener { private CompletableFuture&lt;Void&gt; waitingForAlignmentFuture = CompletableFuture.completedFuture(null); private void checkWatermarkAlignment() { if (operatingMode == OperatingMode.READING) { checkState(waitingForAlignmentFuture.isDone()); if (shouldWaitForAlignment()) { operatingMode = OperatingMode.WAITING_FOR_ALIGNMENT; waitingForAlignmentFuture = new CompletableFuture&lt;&gt;(); } } else if (operatingMode == OperatingMode.WAITING_FOR_ALIGNMENT) { checkState(!waitingForAlignmentFuture.isDone()); if (!shouldWaitForAlignment()) { operatingMode = OperatingMode.READING; waitingForAlignmentFuture.complete(null); } } } // 如果lastEmittedWatermark大于currentMaxDesiredWatermark, 则需要等待 private boolean shouldWaitForAlignment() { return currentMaxDesiredWatermark &lt; lastEmittedWatermark; } // 由StreamTask触发调用, 当返回的CompletableFuture阻塞时, Mailbox线程将阻塞 @Override public CompletableFuture&lt;?&gt; getAvailableFuture() { switch (operatingMode) { case WAITING_FOR_ALIGNMENT: return availabilityHelper.update(waitingForAlignmentFuture); case OUTPUT_NOT_INITIALIZED: case READING: return availabilityHelper.update(sourceReader.isAvailable()); case SOURCE_STOPPED: case SOURCE_DRAINED: case DATA_FINISHED: return AvailabilityProvider.AVAILABLE; default: throw new IllegalStateException(&quot;Unknown operating mode: &quot; + operatingMode); } }} Split级别的Watermark对齐在checkSplitWatermarkAlignment()中实现, 其调用链较长, 如上图所示. 最终的实现在KafkaPartitionSplitReader.pauseOrResumeSplits()中, 通过KafkaConsumer提供的pause()和resume()两个方法分别用于停止和继续读取相应的Partition. 此外, 对于Multi-split Multi-threaded模式的实现, 由于一个SplitFetcher仅读取一个Split, 在需要对齐时可直接在SplitFetcherManager.pauseOrResumeSplits()通过SplitFetcher.pause()和SplitFetcher.resume()将指定的SplitFetcher暂停或唤醒. Checkpoint和Failover流程除了Split分配和数据读取流程, 在生产环境中还需要关注的是Checkpoint流程和Failover流程. KafkaSourceEnumerator的Checkpoint流程比较简单, 当Checkpoint触发时SourceCoordinator的checkpointCoordinatot()会调用KafkaSourceEnumerator的snapshotState(), 需要在状态中保存的仅有KafkaSourceEnumerator的Set&lt;TopicPartition&gt; assignedPartitions, 即已经分配的Partititions. 当Checpoint成功或失败时会分别回调notifyCheckpointComplete()和notifyCheckpointAborted(), 这两个函数其实都没有做任何实现. KafkaSourceReader的Checkpoint流程如下. 当Checkpoint触发时, KafkaSourceReader的snapshotState()将被调用用于保存当前Reader已经被分配的Split(SourceReaderBase中的splitStates变量保存了当前Reader已经被分配的所有Split). 当Checkpoint完成时notifyCheckpointComplete()会被调用, 用于向Kafka提交Offset. KafkaSourceEnumerator的Failover流程如下, 主要分为两部分: 如果出现Global Failover那么SourceCoordinator将会调用resetToCheckpoint()来重置整个KafkaSourceEnumerator, 其中的assignedPartitions会被初始化为Checkpoint中的状态, 之后按启动流程重新启动即可. 如果出现Partial Failover那么SourceCoordinator的subtaskReset()将会调用KafkaSourceEnumerator的addSplitsBack()来将上次Checkpoint之后还没有分配给Reader的Split分配给对应的Reader. KafkaReader的Failover流程比较简单, 重启时在SourceOperator的open()函数中会重新加载当前Reader中已经被分配的所有Split(即原来SourceReaderBase中的splitStates变量保存的内容), 并重新调用addSplits()分配给SplitFetcher. 总结本文以Kafka Connetcor为例, 详细介绍了FLIP-27中提出的新版Source接口的实现原理. 可以看到, 新版的Source接口在设计上还是比较巧妙的, 主要的亮点是解耦了分区发现和数据读取的流程, 简化了动态分区发现的实现. 另外通过对分区的抽象在流批处理上进行了统一. 不过Source的实现本身比较复杂, 实现一个生产可用的Source并不简单, 需要理解各个接口的执行和交互细节, 本文对其中的细节进行了详细介绍, 相信有助于实现新的Source Connector. 参考[1] FLIP-27: Refactor Source Interface[2] FLIP-27: Refactor Source Interface-Apache Mail Archives[3] FLIP-182: Support watermark alignment of FLIP-27 Sources[4] FLIP-217: Support watermark alignment of source splits[5] 漫谈Flink Source接口重构","link":"/flink-code-kafka-source.html"},{"title":"Glink SQL最佳实践 - GeoMesa SQL Connector应用","text":"本文已收录在合集Apche Flink原理与实践中. GeoMesa已经成为时空数据存储领域重要的索引中间件, 京东城市时空数据引擎JUST和阿里云的HBase Ganos均是在GeoMesa的基础上扩展而来. GeoMesa采用键值存储, 支持多种类型的存储后端, 如HBase, Kafka, Redis等. 相对于PostgreSQL+PostGIS这种基于R-tree索引的关系型存储, GeoMesa的存储方案更容易与HBase等现有的分布式数据库相结合, 从而直接利用底层数据库的分布式特性, 更适合时空大数据的存储以及实时场景的应用. 为在时空流计算中利用GeoMesa的高效写入和时空查询能力, Glink扩展Flink SQL Connector框架形成了Flink GeoMesa SQL Connector(简称GeoMesa SQL Connector), 支持使用Flink SQL读写GeoMesa. 本文通过实际的应用案例, 讲述如何在Flink SQL中使用GeoMesa. 在流计算中Flink+GeoMesa主要有以下两种使用场景: 时空数据管道 &amp; ETL: 以GeoMesa作为时空数据存储引擎, 通过Flink SQL构建实时的时空数据ETL管道, 将时空数据从文件, Kafka等数据源导入到GeoMesa; Lookup Join: 将维表存储在GeoMesa中, 通过Flink SQL进行流表与维表的空间Join, 在Glink中称为Spatial Dimention Join. 本文需要glink-1.0.0及以上版本, 可在Zepplin中运行, 关于Glink及zepplin的安装配置参考Glink文档. 为方便复现笔者提供了可直接运行的glink-geomesa.zpln, 下载后可直接在Zepplin打开运行. 时空数据管道 &amp; ETL在IoT等行业, 产生的大量时空数据一般会接入到Kafka, 之后经过清洗, 转换, 增强存入时空数据库, 这就需要建设时空数据ETL管道. Flink在实时数据ETL管道建设中已经起到了重要作用. 然而Flink不支持空间数据类型, 同时也缺乏与空间数据库, 如GeoMesa等的Connector. 为此Glink增加了GeoMesa SQL Connector, 支持与GeoMesa进行交互, 方便了时空数据ETL管道的建设. 在Glink中, 所有空间数据类型均用WKT格式的STRING类型表示, 同时通过Connector参数geomesa.spatial.fields指定空间类型字段和表示的几何类型. GeoMesa SQL Connector在写入GeoMesa时会将WKT转化为实际的几何对象. 下面通过一个实际的案例讲述如何利用Glink构建时空数据ETL管道. 在该案例中我们将CSV文件中的数据通过Flink SQL导入到GeoMesa中. CSV文件中每行代表一个空间点, 总共包含四列, 每列的含义是: 点ID, 点生成时间, 经度, 纬度. 以下是一个简单的案例.12341,2008-02-02 13:30:40,116.31412,70.894542,2008-02-02 13:30:44,116.31412,39.894543,2008-02-02 13:30:45,116.32674,39.895774,2008-02-02 13:30:49,116.31412,39.89454 这里将CSV文件作为source只是为了方便, source可以是Kafka, MySQL等Flink支持的任意组件. 首先创建Source Table, DDL语句如下.12345678910CREATE TABLE csv_table ( id STRING, dtg TIMESTAMP(0), lng DOUBLE NOT NULL, lat DOUBLE NOT NULL) WITH ( &apos;connector&apos; = &apos;filesystem&apos;, &apos;path&apos; = &apos;/path/to/point.csv&apos;, &apos;format&apos; = &apos;csv&apos;); 接着在使用命令行工具在GeoMesa中创建对应的物理表, 建表命令如下.1geomesa-hbase create-schema -c geomesa_table -s &quot;pid:String,dtg:Date,pt:Point&quot; -f geomesa_table 然后创建GeoMesa Sink Table.123456789101112CREATE TABLE geomesa_table ( pid STRING, dtg TIMESTAMP(0), pt STRING, PRIMARY KEY (pid) NOT ENFORCED) WITH ( &apos;connector&apos; = &apos;geomesa&apos;, &apos;geomesa.data.store&apos; = &apos;hbase&apos;, &apos;geomesa.schema.name&apos; = &apos;geomesa_table&apos;, &apos;geomesa.spatial.fields&apos; = &apos;pt:Point&apos;, &apos;hbase.catalog&apos; = &apos;geomesa_table&apos;); 最后通过如下语句即可实现将数据从CSV文件导入GeoMesa, 完成数据管道的构建.12INSERT INTO geomesa_table SELECT id, dtg, ST_AsText(ST_Point(lng, lat)) FROM csv_table; Lookup Join在流计算中, 流表与维表的关联是一项重要的基础功能. Flink可通过Lookup Join实现流表与维表的关联. 然而, 目前Flink的Lookup Join只支持等值Join, 对于时空数据而言, 通常需要基于流表与维表中对象的空间关系进行Join. 为此, Glink抽象出了Spatial Dimension Join, 支持基于空间关系的Lookup Join, 目前Glink的Spatial Dimension Join支持距离Join, 相交Join和包含Join. Spatial Dimension Join具有大量的应用场景, 比如: 地理围栏应用, 流表中每条记录表示行人或车辆的轨迹点, 维表存储在GeoMesa中, 每条记录都是一个由多边形表示的地理围栏. 为了判断流表中的轨迹点是否出入了某个地理围栏, 可以将流表与维表做一个包含Join, 若某个轨迹点被包含在某个多边形围栏中, 则这两条记录会执行Join. 订单调度应用, 流表中每条记录都表示一个订单, 包含订单送达目的地的经纬度坐标, 维表存储在GeoMesa中, 每条记录都是由经纬度点表示的仓库位置. 为了获取与订单位置在某个距离范围内的仓库, 可以将流表与维表做一个距离Join, 若订单位置与仓库位置在距离范围内, 则这两条记录会执行Join. 在Glink中可以通过geomesa.temporal.join.predict这一Connector参数指定进行何种类型的空间join: R:&lt;distance&gt;表示距离Join, 流表中空间对象与维表中空间对象距离在distance之内的记录都会被Join, distance的单位为米; I表示相交Join, 流表中空间对象与维表中空间对象在空间上相交的记录都会被Join; +C表示正相交Join, 流表中空间对象若在空间上包含维表中空间对象, 则两条记录会被Join; -C表示负相交Join, 维表中空间对象若在空间上包含流表中空间对象, 则两条记录会被Join. 下面通过具体的案例讲述如何使用Glink进行Spatial Dimension Join. 相交/包含join我们通过地理围栏应用讲述如何在Glink中进行相交Join或包含Join. 在地理围栏应用中, 流表中的一条记录通常是行人或车辆的轨迹点, 包含一些非空间属性及轨迹点的经纬度坐标. 维表中的一条记录通常代表一块地理区域, 包含一些非空间属性及地理区域的空间范围(由多边形表示). 在本例中, 流表来自CSV文件, 维表存储在GeoMesa中. 通过相交Join或负相交Join可以实现轨迹点与地理围栏的关联. 首先定义流表, DDL语句如下.1234567891011CREATE TABLE csv_point ( id STRING, dtg TIMESTAMP(0), lng DOUBLE NOT NULL, lat DOUBLE NOT NULL, proctime AS PROCTIME())WITH ( &apos;connector&apos; = &apos;filesystem&apos;, &apos;path&apos; = &apos;/path/to/traj_point.csv&apos;, &apos;format&apos; = &apos;csv&apos;); GeoMesa维表的定义语句如下, geomesa.temporal.join.predict用于指定空间join的类型, 在地理围栏应用中使用I和-C可以达到相同的结果. 但是使用I有更高的效率.1234567891011121314CREATE TABLE geomesa_area ( pid STRING, dtg TIMESTAMP(0), geom STRING, PRIMARY KEY (pid) NOT ENFORCED)WITH ( &apos;connector&apos; = &apos;geomesa&apos;, &apos;geomesa.data.store&apos; = &apos;hbase&apos;, &apos;geomesa.schema.name&apos; = &apos;restricted_area&apos;, &apos;geomesa.spatial.fields&apos; = &apos;geom:Polygon&apos;, &apos;geomesa.temporal.join.predict&apos; = &apos;I&apos;, &apos;hbase.zookeepers&apos; = &apos;localhost:2181&apos;, &apos;hbase.catalog&apos; = &apos;restricted_area&apos;); 通过如下语句进行Spatial Dimension Join.1234SELECT A.id AS point_id, A.dtg, ST_AsText(ST_Point(A.lng, A.lat)) AS point, B.pid AS area_id FROM csv_point AS A LEFT JOIN geomesa_area FOR SYSTEM_TIME AS OF A.proctime AS B ON ST_AsText(ST_Point(A.lng, A.lat)) = B.geom; 距离join我们通过订单调度应用讲述如何在Glink中进行距离join. 在订单调度应用中, 流表中每条记录都表示一个订单, 包含相关的非空间属性及订单送达目的地的经纬度坐标; 维表存储在GeoMesa中, 每条记录都是由经纬度点表示的仓库位置. 在订单调度应用中, 通常需要为每个订单关联某个距离范围内的仓库, 用于订单的分发调度. 这可以通过Glink的距离join实现. 首先定义流表, DDL语句如下.1234567891011CREATE TABLE csv_order ( id STRING, dtg TIMESTAMP(0), lng DOUBLE, lat DOUBLE, proctime AS PROCTIME())WITH ( &apos;connector&apos; = &apos;filesystem&apos;, &apos;path&apos; = &apos;/path/to/csv&apos;, &apos;format&apos; = &apos;csv&apos;); 然后定义GeoMesa维表.12345678910111213CREATE TABLE geomesa_warehouse ( id STRING, geom STRING, PRIMARY KEY (id) NOT ENFORCED)WITH ( &apos;connector&apos; = &apos;geomesa&apos;, &apos;geomesa.data.store&apos; = &apos;hbase&apos;, &apos;geomesa.schema.name&apos; = &apos;warehouse_point&apos;, &apos;geomesa.spatial.fields&apos; = &apos;geom:Point&apos;, &apos;geomesa.temporal.join.predict&apos; = &apos;R:400000&apos;, &apos;hbase.zookeepers&apos; = &apos;localhost:2181&apos;, &apos;hbase.catalog&apos; = &apos;warehouse_point&apos;); 最后通过如下语句即可实现距离join.1234SELECT A.id AS order_id, A.dtg, ST_AsText(ST_Point(A.lng, A.lat)) AS order_point, B.id AS warehouse_id FROM csv_order AS A LEFT JOIN geomesa_warehouse FOR SYSTEM_TIME AS OF A.proctime AS B ON ST_AsText(ST_Point(A.lng, A.lat)) = B.geom; 参考[1] Flink SQL最佳实践 - HBase SQL Connector应用","link":"/glink-sql-geomesa.html"},{"title":"Flink最佳实践 - Table与DataStream互相转换","text":"本文已收录在合集Apche Flink原理与实践中. Flink SQL在很多场景下可以简化实时数据处理管道的开发，然而SQL的表达能力毕竟有限, 一些复杂的处理逻辑还是不得不借助DataStream API实现, 如复杂Lookup Join, 自定义定时器处理等. 然而如果所有处理逻辑都用DataStream API实现, 则又需要编写大量的Java代码, 不仅效率低下, 而且相比于SQL更难维护. 这时候比较好的方法是用SQL进行尽可能多的处理, 然后将结果转换为DataStream借助DataStream API实现复杂的自定义处理逻辑. 这就需要在Flink中实现Table与DataStream的互相转换. 本文首先介绍Table与DataStream互相转换的使用场景, 之后具体介绍转换方法及需要注意的细节问题. 应用场景文章开头已经说到, 混合使用Table API &amp; SQL和DataStream API既可以借助SQL的表达能力提升开发效率, 又可以借助DataStream API实现SQL难以表达的复杂处理逻辑. 结合笔者的经验, 在以下这些场景中, 都适合混用Table API &amp; SQL和DataStream API. 以SQL为主的作业: 整个实时数据处理管道的大部分处理逻辑都可以用SQL表达, 但是存在部分难以表达的处理逻辑, 比如复杂Lookup Join, 自定义定时器处理等. 这种场景下可以在适当的时候将处理结果转换为DataStream进行操作. 以DataStream为主的作业: 整个处理管道都比较复杂, SQL几乎不能进行处理, 需借助DataStream API实现. 这种场景下可以借助Flink的Table &amp; SQL Connector进行数据的读写, 中间的处理逻辑使用DataStream实现. 因为多数场景下, Table &amp; SQL Connector的完善度是高于DataStream Connector的, 例如开源版本中的HBase Connector并不支持DataStream. 具体案例入门案例以下是一个来自Flink文档中的入门案例, 可以看到, Flink提供了十分简单的API用于支持Table和DataStream的转换. 其转换接口在StreamTableEnvironment中. 在该案例中用到了以下两个接口: StreamTableEnvironment.fromDataStream用于将DataStream转为Table, 该接口只能转换Insert-Only的DataStream. 对应的还有StreamTableEnvironment.fromChangelogStream接口, 用于转换Changelog的DataStream. StreamTableEnvironment.toChangelogStream用于将Table转为DataStream, 该接口可转换为Changelog的DataStream. 对应的还有StreamTableEnvironment.toDataStream可转换为Insert-Only的DataStream. 1234567891011121314151617181920212223242526272829public class TableToDataStreamExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a DataStream DataStream&lt;Row&gt; dataStream = env.fromElements( Row.of(&quot;Alice&quot;, 12), Row.of(&quot;Bob&quot;, 10), Row.of(&quot;Alice&quot;, 100)); // interpret the insert-only DataStream as a Table Table inputTable = tableEnv.fromDataStream(dataStream).as(&quot;name&quot;, &quot;score&quot;); // register the Table object as a view and query it // the query contains an aggregation that produces updates tableEnv.createTemporaryView(&quot;InputTable&quot;, inputTable); Table resultTable = tableEnv.sqlQuery( &quot;SELECT name, SUM(score) FROM InputTable GROUP BY name&quot;); // interpret the updating Table as a changelog DataStream DataStream&lt;Row&gt; resultStream = tableEnv.toChangelogStream(resultTable); // add a printing sink and execute in DataStream API resultStream.print(); env.execute(); }} 仿真场景从上述入门案例中, 我们可以对Flink中Table与DataStream的转换有一个整体的了解. 为了更深入地介绍Table与DataStream相互转换的应用场景, 笔者设计了一个在现实生活中可能出现的一个场景. 希望借此能够启发读者在更多的场景中应用Table与DataStream的互转, 以更高效的方式实现流处理管道. 在生活中我们都有使用高德等地图软件和美团等生活软件的经历, 现在假设地图软件推出了一个新的功能, 可以根据你当前所处的位置实时推荐距离最近的餐厅. 这个功能在实现上就需要将用户的实时轨迹点跟存储于数据库中的餐厅表做Join. 这个功能显然不能直接用Flink SQL实现, 因为Flink SQL的Lookup Join只支持等值连接, 而这里需要从数据库中查询距离当前点最近的餐厅, 然后将二者关联. 这里笔者默认使用Flink解决该场景, 在实际的应用架构中可能并不适合使用Flink实现. 上述场景在DataStream API中很容易实现, 我们只需利用ProcessFunction, 对于每一条记录, 都去餐厅表中查询到最近的餐厅后关联即可. 为了演示上述场景, 我们首先在MySQL中创建一个restaurant_table表, 并在其中插入3条记录, SQL语句如下. 12345678910111213CREATE TABLE restaurant_table ( id BIGINT auto_increment NOT NULL, name varchar(100) NOT NULL, x FLOAT NOT NULL, y FLOAT NOT NULL, CONSTRAINT restaurant_table_PK PRIMARY KEY (id))ENGINE=InnoDBDEFAULT CHARSET=utf8mb4COLLATE=utf8mb4_0900_ai_ci;INSERT INTO restaurant_table (name, x, y) VALUES (&apos;KFC&apos;, 1, 2), (&apos;McDonald&apos;, 5, 5), (&apos;Burger King&apos;, 8, 8); 有了上述准备后, 我们就可以在Flink中通过以下代码实现任意的Lookup Join, 这段代码主要由以下几部分组成: 第一部分使用SQL定义了源表和结果表, 这里为了方便使用了Datagen作为源表, 使用Print作为结果表, 在真实环境中可替换成对应的Connector. 第二部分从源码中读取数据并进行解析处理, 示例中并没有过多的解析工作, 只是使用SELECT *将数据全部读取出来, 在实践中可以在这里加入所需的解析处理或计算逻辑. 最后将Table转换为了DataStream. 第三部分使用DataStream API进行了Lookup Join, 为每个轨迹点关联了最近的餐厅名称. 主要的实现在JdbcRowLookupFunction中, 这是笔者自定义的一个继承于ProcessFunction的类, 其实现逻辑参考了Flink源码中的JdbcRowDataLookupFunction. 第四部分将关联后的结果重新转换为Table, 并使用INSERT INTO写入结果表中. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class LookupJoinExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // PART 1: define source and result tables tableEnv.executeSql( &quot;CREATE TABLE traj_point (\\n&quot; + &quot; id BIGINT,\\n&quot; + &quot; `user` STRING,\\n&quot; + &quot; x FLOAT,\\n&quot; + &quot; y FLOAT,\\n&quot; + &quot; `time` TIMESTAMP(3)\\n&quot; + &quot;) WITH (\\n&quot; + &quot; &apos;connector&apos; = &apos;datagen&apos;,\\n&quot; + &quot; &apos;fields.id.min&apos; = &apos;1&apos;,\\n&quot; + &quot; &apos;fields.id.max&apos; = &apos;100&apos;,\\n&quot; + &quot; &apos;fields.user.length&apos; = &apos;5&apos;,\\n&quot; + &quot; &apos;fields.x.min&apos; = &apos;0&apos;,\\n&quot; + &quot; &apos;fields.x.max&apos; = &apos;10&apos;,\\n&quot; + &quot; &apos;fields.y.min&apos; = &apos;0&apos;,\\n&quot; + &quot; &apos;fields.y.max&apos; = &apos;10&apos;\\n&quot; + &quot;)&quot;); tableEnv.executeSql( &quot;CREATE TABLE traj_point_with_restaurant (\\n&quot; + &quot; id BIGINT,\\n&quot; + &quot; `user` STRING,\\n&quot; + &quot; x FLOAT,\\n&quot; + &quot; y FLOAT,\\n&quot; + &quot; `time` TIMESTAMP(3),\\n&quot; + &quot; restaurant STRING\\n&quot; + &quot;) WITH (\\n&quot; + &quot; &apos;connector&apos; = &apos;print&apos;\\n&quot; + &quot;)&quot;); // Part 2: get source table and convert to DataStream Table table = tableEnv.sqlQuery(&quot;SELECT * FROM traj_point&quot;); DataStream&lt;Row&gt; dataStream = tableEnv.toDataStream(table); // Part 3: do lookup join with DataStream API JdbcConnectionOptions connectionOptions = new JdbcConnectionOptions .JdbcConnectionOptionsBuilder() .withDriverName(&quot;com.mysql.cj.jdbc.Driver&quot;) .withUrl(&quot;jdbc:mysql://localhost:3306/test&quot;) .withUsername(&quot;root&quot;) .withPassword(&quot;0407&quot;) .build(); JdbcLookupOptions lookupOptions = new JdbcLookupOptions .Builder() .setCacheMaxSize(100) .setCacheExpireMs(10000) .setMaxRetryTimes(3) .setCacheMissingKey(false) .build(); String lookupSQL = &quot;SELECT name FROM restaurant_table &quot; + &quot;ORDER BY SQRT(POW(x - ?, 2) + POW(y - ?, 2)) LIMIT 1&quot;; JoinFunction&lt;Row, Row, Row&gt; joinFunction = Row::join; RowLookupFunction lookupFunction = new JdbcRowLookupFunction( connectionOptions, lookupOptions, lookupSQL, new int[] {2, 3}, joinFunction, true); RowTypeInfo rowTypeInfo = new RowTypeInfo( new TypeInformation[] { Types.LONG, Types.STRING, Types.FLOAT, Types.FLOAT, Types.LOCAL_DATE_TIME, Types.STRING}, new String[] {&quot;id&quot;, &quot;user&quot;, &quot;x&quot;, &quot;y&quot;, &quot;time&quot;, &quot;restaurant&quot;}); DataStream&lt;Row&gt; resultDataStream = dataStream.process(lookupFunction).returns(rowTypeInfo); // PART 4: convert join result to Table Table resultTable = tableEnv.fromDataStream(resultDataStream); tableEnv.executeSql(&quot;INSERT INTO traj_point_with_restaurant SELECT * FROM &quot; + resultTable); env.execute(); }} 从上述案例中可以看到, 借助Table与DataStream的互相转换, 可以很方便地实现复杂的Lookup Join. 其实上述代码中很大一部分都是模板代码, 如果有需要可以更进一步把这个Lookup Join进行封装, 最终做到只需要传入相应的配置参数和SQL即可. 实际上在生产实践中, 我们完全可以把这些常用的功能构建成一个通用的模板, 这样后续根据具体场景传入具体的参数和SQL即可, 而不需要编写任何Java代码. 注意事项相信通过上述案例读者已经对Table和DataStream互转的方法和使用场景有了比较深入的了解, 本节讲述在互转过程中需要注意的几个细节, 这些细节在生产实践中都需要格外注意. Insert-Only和Changelog流的区别上文也说到了, 对于Insert-Only流和Changelog流, Flink提供了两类互转接口: 对于Insert-Only流: StreamTableEnvironment.fromDataStream将DataStream转为Table; StreamTableEnvironment.toDataStream将Table转为DataStream. 对于Changelog流: StreamTableEnvironment.fromChangelogStream将DataStream转为Table; StreamTableEnvironment.toChangelogStream将Table转为DataStream. 在转换过程中需要注意的是, 对于Changelog流, 如果使用了fromDataStream接口和toDataStream接口将会出错, 其错误提示大致如下. 12345# 误用fromDataStream接口将Changelog的DataStream转为TableError during input conversion. Conversion expects insert-only records but DataStream API record contains: UPDATE_BEFORE# 误用toChangelogStream接口将Changelog的Table转为DataStreamTable sink &apos;*anonymous_datastream_sink$2*&apos; doesn&apos;t support consuming update changes which is produced by node [...] 事实上, Insert-Only流只是Changelog流的一种特例, 为了安全起见, 在进行Table和DataStream转换时, 我们可以总是使用fromChangelogStream接口和toChangelogStream. 在性能上不会有任何影响. DataStream转Table的Schema确定Table包含的是结构化的数据, 因此在将DataStream转为Table时需要有一个Schema来指定如何将DataStream中的Java对象对应到Table中的一行记录. 对于入门案例中的代码, 也就是以下代码中的EXAMPLE 1, Flink会自动推断出其Schema类型, 默认的Schema是(f0STRING,f1INT)这样的形式, 其字段名是自动分配的. 可以在转换为Table时通过as方法指定别名. 使用fromCollection接口生成的DataStream同样可以自动类型推断. 然而如果不是由上述两个接口生成的DataStream, Flink的自动类型推断很可能失效, 比如EXAMPLE 2中将DataStream中的记录识别为了RAW类型, 它其实把整个ROW识别为了一个字段, 而无法将ROW中的字段分别识别. 这时候可以通过returns方法为DataStream明确指定结果类型, 例如EXAMPLE 3中通过RowTypeInfo指定了每个字段的类型和名称. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);// === EXAMPLE 1 ===DataStream&lt;Row&gt; dataStream = env.fromElements( Row.of(&quot;Alice&quot;, 12), Row.of(&quot;Bob&quot;, 10), Row.of(&quot;Alice&quot;, 100));Table inputTable = tableEnv.fromChangelogStream(dataStream).as(&quot;name&quot;, &quot;score&quot;);inputTable.printSchema();// prints:// (// `name` STRING,// `score` INT// )// === EXAMPLE 2 ===DataStream&lt;Row&gt; dataStream = env.fromElements( Row.of(&quot;Alice&quot;, 12), Row.of(&quot;Bob&quot;, 10), Row.of(&quot;Alice&quot;, 100));DataStream&lt;Row&gt; processedStream = dataStream.process(new ProcessFunction&lt;Row, Row&gt;() { @Override public void processElement(Row row, Context context, Collector&lt;Row&gt; collector) { collector.collect(row); }});Table inputTable = tableEnv.fromChangelogStream(processedStream);inputTable.printSchema();// prints:// (// `f0` RAW(&apos;org.apache.flink.types.Row&apos;, &apos;...&apos;)// )// === EXAMPLE 3 ===DataStream&lt;Row&gt; dataStream = env.fromElements( Row.of(&quot;Alice&quot;, 12), Row.of(&quot;Bob&quot;, 10), Row.of(&quot;Alice&quot;, 100));RowTypeInfo rowTypeInfo = new RowTypeInfo( new TypeInformation&lt;?&gt;[] {Types.STRING, Types.INT}, new String[] {&quot;name&quot;, &quot;score&quot;});DataStream&lt;Row&gt; processedStream = dataStream.process(new ProcessFunction&lt;Row, Row&gt;() { @Override public void processElement(Row row, Context context, Collector&lt;Row&gt; collector) { collector.collect(row); }}).returns(rowTypeInfo);Table inputTable = tableEnv.fromChangelogStream(processedStream);inputTable.printSchema();// prints:// (// `name` STRING,// `score` INT// ) TypeInformation和DataType的映射关系在Flink中, DataStream API使用TypeInformation来描述流中传输的记录类型, 而Table API在内部使用RowData表示记录, 用户可以使用DataType声明RowData转换成的外部数据类型. DataType用于表示SQL数据类型, 比TypeInformation更丰富, 因此在进行Table与DataStream互转时需要注意其记录类型的转换关系. 在将Table转换为DataStream时, Flink会保证将Table的DataType转换为合适的TypeInformation类型. 重点是在DataStream转换为Table时, 需要注意其对应的转换关系, 完整的转换关系可参考TypeInfoDataTypeConverter, 其中需要格外注意的几种类型是: Types.LONG转换为BIGINT, 对应Java中的Long类型, 注意不要使用Types.BIG_INT. 时间类型中 Types.LOCAL_DATE转换为DATE, 对应Java中的LocalDate. Types.LOCAL_TIME转换为TIME, 对应Java中的LocalTime. Types.LOCAL_DATE_TIME转换为TIMESTAMP, 对应Java中的LocalDateTime. Types.INSTANT转换为TIMESTAMP_LTZ, 对应Java中的Instant. Types.SQL_DATE转换为DATE, 对应Java中的java.sql.Date. Types.SQL_TIME转换为TIME, 对应Java中的java.sql.Time. Types.SQL_TIMESTAMP转换为TIMESTAMP, 对应Java中的java.sql.Timestamp. Watermark传播Watermark的传播也是在进行Table与DataStream互转时格外需要注意的一点. 从Table转为DataStream时, Event-time和Watermark都会被保留, 即如果在Table中指定了Watermark, 在DataStream中不再需要指定Watermark即可进行时间类型操作. 从DataStream转为Table时, 在默认情况下Event-time和Watermark都不会进行传播, 必须通过Schema指定Watermark策略, 具体如以下代码所示. 指定Watermark策略的表达式与SQLWATERMARK FOR rowtime_column AS watermark_strategy中的watermark_strategy一致. 如果在DataStream中已经申明了Watermark, 可以使用SOURCE_WATERMARK()直接复用. 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class WatermarkExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); tableEnv.executeSql( &quot;CREATE TABLE source (\\n&quot; + &quot; id BIGINT,\\n&quot; + &quot; price DECIMAL(32,2),\\n&quot; + &quot; buyer STRING,\\n&quot; + &quot; order_time TIMESTAMP(3),\\n&quot; + &quot; WATERMARK FOR order_time AS order_time - INTERVAL &apos;5&apos; SECOND\\n&quot; + &quot;) WITH (\\n&quot; + &quot; &apos;connector&apos; = &apos;datagen&apos;,\\n&quot; + &quot; &apos;fields.id.min&apos; = &apos;1&apos;,\\n&quot; + &quot; &apos;fields.id.max&apos; = &apos;1000&apos;,\\n&quot; + &quot; &apos;fields.price.min&apos; = &apos;0&apos;,\\n&quot; + &quot; &apos;fields.price.max&apos; = &apos;10000&apos;,\\n&quot; + &quot; &apos;fields.buyer.length&apos; = &apos;5&apos;\\n&quot; + &quot;)&quot;); Table table = tableEnv.sqlQuery(&quot;SELECT * FROM source&quot;); DataStream&lt;Row&gt; dataStream = tableEnv.toDataStream(table); Table resultTable = tableEnv.fromDataStream(dataStream, Schema.newBuilder() .column(&quot;id&quot;, &quot;BIGINT&quot;) .column(&quot;price&quot;, &quot;DECIMAL(32,2)&quot;) .column(&quot;buyer&quot;, &quot;STRING&quot;) .column(&quot;order_time&quot;, &quot;TIMESTAMP(3)&quot;) .watermark(&quot;order_time&quot;, &quot;SOURCE_WATERMARK()&quot;) .build()); resultTable.printSchema(); tableEnv.createTemporaryView(&quot;result_table&quot;, resultTable); Table table1 = tableEnv.sqlQuery(&quot;SELECT window_start, window_end, SUM(price)\\n&quot; + &quot; FROM TABLE(\\n&quot; + &quot; TUMBLE(TABLE result_table, DESCRIPTOR(order_time), INTERVAL &apos;10&apos; SECONDS))\\n&quot; + &quot; GROUP BY window_start, window_end&quot;); table1.execute().print(); env.execute(); }} 总结本文通过一个仿真案例介绍了Flink中Table与DataStream互转的使用场景, 虽然随着Flink SQL的不断完善, 我们可以借助SQL快速实现绝大多数的实时处理管道, 然而还是有部分场景难以直接使用SQL实现. 这时候通过SQL构建整个实时处理管道的主体, 在适当位置插入DataStream API代码是当前来看最好的选择. 本文最后总结了几个Table与DataStream互转的几个注意事项, 都是在生产实践中需要格外注意的. 参考[1] Flink Documentation - DataStream API Integration","link":"/flink-table-datastream-convert.html"},{"title":"Flink最佳实践 - Watermark原理及实践问题解析","text":"本文已收录在合集Apche Flink原理与实践中. Watermark在Google的The Dataflow Model论文中被首次提出, 它在基于Event Time的流处理中具有重要作用, 是一种平衡计算结果准确性和延迟的机制. 虽然Watermark的概念不难理解, Flink中也有完善的Watermark策略, 但是在实际场景中生成合理的Watermark却并非那么简单, 在并行流下更是可能会出现多种问题. 本文在简单介绍Watermark的背景及概念之后, 详细介绍Flink在DataStream API和SQL API中对Watermark的支持, 接着解析在并行流下Watermark可能产生的一些问题, 最后通过一个具体案例介绍如何生成合理的Watermark. Watermark背景上文也说到了Watermark是在The Dataflow Model论文中提出的, 事实上The Dataflow Model对流处理来说是具有划时代意义的. 在Dataflow Model提出之前, 由于缺乏相关的理论指导, 开源的流处理引擎大多只能支持近似计算. 因此, 在当时Lambda架构是一种普遍的选择, 即采用两条数据处理链路, 使用批处理引擎产生准确但延迟高的结果, 使用流处理引擎产生低延迟但近似的结果. Dataflow Model通过引入Time Domain, Watermark, Window等一系列概念, 在理论上统一了流处理与批处理, 认为批处理是流处理的一种特例, 通过一些机制时可以实现准确且低延迟的流处理的. 在Streaming 101中作者也说到了设计良好的流系统实际上提供了严格的批处理功能超集. 关于Watermark与流处理更详细的背景可以阅读深入理解流计算中的 Watermark, 本文不再过多赘述. Watermark概念关于Watermark的概念, Flink文档中的描述比较容易理解, 以下是文档中的原话. A Watermark(t) declares that event time has reached time t in that stream, meaning that there should be no more elements from the stream with a timestamp t’ &lt;= t (i.e. events with timestamps older or equal to the watermark). 简单来说, 一个Watermark就是一个标识, 一个时间戳为$t$的Watermark表示Event Time小于或等于$t$的事件都已经到达. 有了这个前提, 基于Event Time的窗口计算才能产生准确的结果, 例如, 如果一个时间窗口的结束时间为$t_0$, 当前已经产生的最大Watermark为$t_1$, 并且$t_1 &gt; t_0$, 那么现在触发该窗口的计算可以得到准确的结果, 因为属于该窗口的数据都已经到达. 为了更好地理解Watermark的含义, 这里举两个具体的例子. 下图(来自Flink文档)是带有时间戳的事件流, 由于事件是严格按顺序到达的, 因此Watermark的生成就非常容易, 只需要周期性地将当前某个事件携带的时间戳作为Watermark即可. 在严格按时间顺序到达的事件流中生成Watermark十分简单, 但是现实场景中的事件往往是乱序点的, 如下图所示. 对于这种情况, 如何生成Watermark就需要多方权衡了. 进一步思考其实可以发现, 之所以需要Watermark是因为在现实中Event Time总是滞后于Processing Time的, 这是由于现实中的数据源在地理上分散, 且其所处的硬件和网络环境各异, 将数据传输到处理中心存在延迟, 且可能各个数据源的延迟各不相同, 这就进一步导致了数据乱序. 如果Processing Time和Event Time总是同步, 那么处理时也就不会有乱序数据, 也就不需要Watermark. 通过下图我们可以进一步理解Event Time, Processing Time, Watermark与数据乱序之间的关系. 首先从图(a)中我们可以明确以下内容: 理想状态下数据一旦产生立即被处理, 即Event Time和Processing Time之间不存在偏差, 也就是图中斜率为1虚线展示的情况, 它表示Event Time总是等于Processing Time. 真实状态下由于处理管道引入的随机延时, Processing Time总是滞后于Event Time, 也就是图中红色曲线展示的情况, 它表示Processing Time总是大于Event Time. 曲线与虚线在纵轴方向的差值为Processing Time Lag, 也就是处理管道引入的时间误差, 以图中的状态为例, 发生在$t_1$时刻的事件由于管道误差, 直到$t_2(t_2&gt;t_1)$时刻才被处理, 纵轴方向上$t_2-t_1$的值就称为Processing Time Lag. 曲线与虚线在横轴方向的差值为Event Time Skew, 可以理解为由于管道误差而使得计算引擎观察到的数据存在滞后性, 以图中的状态为例, 在$t_2$时刻只能观察到$t_2-t_1$时刻以前的数据, 横轴方向上$t_2-t_1$的值就称为Event Time Skew. 明确以上概念后我们再来看图(b): 图中有三个时间窗口$[t_0, t_1), [t_1, t_2), [t_2, t_3)$, 蓝色横线是Watermark, 这里取目前为止观察到的最大事件时间戳为Watermark. 图中圆点表示事件. A和B, C和D之间出现了数据乱序, 因为A的Event Time小于B, 但A的Processing Time大于B, 也就是说A比B先发生, 但是达到处理引擎的时间比B晚. C和D之间同理. 不过A和B, C和D之间的数据乱序并不影响窗口计算的准确性. 图中E也是一个乱序点, 它属于第二个窗口, 但是在大于$t_2$的Watermark生成之前E并未到达, 也就是说E是迟到数据, 在E到达之前窗口已经触发计算. 这里也可以看出在乱序情况下取当前最观察到的最大时间戳为Watermark并不是完美的方案. 最简单的解决方案是增大Event Time Skew, 我们可以将当前观察到的最大事件时间戳减去一个固定值作为Watermark, 如图中黄色虚线所示, 若当前观察到的最大时间戳为$t_2$, 那么我们将$t’$作为Watermark, 由于$t’$小于E的事件时间, 因此在E到达时窗口还未触发计算, 就能更大程度地容忍乱序. 从以上案例也可以发现, 为实现中的数据流生成Watermark并不简单. 最简单的方法是以当前观察到的最大时间戳减去一个固定值为Watermark, 这也是Flink内置的BoundedOutOfOrdernessWatermarks策略的实现原理. 至于这个固定值是多少, 只能通过观察具体应用的数据延迟状况来设定了. 如果这个值设置的太大, 那么虽然避免了数据迟到, 但是却增加了窗口触发计算的延迟, 如果设置得太小又会导致窗口过早触发计算, 从而使得结果不准确. 这也可以看出Watermark可以用来平衡计算结果的准确性和延迟. 如果从Watermark的角度来审视一下批处理和流处理的关系我们可以发现: 在批处理中我们实际上是使用了非常宽松的Watermark策略, 比如我们通常会在当天处理前一天的数据, 这样的Watermark策略可以理解为将当前观察到的数据的最大时间戳减去数个小时作为Watermark. 在这样宽松的Watermark策略下, 总能保证在批处理程序启动时所有数据已经全部到达, 因此产生准确的结果. 而在流处理中我们通常会使用紧迫的Watermark策略, 以更快得到处理结果, 这在降低延迟的情况下可能牺牲一定的准确性. 从中也可以看出, Watermark是统一流处理与批处理的重要理论依据. 实际上, 如果把Watermark生成抽象为一个算法, 那么其输入就是当前已经观察到的数据, 输出就是Watermark. 关键在于如何根据观察到的数据生成更好的Watermark, 尽可能使得计算结果准确并降低延迟. 本文后面会根据一个具体的案例, 来分析如何根据现实的数据情况生成相对合理的Watermark. Flink中的WatermarkFlink DataStream中的WatermarkFlink的DataStream API提供了对Watermark的完善支持, 不仅内置了一些常用的Watermark生成器, 也提供了扩展接口, 用户可实现自己的Watermark生成算法. Watermark的生成Flink 1.11中对Watermark相关的API进行了重构, 具体可见FLIP-126. 由于旧API已经废弃, 且新API提供了更好的抽象, 本文只讲述新API的使用方法. 新API提供了WatermarkStrategy接口, 用于组装WaterMarkGenerator(用于实现Watermark生成算法)和TimestampAssigner(用于指示如何从事件中提取事件时间). 在DataStream API中有两种使用WatermarkStrategy的方法: 1) 直接在Source中给出; 2) 调用DataStream的assignTimestampsAndWatermarks方法. 具体使用方式如下.123456789101112131415161718final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 方法一: 在Source中给定WatermarkStrategyDataStream&lt;Tuple2&lt;Long, String&gt;&gt; streamSource = env.fromSource( source, WatermarkStrategy .&lt;Tuple2&lt;Long, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withTimestampAssigner((event, timestamp) -&gt; event.f0), &quot;source&quot;);// 方法二: 调用DataStream的方法DataStream&lt;Tuple2&lt;Long, String&gt;&gt; streamSource = env.fromSource( source, WatermarkStrategy.noWatermarks(), &quot;source&quot;);streamSource = streamSource.assignTimestampsAndWatermarks(WatermarkStrategy .&lt;Tuple3&lt;Integer, String, Long&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withTimestampAssigner((t, time) -&gt; t.f2)); 尽管上述两种方法都可实现Watermark生成, 但应当尽可能使用第一种方法, 第二种方法在并行流下可能出现难以预料的问题, 后文会进一步分析这种问题. WatermarkStrategy中的TimestampAssigner一般根据数据格式传入相应的Lambda函数提取时间戳即可, 因此这里不再赘述. 重点是WaterMarkGenerator, 它是Watermark生成算法的实现, Flink内置了两种实现: WatermarkStrategy.forMonotonousTimestamps是以当前最大时间戳为Watermark的实现; WatermarkStrategy.forBoundedOutOfOrderness是以当前最大时间戳减去固定值为Watermark的实现. 以上两种算法的实现类都是BoundedOutOfOrdernessWatermarks, 我们可以看下它的源代码.1234567891011121314151617181920212223242526272829public class BoundedOutOfOrdernessWatermarks&lt;T&gt; implements WatermarkGenerator&lt;T&gt; { /** The maximum timestamp encountered so far. */ private long maxTimestamp; /** The maximum out-of-orderness that this watermark generator assumes. */ private final long outOfOrdernessMillis; /** * Creates a new watermark generator with the given out-of-orderness bound. * * @param maxOutOfOrderness The bound for the out-of-orderness of the event timestamps. */ public BoundedOutOfOrdernessWatermarks(Duration maxOutOfOrderness) { checkNotNull(maxOutOfOrderness, &quot;maxOutOfOrderness&quot;); checkArgument(!maxOutOfOrderness.isNegative(), &quot;maxOutOfOrderness cannot be negative&quot;); this.outOfOrdernessMillis = maxOutOfOrderness.toMillis(); // start so that our lowest watermark would be Long.MIN_VALUE. this.maxTimestamp = Long.MIN_VALUE + outOfOrdernessMillis + 1; } // 为每个事件调用 @Override public void onEvent(T event, long eventTimestamp, WatermarkOutput output) { maxTimestamp = Math.max(maxTimestamp, eventTimestamp); } // 周期性调用, 生成Watermark @Override public void onPeriodicEmit(WatermarkOutput output) { output.emitWatermark(new Watermark(maxTimestamp - outOfOrdernessMillis - 1)); }} 上述WatermarkGenerator中的onPeriodicEmit方法是周期性触发的, 它的触发间隔可由ExecutionConfig.setAutoWatermarkInterval()设置, 默认状况下为200毫秒. 如果要改变生成周期, 可通过如下方法更改.123StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 设置Watermark生成周期为300毫秒env.getConfig().setAutoWatermarkInterval(300); 在旧的API中有两种生成Watermark的方式: Periodic模式: 对应原API中的AssignerWithPeriodicWatermarks, 即周期性地生成Watermark, 在新API中只要在WatermarkGenerator.onPeriodicEmit()中实现Watermark的生成逻辑, 系统便会根据ExecutionConfig.getAutoWatermarkInterval()的值周期性地调用上述方法. Punctuated模式: 对应原API中的AssignerWithPunctuatedWatermarks, 即根据每个事件绝对是否生成Watermark, 在新API中只要在WatermarkGenerator.onEvent()中实现Watermark的生成逻辑, 系统便会为每个事件调用该方法, 同时需要调用ExecutionConfig.setAutoWatermarkInterval(0)关闭周期性Watermark生成. Watermark的传播Watermark可在Source或之后的操作中生成, 一旦生成必须不断往下游传播. 要理解Watermark在Flink中的传播流程, 我们先来看下Flink中Watermark的实体是什么. 如下图所示, Flink中有一个Watermark类, 它和StreamRecord(DataStream中的每条数据在运行时都作为StreamRecord的实例在算子间流转)一样都继承自StreamElement. 也就是说在Flink中, Watermark同数据流中的数据一样是以StreamElement实例在算子间流转的. 对于无需缓存数据的算子(如map算子), 在接收到Watermark之后可直接发送到下游. 而对于需要缓存数据的算子, 如窗口算子, 则需要分两种情况: 若是当前Watermark没有触发计算, 那么直接将其往下游发送即可; 若是当前Watermark触发了计算, 那么需要等计算结果发送之后, 再发送Watermark. 另外需要注意的是在并行流中, 算子会向下游的所有算子广播Watermark. 如下图所示(来自Flink文档), map算子会向window算子广播Watermark, 而window算子会将来自上游各个算子的Watermark的最小值作为当前Watermark. 这一机制产生的现象就是所有window算子的Watermark都是一致的. Flink SQL中的Watermark从Flink 1.10开始(见FLIP-66), Flink利用SQL方言在SQL DDL中增加了对时间属性定义的支持, 包括Processing Time和Event Time. 其中, Processing Time的定义比较简单, 这里不再赘述. 对于Event Time的定义, Flink SQL引入了一个新名词Rowtime属性. Rowtime属性必须是表Schema中具有TIMESTAMP类型的现有列, 一旦在其上定义水印, 它将被标记为Rowtime属性. 在DDL中定义Watermark的语法如下. 12345CREATE TABLE table_name ( WATERMARK FOR columnName AS &lt;watermark_strategy_expression&gt;) WITH ( ...) 从中可以看出, Watermark的定义包含两部分: (1) Rowtime列, FOR之后的单词指定; (2) Watermark生成策略, AS之后的表达式指定. 另外值的说明的是: Watermark策略的返回值是可为null的BIGINT或TIMESTAMP(3)类型; 每个记录都会生成Watermark, 但只有当生成的Watermark不为null, 且大于之前已经发出的最大Watermark时, 当前Watermark才会下发; Watermark发出的时间间隔由ExecutionConfig.getAutoWatermarkInterval()决定, 这其实是一种Periodic模式, 如果要实现Punctuated模式, 需要调用ExecutionConfig.setAutoWatermarkInterval(0). Rowtime属性列也可以是一个计算字段, 下文将详细说明. 在DDL中定义的Watermark策略会尽可能下推到Source算子, 比如目前Kafka的SQL Connector就实现了这一功能. 在Periodic模式中, DataStream API内置了两种Watermark生成策略, 也同样可在SQL DDL中定义. 其中, 单调递增的时间戳分配器, 对应WatermarkStrategy.forMonotonousTimestamps()可使用如下语句实现. 12345678CREATE TABLE kafka_source ( user_id STRING, log_ts TIMESTAMP(3), ingestion_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL &apos;0.001&apos; SECOND) WITH ( ...) 最大固定延迟的时间戳分配器, 对应WatermarkStrategy.forBoundedOutOfOrderness()可使用如下语句实现.12345678CREATE TABLE kafka_source ( user_id STRING, log_ts TIMESTAMP(3), ingestion_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL &apos;5&apos; SECOND) WITH ( ...) 在Punctuated模式中, 常需要根据记录的具体特征决定是否发送Watermark, 此时我们也可以使用自定义的ScalarFunction作为Watermark策略. 例如, 假设log_ts字段描述了记录的创建时间, 并且一些记录带有一个flag, 将它们标记为序列的结尾, 这样时间戳较小的元素就不能再出现了. 然后我们可以实现一个简单的UDF来根据log_ts和flag字段发出水印. 1234567CREATE TABLE myTable ( log_ts TIMESTAMP(3), flag BOOLEAN, WATERMARK FOR log_ts AS watermarkOnFlag(log_ts, flag)) WITH (...) 12345public class WatermarkOnFlag extends ScalarFunction { Long eval(long rowtime, boolean isEndOfSequence) { return isEndOfSequence ? rowtime : null; }} 需要注意的是: 在Flink SQL中使用Punctuated模式需要设置ExecutionConfig.setAutoWatermarkInterval(0); 受限于Flink对有状态ScalarFunction的支持, 目前无法实现一些高级的Watermark生成策略, 例如基于过去一部分数据计算得到Watermark. 这也是当前SQL相对于DataStream在Watermark上支持的不足, 如果有复杂的Watermark生成需求, 目前只能使用DataStream API. 并行流下的Watermark非Source算子生成Watermark的问题Flink DataStream API支持在非Source算子中生成Watermark, 然而在并行流下这种生成方式很可能出现问题. 我们用下面的例子详细说明. 假设有如下Flink作业图, 数据源有三个Paritition, Flink作业的并行度为2, 在map算子之后调用了assignTimestampsAndWatermarks生成Watermark, 策略是当前最大时间戳减5秒. keyby之后是两个Tumbling window算子, 窗口大小为10秒. 输入数据的格式为(id, value, time). 并且id为0的数据会进入partition(0), 依次类推. 由于Flink作业的并行度为2, 因此其中一个Source算子必然会对应两个Partition. 现在如果有如下数据按顺序达到, 那么由于在第5条数据达到的时候, 窗口[2022-04-25 10:00:00, 2022-04-25 10:00:10]被触发, 之后达到的第6条数据就会变为迟到数据.1234560,A,2022-04-25 10:00:001,B,2022-04-25 10:00:002,C,2022-04-25 10:00:001,B,2022-04-25 10:00:162,C,2022-04-25 10:00:160,A,2022-04-25 10:00:08 上述案例中, 其中一个Source算子读取了两个Partition的数据, 且并没有将各个Partition的Watermark进行对齐, 在之后的map算子中, 来自多个Partition的数据进行混合, 因此生成的Watermark将无法表达各个Partition内数据的情况. 而如果将Watermark的生成放到Source算子中, 那么即使Source算子的并行度与Partition数量不同, Source算子也会为每个Partition生成单独的Watermark并进行对齐, 这样就不会出现上述问题了. 因此, 在并行流下对于像Kafka这样的多Partition数据源, 应该把Watermark的生成放在Source算子中. 在原来的SourceFunction API和最新的Source API中都提供了对应的方法.12345678910111213141516171819202122// 在新的Source API中给定Watermark策略KafkaSource&lt;String&gt; source = KafkaSource.&lt;String&gt;builder() .setBootstrapServers(brokers) .setTopics(&quot;input-topic&quot;) .setGroupId(&quot;my-group&quot;) .setStartingOffsets(OffsetsInitializer.earliest()) .setValueOnlyDeserializer(new SimpleStringSchema()) .build();env.fromSource( source, WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(5)), &quot;Kafka Source&quot;);// 在原来的SourceFunction API中给定Watermark策略FlinkKafkaConsumer&lt;String&gt; myConsumer = new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties);myConsumer.assignTimestampsAndWatermarks( WatermarkStrategy. .forBoundedOutOfOrderness(Duration.ofSeconds(20)));DataStream&lt;String&gt; stream = env.addSource(myConsumer); 空闲Source的问题上文也说到了, 在并行流下Flink会实行Watermark对齐, 即上游算子向下游广播Watermark, 下游算子将来自上游各个算子的Watermark的最小值作为当前的Watermark. 在多Partition情况下, 如果存在长时间不更新数据的空闲Partition, 那么由于Watermark对齐, 会使其他window算子长时间得不到触发. 这里还是以一个详细的案例说明上述情况. 假设有如下Flink作业图, Watermark在Source算子中生成, 生成策略, window类型及大小, 输入数据的格式和对应Partition同上一小节. 现在我们有如下数据按顺序到达, 可以看到, 在2022-04-25 10:00:00之后的半个小时内都没有id为0数据, 也就是说partition(0)中有很长一段时间没有数据写入, 而partition(1)中不断有数据写入. 但是由于Watermark对齐机制, window算子的Low Watermark一直无法推进, 需要一直等到直到partition(0)重新有数据写入.123456780,A,2022-04-25 10:00:001,B,2022-04-25 10:00:001,B,2022-04-25 10:00:011,B,2022-04-25 10:00:081,B,2022-04-25 10:00:13...一直是id为1的数据1,B,2022-04-25 10:30:000,A,2022-04-25 10:30:00 上述这种情况会导致两个问题: 其一是window算子中需要缓存大量id为1的数据, 使得作业的State不断增大, 给Checkpoint带来压力, 失败后的重启时间也变大. 其二是计算结果的延迟变大, 可以看到id为1在2022-04-25 10:00:00后的数据最大需要延迟30分钟后才能输出计算结果. 对于空闲Source的问题, Flink中已经提供了解决方案. 可以在WatermarkStrategy中通过withIdleness指定判断Source为空闲的最大时间间隔. 比如以下代码表示, 如果一个Source超过1分钟没有数据更新, 那么将该Source标识为空闲Source, 其他Source的Watermark不需要再与该Source对齐.123WatermarkStrategy .&lt;Tuple2&lt;Long, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withIdleness(Duration.ofMinutes(1)); Event Time倾斜的问题在并行流下多Partition数据源中可能产生的另一个问题是Event Time倾斜. Event Time倾斜即各个Partition中数据的Event Time推进不一致, 部分Partition中的Event Time与其他Partition中的Event Time存在较大差距. 在这种情况下, 由于Watermark对齐机制, 就导致了下游Low Watermark不能推进, 而Event Time推进较快的Partition的数据又被不断读入, 对于需要Watermark触发的window算子就会缓存大量数据. 举例来说, 如果我们从某个Kafka Topic的开头读取历史数据, 各个分区的Event Time很可能并不同步, 如果一个Partition的Event Time明显比其他Partition慢, 那么由于Watermark对齐, window算子的Low Watermark会被拖慢, 而其他分区的数据又在不断读入, 这就造成了大量的数据缓存. 从表象上来看, 空闲Source和Event Time倾斜都会造成大量的数据缓存, 不过这两个问题是存在本质区别的: 空闲Source是某一Partition在一段时间内没有数据写入, 经过一段时间后又有数据写入, 在这个过程中数据的Event Time存在跳跃式推进, 也就是说这段时间内确实没有数据, 而不是数据迟到. 在这种情况下我们可以将这个Partition标识为空闲从而直接忽略. 然而在Event Time倾斜问题中, 各个Partition中并不存在Event Time的跳跃式推进, 也就是说并不存在某个Partition在某段时间内没有数据, 而是各个Partition的Event Time推进不一致. 这也就无法通过将某个Partition标识为空闲解决. 对于Event Time倾斜问题, 在Flink 1.15中提供了解决方案. 可通过如下方式使用, 即通过在WatermarkStrategy中添加withWatermarkAlignment来实现各个Source的Watermark同步推进. 它实现了这样的语义, 如果两个Source间Watermark的差值超过了一个给定值maxAllowedWatermarkDrift, 那么停止读取Watermark推进较快的Source, 等到两个Source间的Watermark小于maxAllowedWatermarkDrift时再重新开始读取该Source. 需要注意的是Source间的Watermark同步只支持新的Source API, 且只能通过以下方式使用, 在DataStream.assignTimestampsAndWatermarks中使用是无效的.123456DataStream&lt;Tuple2&lt;Long, String&gt;&gt; streamSource = env.fromSource( source, WatermarkStrategy .&lt;Tuple2&lt;Long, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withWatermarkAlignment(&quot;alignment-group-1&quot;, Duration.ofSeconds(20), Duration.ofSeconds(1)), &quot;source&quot;); withWatermarkAlignment有三个参数: 第一个参数是String类型, 用于对Source进行分组, 只有在同一个分组中的Source才会实行Watermark同步; 第二个参数是Duration类型, 表示maxAllowedWatermarkDrift; 第三个参数也是Duration, 表示间隔多久进行一次同步. 在实现上Flink增加了一个协调者为各个Source的Watermark进行校准, 每个Source实例需要定期向协调者报告目前的Watermark, 并接受协调者的返回决定是否需要继续拉取数据. Watermark生成实践在Watermark概念这一节中我们已经说到, Watermark是需要根据数据本身的特征以及已经观察到的数据来生成的. 这里我们通过一个具体的可能在真实场景中遇到的案例来进一步说明这一点. 这一案例参考了深入理解流计算中的 Watermark. 假设我们有一个网站, 为了同时服务中国和美国的用户, 在中国杭州和美国旧金山各购置了一台云服务器. 服务器会实时将登录用户的数据发送到位于中国上海的Kafka集群中. 现在需要每隔10分钟统计一次网站的总访问量. 在这种场景下, 由于国内服务器与国外服务器和Kafka集群的网络延迟不同, 到达的数据可能存在乱序. 这里的乱序体现在两个方面: 其一是相同服务器上的数据乱序, 比如国内服务器上的数据由于网络抖动先产生的数据反而比后产生的数据迟到, 这种情况具有很大的随机性. 其二是国内服务器和国外服务器之间的数据乱序, 由于国外服务器的网络延迟大, 所以其上的数据整体上比国内服务器上的数据慢到一段时间. 比如国内服务器在$t_2$时刻的数据已经到达, 而国外服务器$t_1$时刻的数据反而在之后到达. 两个服务器之间的乱序通常有一定的系统性. 为了解决第一个乱序问题, 我们可以简单采用固定延迟的Watermark生成策略, 即使用WatermarkStrategy.forBoundedOutOfOrderness. 对于第二个乱序问题, 我们可以将两个服务器的Watermark进行对齐, 即对不同服务器的数据分别计算Watermark, 再取其中的最小值作为最终的Watermark. 在只有两个服务器的情况下, 我们可以将不同服务器的数据写入不同的Kafka Partition. 这样只要保证在Source中生成Watermark, 依据Watermark的对齐机制, Flink会自动为我们选择最小的Watermark, 我们就只需直接使用WatermarkStrategy.forBoundedOutOfOrderness策略即可. 但是在更多的服务器场景下, 我们可能很难控制服务器与Kafka Partition之间的对应关系, 这时我们只能编写自定义的Watermark生成算法. 具体实现如下.12345678910111213141516171819202122232425262728293031public class VisitWatermarkGenerator implements WatermarkGenerator&lt;Visit&gt; { private final long outOfOrdernessMillis; private final Map&lt;String, Long&gt; maxTimePerServer = new HashMap&lt;&gt;(2); public VisitWatermarkGenerator(Duration maxOutOfOrderness) { this.outOfOrdernessMillis = maxOutOfOrderness.toMillis(); } @Override public void onEvent(Visit event, long eventTimestamp, WatermarkOutput output) { String server = event.serverId; if (!maxTimePerServer.containsKey(server) || eventTimestamp &gt; maxTimePerServer.get(server)) { maxTimePerServer.put(server, eventTimestamp); } } @Override public void onPeriodicEmit(WatermarkOutput output) { Optional&lt;Long&gt; maxTimestamp = maxTimePerServer.values().stream() .min(Comparator.comparingLong(Long::valueOf)); maxTimestamp.ifPresent( t -&gt; output.emitWatermark(new Watermark(t - outOfOrdernessMillis - 1))); }}public class Visit { public String serverId; public String userId; public long eventTime;} 总结Watermark是流处理中的重要抽象, 它是平衡流处理准确性和延迟的机制. Watermark需要根据数据的具体情况和已经观察到的数据来生成. Flink在DataStream API和SQL API中都已经增加了对Watermark的支持, 不过对于复杂的Watermark策略, 目前只能用DataStream API实现. 在并行流下使用Watermark可能会出现两类问题: 一类是由于在非Source中生成Watermark导致数据乱序, 这一问题是可以避免的; 另一类问题是由于数据特征而导致的空闲Source和Event Time倾斜, Flink现在也引入了对这两种问题的解决方案. 参考[1] Flink Documentation - Timely Stream Processing[2] Flink Documentation - Generating Watermarks[3] Flink Documentation - Builtin Watermark Generators[4] The Dataflow Model[5] Streaming 101: The world beyond batch[6] Streaming 102: The world beyond batch[7] FLIP-66: Support Time Attribute in SQL DDL[8] Flink 小贴士 (3): 轻松理解 Watermark[9] 深入理解流计算中的 Watermark[10] Flink Event Time 倾斜","link":"/flink-watermark.html"},{"title":"HBase最佳实践 - Bulk Loading原理与Spark实现","text":"Apache HBase能够在大数据集上为我们提供随机, 实时的读写访问. 然而, 在实际业务中, 我们的原始应用并非基于HBase构建. 这时候, 如何将大量的数据(这些数据的存储量可能是TB甚至PB级别的)导入到HBase中成了我们首先需要解决的问题. 最基本的, 我们可能会想到使用Client APIs或利用MapReduce Job通过TableOutputFormat写入. 然而, 这两种方式都不是最高效的, 在向HBase中导入大规模数据集时, 首先应该考虑的是HBase提供的Bulk Loading方法. 但是, 在利用Bulk Loading向HBase中导入数据时, 常常会遇到很多困难. 一方面是由于HBase的官方指南仅给出了一些关于Bulk Loading的原理说明, 并没有给出代码示例; 另一方面Bulk Loading涉及到一些底层的原理(比如数据的排序是如何进行的, 数据是如何分区的), 如果不了解这些原理就难以写出高效的Bulk Loading程序. 此外, 通过MapReduce对数据进行预处理并使用HFileOutputFormat2.configureIncrementalLoad对数据进行全局排序的方法并不能最大限度的利用集群的计算能力, 无论如何目前已经很少有人还在编写MapReduce程序, 取而代之的是Spark. 为此, HBase官方推出了Apache HBase Connectors支持Spark与HBase的连接, 提供了基于Spark的bulk put/get/delete/load等操作, 但是HBase Connectors仅支持2.2.x版本的HBase. 本文首先讲述HBase Bulk Loading的原理以及为甚么需要使用Bulk Loading, 最后讲述如何基于Spark在1.4.x系列版本上利用Spark而非HFileOutputFormat2.configureIncrementalLoad进行更加高效的Bulk Loading, 同时也给出了在2.2.x系列版本上使用HBase Connectors进行Bulk Loading的示例. Bulk Loading原理简单来说, Bulk Loading即通过MapReduce或Spark等分布式计算框架对大规模数据集进行预处理和排序, 输出为HBase的底层存储文件HFile, 最终通过HBase提供的LoadIncrementalHFiles.completebulkload工具将输出的HFile移动到RegionServer对应Region的存储文件夹中, 从而完成数据导入. 事实上, Bulk Loading的过程与ETL类似, 主要包含以下几个步骤: 1. Extract: 从数据源中导出数据, 一般为文本文件或其他数据库的dump文件. 如何从数据源中导出数据并不在本文的讨论范围之内, 一般来说现有的数据库都支持各种类型的导出, 也可以很方便地通过MapReduce或Spark提供的接口进行读取. 在进行下一步之前, 需要提前将数据源文件上传到HDFS中. 2. Transform: 将原始数据处理排序并重写为HFile. 本文利用Spark进行这一步操作, 具体流程如下(HBase Connectors中的实现类似): 读取并解析原始文件, 通常需要读者自行编写解析函数, 从原始文件生成RDD; 通过Spark的repartitionAndSortWithinPartitions函数对RDD进行重分区和排序, 笔者提供了类BulkLoadPartitioner用于重分区, 类KeyFamilyRow用于根据row key, column family, column qualifier对记录进行全局排序; 将RDD通过Spark的saveAsNewHadoopFile写为HFile文件; 这一步中有几个细节值的说明. 首先, BulkLoadPartitioner进行重分区的分区数量等于HBase table中存在的Region数量(仅针对未进行预分区的table, 对于预分区的table则分区数等于table的预分区数), 也就是说, 对于一个未进行预分区的空table, 在利用Spark进行repartition后只会有一个分区, 这可能会影响生成HFile的效率. 然而读者应当慎重改变RDD分区的数量, 虽然这可能提升生成HFile的效率, 但是这可能导致无法确定生成的HFile应当加载到哪个RegionServer中. 其次, 本文的方法利用saveAsNewHadoopFile将RDD输出为HFile, HBase Connectors中采用了自定义的方法, 具体可参见HBaseContext.scala中的bulkLoad函数. 3. Load: 将生成的HFile导入RegionServer相应的目录中. 调用LoadIncrementalHFiles.completebulkload将HFile导入RegionServer中, 并使数据对客户端可见. 在调用completebulkload前如果有新数据插入, 或者Region split发生, 则completebulkload会根据当前table的Region起止row key范围, 重新对HFile进行分割. 这并不是高效的, 因此应当避免目标table在生成HFile的过程中有较大变动. 此外, completebulkload并不是线程安全的, 切勿对同一个表同时调用此函数. Bulk Loading的过程如下图所示. 数据从源导入HDFS中, 通过Spark定义的DAG操作进行处理生成HFile. 最后将HFile移动到对应的RegionServer. 使用Spark实现Bulk Loading在HBase 2.2.x版本中HBase官方推出了HBase Connectors, 提供了Spark, Kafka与HBase的各种交互操作. 其中包括利用Spark进行Bulk Loading. 然而, HBase Connectors并不兼容HBase 1.4.x系列, 为此本文提供了一个利用Spark在1.4.x版本上进行Bulk Loading的例子, 根据该例读者只需利用Spark重写源文件处理操作即可. HBase 1.x中的Bulk Loading在HBase 1.4.x中利用Spark进行Bulk Loading需要自定义分区及排序逻辑, 作者提供了通用的实现. 分区实现见BulkLoadPartitioner, 排序实现见KeyFamilyQualifier. 在此基础上, 便可根据Spark现有的API函数实现Bulk Loading. 以下是一个简单例子的部分代码, 完整代码可参见HBaseSparkBulkLoad. 123456789101112131415161718192021222324252627282930313233343536// get table infoval hConf = HBaseConfiguration.create()val connection = ConnectionFactory.createConnection(hConf)val table = connection.getTable(TableName.valueOf(tableName))val regionLocator = connection.getRegionLocator(TableName.valueOf(tableName))val sConf = new SparkConf().setAppName(&quot;bulkload&quot;).setMaster(&quot;local&quot;)sConf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)val sc = new SparkContext(sConf)// simple data to bulk load to hbaseval rdd = sc.parallelize(Array( (Bytes.toBytes(&quot;4&quot;), Array((Bytes.toBytes(columnFamily), Bytes.toBytes(&quot;4&quot;), Bytes.toBytes(&quot;1&quot;)))), (Bytes.toBytes(&quot;1&quot;), Array((Bytes.toBytes(columnFamily), Bytes.toBytes(&quot;2&quot;), Bytes.toBytes(&quot;2&quot;)))) )).repartition(2) // random partition to simulate a real scene// parse the source data and repartition and sort by row key, family, qualifyval sortedRdd = rdd.map(line =&gt; { val keyFamilyQualifier = new KeyFamilyQualifier(line._1, line._2(0)._1, line._2(0)._2) val value = line._2(0)._3 (keyFamilyQualifier, value) }).repartitionAndSortWithinPartitions(new BulkLoadPartitioner(regionLocator.getStartKeys))// reformat the data so that we can save as HFileOutputFormat2val hfileRdd = sortedRdd.map(line =&gt; { val rowKey = new ImmutableBytesWritable(line._1.getRowKey) val keyValue = new hbase.KeyValue(line._1.getRowKey, line._1.getFamily, line._1.getQualifier, line._2) (rowKey, keyValue) })// save the rdd as hfilehfileRdd.saveAsNewAPIHadoopFile( stagingFolder, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], hConf)// load the hfile from hdfsval loader = new LoadIncrementalHFiles(hConf)loader.doBulkLoad(new Path(stagingFolder), connection.getAdmin, table, regionLocator) HBase 2.x中的Bulk Loding在HBase 2.2.x中可利用HBase Connectors提供的API更加方便地实现Bulk Loading. 以下是一个简单的案例, 完整的实现可参考JavaHBaseBulkLoadExample. 1234567891011SparkConf sparkConf = new SparkConf().setAppName(&quot;JavaHBaseBulkLoadExample &quot; + tableName);JavaSparkContext jsc = new JavaSparkContext(sparkConf);List&lt;String&gt; list= new ArrayList&lt;String&gt;();list.add(&quot;1,&quot; + columnFamily1 + &quot;,b,1&quot;); // row1list.add(&quot;2,&quot; + columnFamily2 + &quot;,a,3&quot;); // row2list.add(&quot;2,&quot; + columnFamily2 + &quot;,b,3&quot;);JavaRDD&lt;String&gt; rdd = jsc.parallelize(list);Configuration conf = HBaseConfiguration.create();JavaHBaseContext hbaseContext = new JavaHBaseContext(jsc, conf);hbaseContext.bulkLoad(rdd, TableName.valueOf(tableName),new BulkLoadFunction(), args[0], new HashMap&lt;byte[], FamilyHFileWriteOptions&gt;(), false, HConstants.DEFAULT_MAX_FILE_SIZE); 参 考[1] Apach HBase ™ Reference Guide 72 110[2] How-to: Use HBase Bulk Loading, and Why","link":"/hbase-bulk-loading.html"},{"title":"Hadoop YARN原理 - 编写YARN Application","text":"本文是Hadoop YARN原理系列的第二篇, 主要讲述如何编写YARN Application. 实际上, MapReduce, Spark以及Flink等框架在YARN上运行时, 都可以视为一种特定的YARN Application. 不过这些系统的on YARN模式实现都是生产级别的, 代码相对复杂, 所以本文并不打算以这些系统的on YARN实现为例分析如何编写YARN Application, 而是以Hadoop自带的hadoop-yarn-applications-distributedshell(后文简称DistributedShell)为例进行讲述. DistributedShell可以在YARN的Container上执行用户指定的Shell命令或脚本, 虽然简单, 却包含了编写一个YARN Application的完整内容. 本文首先基于DistributedShell从整体上升分析YARN的工作流程, 然后讲述具体的代码实现. 所有相关的代码和运行方法都可以在此链接中找到. YARN工作机制从整体上来看, 一个YARN Application的提交和启动流程是这样的: 用户编写的Client向ResourceManager申请Application. ResourceManager返回所申请的Application相关信息. Client封装Application所需资源, 包括运行ApplicationMaster所需的资源和命令, 以及运行具体任务所需的相关资源等. ResourceManager启动一个Container并运行ApplicationMaster. ApplicationMaster负责申请运行任务所需的Container, 并封装其所需的资源和启动命令, 最后启动并监控Container的运行情况. 这其中, 需要用户实现的内容是Client和ApplicationMaster, 不同的应用会有不同的实现. 比如Hadoop MapReduce和Flink作业向YARN提交时, 就需要不同的Client和ApplicationMaster实现. 下面具体讲述DistributedShell在YARN上的具体提交和启动流程. DistributedShell的提交和启动流程主要有以下几个步骤: 在Client中通过YarnClient向ResourceManager的ApplicationsManager申请Application, 在内部这是由ApplicationClientProtocol封装的通信协议完成的. ResourceManager会返回一个GetNewApplicationResponse对象, 其中包含ApplicationSubmissionContext以及ApplicationId等. Client创建ApplicationSubmissionContext所需内容, 包括设置application name, 设置优先级, 设置提交队列并创建ContainerLaunchContext. 最终Client会向ResourceManager提交Application到队列中. ResourceManager的ApplicationsManager向NodeManager申请Container, 并根据Client提供的ContainerLaunchContext启动ApplicationMaster. Application启动Container用于运行具体的任务. YARN Application实现从上面的分析中已经可以知道, 在编写YARN Application时, 需要我们自己实现的是两部分: 第一是Client, 主要用于接收用户命令, 向ResourceManager申请创建Application, 并提交相关资源. 第二是ApplicationMaster, 主要用于向ResourceManger申请创建用于工作的Container. 本节将具体讲述Client和ApplicationMaster的实现流程. 由于完整的实现包含大量的辅助代码, 所以这里只给出了核心代码用于梳理主要的实现流程, 对于完整的代码和运行方式可参考文章开头给出的链接. Client实现Client是运行在客户端的程序, 即会运行在向YARN集群提交Application的机器上. Client的实现主要包含以下几个核心步骤: 创建并启动YarnClient; 通过YarnClient创建Application; 完善ApplicationSubmissionContext所需内容:a. 设置application name;b. 设置ContainerLaunchContext;c. 设置优先级;d. 设置队列 提交Application. 核心代码如下.123456789101112131415161718192021222324252627282930313233Configuration conf = new YarnConfiguration();// 1. 创建并启动YarnClient// YarnClient内容通过ApplicationClientProtocol与ResourceManager通信YarnClient yarnClient = YarnClient.createYarnClient();yarnClient.init(conf);yarnClient.start();// 2. 通过YarnClient创建Application// GetNewApplicationResponse中包含了ApplicationId, ApplicationSubmissionContext等内容YarnClientApplication app = yarnClient.createApplication();GetNewApplicationResponse appResponse = app.getNewApplicationResponse();// 3. 完善ApplicationSubmissionContext所需内容ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();ApplicationId applicationId = appContext.getApplicationId();// 创建ContainerLaunchContext, 其中包括启动ApplicationMaster所需资源和命令ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance( localResources, env, commands, null, null, null);// 3.1 设置application nameappContext.setApplicationName(appName);// 3.2 设置ContainerLaunchContextappContext.setAMContainerSpec(amContainer);// 3.3 设置优先级Priority pri = Priority.newInstance(amPriority);appContext.setPriority(pri);// 3.4 设置队列appContext.setQueue(amQueue);// 4. 提交ApplicationyarnClient.submitApplication(appContext); 这里的ContainerLaunchContext是用于启动ApplicationMaster的Container上下文环境, 其中包括: localResources: 包含ApplicationMaster的Jar包, 一些本地文件, 以及需要运行的Shell命令等(这里需要注意的是, 如果用户给定的是一个Shell脚本, 那么这个脚本会被上传到HDFS, 而不会被作为本地资源上传); env: 包含ApplicationMaster运行所需的环境变量, 比如Shell脚本位置(用于创建任务Container), Java CLASSPATH等; commands: 运行ApplicationMaster的Java命令. ApplicationMaster实现ApplicationMaster运行在Container中, 其核心实现步骤如下: 创建并启动AMRMClientAsync, 用于与ResourceManager通信; 创建并启动NMClientAsync, 用于与NodeManager通信; 向ResourceManager注册, 之后会向ResourceManager发送心跳; 向ResourceManager申请Container. 12345678910111213141516171819202122// 1. 创建并启动AMRMClientAsync, 用于与ResourceManager通信 AMRMClientAsync.AbstractCallbackHandler allocListener = new RMCallbackHandler();amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);amRMClient.init(conf);amRMClient.start();// 2. 创建并启动NMClientAsync, 用于与NodeManager通信containerListener = createNMCallbackHandler();nmClientAsync = new NMClientAsyncImpl(containerListener);nmClientAsync.init(conf);nmClientAsync.start();// 3. 向ResourceManager注册, 之后会向ResourceManager发送心跳RegisterApplicationMasterResponse response = amRMClient .registerApplicationMaster(appMasterHostname, appMasterRpcPort, appMasterTrackingUrl, placementConstraintMap);// 4. 向ResourceManager申请Containerfor (int i = 0; i &lt; numTotalContainersToRequest; ++i) { ContainerRequest containerAsk = setupContainerAskForRM(); amRMClient.addContainerRequest(containerAsk);} 其中, 用于运行任务的Container的启动逻辑是在AMRMClientAsync的回调接口里面的. 一旦ApplicationMaster向ResourceManager申请的Container创建成功后, 就会触发如下回调. 该回调会创建一个LaunchContainerRunnable线程并启动.123456789101112131415161718192021222324252627public void onContainersAllocated(List&lt;Container&gt; allocatedContainers) { for (Container allocatedContainer : allocatedContainers) { if (numAllocatedContainers.get() == numTotalContainers) { amRMClient.releaseAssignedContainer(allocatedContainer.getId()); } else { numAllocatedContainers.addAndGet(1); String yarnShellId = Integer.toString(yarnShellIdCounter); yarnShellIdCounter++; // 用于启动Container的线程 Thread launchThread = createLaunchContainerThread(allocatedContainer, yarnShellId); launchThreads.add(launchThread); launchedContainers.add(allocatedContainer.getId()); launchThread.start(); // Remove the corresponding request Collection&lt;AMRMClient.ContainerRequest&gt; requests = amRMClient.getMatchingRequests( allocatedContainer.getAllocationRequestId()); if (requests.iterator().hasNext()) { AMRMClient.ContainerRequest request = requests.iterator().next(); amRMClient.removeContainerRequest(request); } } }} 真正运行Container的逻辑在LaunchContainerRunnable线程中, 这其中的逻辑其实就是创建一个ContainerLaunchContext并提交给NodeManager, 只不过这里的ContainerLaunchContext是用于运行Shell命令的Container的上下文环境. 其流程与创建ApplicationMaster的ContainerLaunchContext是一致的, 这里不再赘述. 小结本文首先介绍了DistributedShell的整体运行流程, 实际上Hadoop MapReduce, Flink等框架在YARN上运行时也符合这个流程. 然后从核心代码方面分析了编写一个YARN Application具体需要实现的内容: Client和ApplicationMaster. 实际上, 需要在YARN上运行不同的应用, 就是要实现这两部分内容. 相信理解了本文介绍的内容之后, 再去看Flink等框架中与在YARN中提交作业相关的源码就不会一头雾水了. 参考[1] Hadoop: Writing YARN Applications[2] Service oriented application hosted on YARN demo","link":"/hadoop-yarn-write-application.html"},{"title":"Hadoop YARN原理 - 整体架构","text":"YARN(Yet Another Resource Negotiator)是一个分布式的资源调度和管理系统, 负责管理和分配集群的资源(目前主要是vcores和memory), 在一些资料中形象地把YARN比作一个分布式的操作系统. YARN在Hadoop 2.x中引入, 目前已经有批式, 流式, 机器学习等多种类型的计算框架支持在YARN上运行, 如Spark, Flink, Tensorflow等框架都对YARN有成熟的支持. YARN已经成为名副其实的”数据操作系统”. 本文是Hadoop YARN原理系列文章的第一篇, 主要讲述YARN的整体架构, 为后文讲述如何编写YARN application打下基础. Hadoop YARN系列文章是对YARN原理以及如何编写YARN application的总结, 主要目的是为进一步探索Flink等计算框架on YARN Runtime层打下基础, 并非想要深入并完整地介绍YARN的所有内容, 不过文中也给出了一些进阶资料, 供有需要的读者阅读. YARN架构YARN采用了经典的Master-Slave架构, 其中ResourceManager对应Master, NodeManager对用Slave. YARN集群的组成部分如下图所示(图片来自Hadoop官方文档). YARN集群中包含ResourceManager, ApplicationMaster和NodeManager三个主要组件: ResourceManager是一个全局组件, 一个YARN集群中只存在一个ResourceManager, 它持有整个系统的资源情况, 其主要作用是:a. 接收并处理来自Client的请求;b. 监控NodeManager;c. 启动并监控ApplicationMaster;d. 资源分配调度. ApplicationMaster与YARN application对应, 每个application都会有一个ApplicationMaster, ApplicationsMaster也运行在Container中. 需要说明的是, ApplicationMaster其实并不是YARN集群的常驻进程, 只有在YARN集群中存在正在运行的application时, 才会存在ApplicationMaster, 严格来说它并不是YARN集群的组件, 而是属于每个YARN application生命周期中的一个组件. 其主要作用是:a. 为application申请资源, 并分配给内部各个任务;b. 监控application并提供容错. NodeManager用于管理单个节点上的资源, 每个工作节点上都会存在一个NodeManager, 它会向ResourceManager报告其掌握的资源情况, 其主要作用是:a. 管理单个结点上的资源;b. 处理来自ResourceManager的请求;c. 处理来自ApplicationMaster的请求. 这里的YARN application是指运行在YARN中的一个具体的程序, 也有地方叫做Job(作业), 但是在Hadoop官方文档中叫做YARN application, 所以这里沿用了这种叫法. 实际上只要不产生歧义这两种叫法都是可以的. 具体来说, ResourceManager包含两个主要的内部组件Scheduler和ApplicationsManager. Scheduler负责为运行的YARN application分配资源, 它持有集群资源和调度队列信息. Scheduler是一个单纯的调度器, 不负责监控application的运行情况以及容错保证, 这部分功能由ApplicationManager提供. Scheduler框架提供了可扩展的接口, Hadoop目前支持CapacityScheduler和FairScheduler, 具体在YARN Scheduler章节说明. ApplicationManager负责接收Client的Job提交, 并启动第一个Container用于运行ApplicationMaster, 同时提供容错服务. 接下来由ApplicationMaster从Scheduler调度合适的资源, 并监控作业运行状态. 在一些资料中, 把Container也归为YARN集群的组件, 严格来说是不准确的. Container是YARN集群资源调度的单位, 它封装了某个节点上多个维度的资源(vcores, memory). 提交给YARN集群的Job最终都会分配到具体的一个或多个Container中运行. 但是Container并不是预先分配好的, 而是根据提交给YARN集群的Job所需资源动态分配的, 一旦Job运行结束, 其对应的Container就会销毁. YARN核心参数上一节从概念上讲述了YARN集群的组成部分及其作用, 相对来说比较抽象. 这一节讲述YARN集群的一些核心配置参数, 通过这些参数可以进一步了解上述组件在YARN集群中的物理存在形式. 这些参数名和默认值都可以在yarn-default.xml文件中找到. 在集群中可以在$HADOOP_HOME/etc/hadoop/yarn-site.xml文件中配置这些参数. 组件 参数 默认值 作用 ResourceManager yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler 指定资源调度器类, 默认是容量调度器. 可配置为org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler以启用公平调度器 yarn.resourcemanager.scheduler.client.thread-count 50 Scheduler用于Job调度的线程数 NodeManager yarn.nodemanager.resource.detect-hardware-capabilities false 启用自动检测硬件配置, 例如内存和CPU. yarn.nodemanager.resource.memory-mb -1 NodeManager可使用的物理内存, 若设置为-1且yarn.nodemanager.resource.detect-hardware-capabilities为true, 则会自动计算, 否则为8192MB yarn.nodemanager.resource.cpu-vcores -1 NodeManager可使用的CPU核数, 若设置为-1且yarn.nodemanager.resource.detect-hardware-capabilities为true, 则会自动计算, 否则为8 Container yarn.scheduler.minimum-allocation-mb 1024 Container最小内存 yarn.scheduler.maximum-allocation-mb 8192 Container最大内存 yarn.scheduler.minimum-allocation-vcores 1 Container最小CPU核数 yarn.scheduler.maximum-allocation-vcores 4 Container最大CPU核数 YARN SchedulerYARN Scheduler是决定YARN集群扩展性的重要因素, 很多公司内部为了实现更高效的调度和集群资源使用率, 都对YARN的调度策略做了定制化的更改, 详细的内容可阅读参考[4-5]. 目前YARN提供了FifoScheduler, CapacityScheduler以及FairScheduler三种类型的Sceduler, 并且Hadoop 3.0研发了Global Scheduling颠覆了现有的Scheduler框架, 期望进一步提升单集群调度性能. 目前, CapacityScheduler和FairScheduler在生产环境中都有使用, 这里主要介绍这两种Scheduler, FifoScheduler一般不会在生产环境中使用. CapacitySchedulerCapacityScheduler是一种容量调度器, 它使YARN集群可以以友好的形式提供可共享的, 多租户的对外服务. 同时可以最大化集群吞吐量和使用效率. CapacityScheduler有以下几个重要特征: 层次队列: 支持配置多个具有层级结构的队列(如上图所示), 每个队列可配置一定量的资源. 如果某个队列有资源剩余, 可以共享资源给资源紧缺的队列, 一旦该队列有新的application提交, 其他队列会归还借调资源. 容量保证: 管理员可以为每个队列配置最低资源和资源使用上限. 多租户: 可通过配置层次化队列实现多租户(每个用户对应一个叶子节点队列), 以防止单个application, 用户或队列垄断队列或集群的资源, 以确保集群不会过载. 优先级调度: 允许application以不同的优先级提交. 整数值越高, 应用的优先级越高. 目前应用优先级仅支持先进先出排序策略. 当队列中存在待调度的Job, 并且队列有空闲资源时, CapacityScheduler会为Job进行资源调度. 其过程如下: 选择队列: 从根队列开始, 对子队列按资源使用率从小到大排序. 若子队列为叶子队列, 则依次按照步骤2和步骤3在队列中选出一个Container; 否则以该子队列为根队列, 重复以上过程, 直到找到一个合适的队列并退出. 选择Job: 在步骤1选中一个叶子队列后, CapacityScheduler按照Job优先级和提交时间对子队列中的Job进行排序, 选择优先级最高(优先级最高的情况下选择最先提交的)的Job资源分配. 选择Container: 同一个Job请求的Container可能涉及不同的优先级, 节点, 资源量和数量. 当选中一个Job后, CapacityScheduler将尝试优先分配优先级高的Container. 同一类优先级, 优先选择满足本地性的Container, 依次选择node local, rack local和no local. 队列资源使用率 = 已经使用的资源量 / 除以最小队列资源容量.对于非叶子队列, 它的已使用资源量是各个子队列已使用资源量之和. CapacityScheduler有两种资源比较器用以比较两个资源的大小(比如比较用户当前使用的资源量是否超过了设置的上限资源量), 默认的DefaultResourceCalculator只考虑了内存资源. DominantResourceCalculator采用了DRF(Dominant Resource Fairness)比较算法, 同时考虑内存和CPU两种资源, 可通过参数yarn.scheduler.capacity.resource-calculator设置资源比较器. 下面以一个具体案例说明CapacityScheduler如何进行调度. 假设现在集群中存在4个job, 其所属队列情况如上图所示. CapacityScheduler的调度过程如下: 从root队列开始, 选择一个资源使用率最低的子队列queueA(这里假设queueA资源使用率最低). 在queueA中, 对其中的job按提交时间和优先级排序, 选择job1进行资源分配. 关于CapacityScheduler在集群中的详细配置, 可阅读参考[3]. FairSchedulerFairScheduler是一种公平调度器, 它和CapacityScheduler一样支持层次队列, 多租户以及优先级调度. 它的资源分配流程与CapacityScheduler一样, 包含选择队列, 选择Job和选择Container三个步骤. 当然两者之间也存在一些区别. CapacityScheduler FairScheduler 调度策略 优先选用资源使用率低的队列 优先选择资源缺额比例大的队列 资源分配方式 FIFO FIFO, Fair, DRF FairScheduler在选择队列时的调度策略是优先选择资源缺额比例大的队列. 资源缺额比例计算方式如下: $$ 缺额比例 = \\frac{队列中Job实际所需资源-队列最大资源}{队列最大资源} $$ 在每个队列中进行Job资源分配时, CapacityScheduler只支持FIFO, FairScheduler支持FIFO, Fair和DRF. 若配置为FIFO则FairScheduler退化为CapacityScheduler. FairScheduler的默认分配方式是Fair, 它采用的是最大最小公平算法, 支持加权, 且仅考虑内存资源. 下面以一个具体案例说明基于最大最小公平算法的Fair分配方式. 不加权 加权 某队列供12个资源, 存在4个Job, 资源需求分别是: Job1: 1, Job2: 2, Job 3: 6, Job4: 5. 某队列供16个资源, 存在4个Job, 资源需求分别是: Job1: 4, Job2: 2, Job 3: 10, Job4: 4. 每个Job的权重是: Job1: 5, Job2: 8, Job3: 1, Job4: 2. 第1次分配 12 / 4 = 3 Job1: 分3 - 多2 Job2: 分3 - 多1 Job3: 分3 - 少3 Job4: 分3 - 少2 16 / (5 + 8 + 1 + 2) = 1 Job1: 分5 - 多1 Job2: 分8 - 多6 Job3: 分1 - 少9 Job4: 分2 - 少2 第2次分配 3 / 2 = 1.5 Job1: 分1 Job2: 分2 Job3: 分3 + 分1.5 - 少1.5 Job4: 分3 + 分1.5 - 少0.5 7 / (1 + 2) = 2.33 Job1: 分4 Job2: 分2 Job3: 分1 + 分2.33 - 少6.67 Job4: 分2 + 分4.66 - 多2.66 第3次分配 Job1: 分4 Job2: 分2 Job3: 分1 + 分2.33 + 分2.66 Job4: 分4 在最大最小公平算法中, 会一直分配直到队列中没有空闲资源或所有Job都获得所需资源. 基于Fair策略的资源分配只考虑了内存, 在一些场景下, 需要考虑多个维度资源的公平分配, 比如CPU, 内存等. FairSceduler支持DRF分配策略, 可以考虑同时考虑CPU和内存两个维度. 关于DRF算法可参考论文Dominant Resource Fairness: Fair Allocation of Multiple Resource Types. 关于FairScheduler在集群中的详细配置, 可阅读参考[4]. 小结本文首先从整体出发, 从概念上剖析了YARN集群的架构和各个组成部分. 为了从物理上进一步了解各个组件, 形成具象化的概念, 又给出了几个核心参数. 最后讲述了YARN中已经实现了两种Scheduler, 由于Schduler是决定集群扩展性的重要因素, 因此很多公司在内部对Scheduler进行了不同程度的优化. 随着Kubernetes等容器化部署技术的不断发展, 未来Kubernetes或许会成为大数据计算框架和机器学习框架的标配, YARN或许会成为历史. 但是由于YARN框架在大数据计算领域的成熟应用以及前期的广泛部署, 学习基本的YARN原理仍是我们进一步学习相关框架runtime层的重要基础. 参考[1] Apache Hadoop YARN[2] Understanding YARN architecture and features[3] Hadoop: Capacity Scheduler[4] Hadoop: Fair Scheduler[5] Hadoop YARN：调度性能优化实践[6] Uber 是如何低成本构建开源大数据平台的？[7] Untangling Apache Hadoop YARN, Part 1: Cluster and YARN Basics[8] Untangling Apache Hadoop YARN, Part 2: Global Configuration Basics[9] Untangling Apache Hadoop YARN, Part 3: Scheduler Concepts[10] Untangling Apache Hadoop YARN, Part 4: Fair Scheduler Queue Basics[11] Untangling Apache Hadoop YARN, Part 5: Using FairScheduler queue properties","link":"/hadoop-yarn-architecture.html"},{"title":"深入探索Java虚拟机运行时数据区域","text":"‘Java虚拟机规范’把Java虚拟机的运行时数据区域划分为线程隔离的程序计数器, 虚拟机栈, 本地方法栈和线程共享的方法区, 堆. 然而, ‘Java虚拟机规范’所描述的是Java虚拟机的概念模型(代表所有Java虚拟机的统一外观), 并未规定上述5个运行时数据区域的具体实现细节, 因此各款具体的Java虚拟机可能会用各种平台相关的, 更高效的方式进行等价的实现. Java虚拟机需要实现自动内存管理(主要是堆和方法区的内存管理), 势必引入垃圾收集器, 而内存的布局与管理又与所选用的垃圾收集器息息相关, 所以即使是同一种虚拟机, 使用不同垃圾收集器时其运行时数据区域(主要指堆和方法区)的实现都可能存在差别. 本文着重讲述Java虚拟机运行时数据区域的概念模型, 在一些重点部分加入了对HotSpot虚拟机具体实现的探讨, 特别总结了HotSpot虚拟机所实现的各个垃圾收集器的原理及其对应的自动内存管理方法. 除讲述Java虚拟机各个运行时数据区域的功能, 原理及实现细节外, 更重要的是, 本文将深入思考诸如, 为什么要设计这些区域? 为什么这个区域要这么实现? 是不是必须这么实现? 等问题. 例如, 我们都知道在’Java虚拟机规范中’规定程序计数器是线程私有的, 那么为什么要规定它是线程私有的? 它是不是必须是线程私有的? 有没有可能将其实现为线程共享的? 线程隔离区域‘Java虚拟机规范’ 中所规定的线程隔离的运行时数据区域包括程序计数器, 虚拟机栈和本地方法栈三部分. 然而, 在HotSpot虚拟机中虚拟机栈和本地方法栈被合二为一, 因此本文也将这两个部分合并讲述. 程序计数器程序计数器是Java虚拟机中最简单的部分, 实际上是一块较小的内存空间, 用于存储当前线程正在执行的字节码指令地址. 字节码解释器通过改变程序计数器的值来指示线程所需要执行的字节码指令. 程序的分支, 循环, 跳转, 异常处理, 线程恢复等基础功能都依赖程序计数器来完成. 这里的程序计数器是在抽象的JVM层上的字节码程序计数器, 而不是CPU上的程序计数器. 若线程正在执行的是一个Java方法, 那么程序计数器记录的是正在执行的虚拟机字节码指令地址, 这个字节码指令地址可能(‘可能’是相对于概念模型而言的, 每款虚拟机的实现可能不同)是对应字节码在方法中的偏移量, 也可能是该字节码指令在虚拟内存中的位置. 若线程正在执行的是一个本地方法, 那么程序计数器为空, 这是由于本地方法由操作系统直接执行, 并不需要JVM层面的程序计数器来指示需要执行的指令, 而CPU层面的程序计数器会进行相应的工作. 值的注意的是, 当线程执行已经由JIT编译器编译的Java方法时, 由于字节码指令已经被编译为机器指令, 因此程序计数器同执行本地方法时一样为空. 在 ‘Java虚拟机规范’ 中, 程序计数器是唯一一个没有规定内存溢出异常的运行时数据区域, 在HotSpot虚拟机中此内存区域也不存在任何内存溢出异常. 这是由于字节码指令地址的大小是固定的, 在JVM启动时即可分配足够的内存空间, 保证其不会溢出. 尽管’Java虚拟机规范’中把程序计数器规定为线程私有的, 但这是否必须取决于Java虚拟机对Java线程调度的实现. 如果由Java虚拟机负责Java线程的调度, 那么程序计数器并不一定需要线程私有, 也可以仅使用一个线程共享的全局程序计数器, 在线程切换时Java虚拟机势必通过一定的数据结构保存当前线程的运行状态, 若同时保存全局程序计数器的值, 即可在线程再次运行时恢复程序计数器的值. 如果Java虚拟机将Java线程的调度交由操作系统, 那么程序计数器必须线程私有, 这是由于操作系统在线程调度时并不会记录当前Java线程所运行的虚拟机字节码指令地址, 因此必须由线程自身记录其指令地址. HotSpot虚拟机在主流平台上的实现都由操作系统调度Java线程, 因此对于在这些平台上运行的HotSpot虚拟机, 程序计数器必须是线程私有的. 虚拟机栈和本地方法栈‘Java虚拟机规范’所描述的虚拟机栈和本地方法栈的功能和原理十分相似, 主要的区别是虚拟机栈为虚拟机执行Java方法服务, 而本地方法栈为虚拟机执行本地方法服务. 因此, 在HotSpot虚拟机中, 虚拟机栈和本地方法栈被合二为一. 本文也将一并讲述虚拟机栈和本地方法栈, 如无特殊说明后文说到的虚拟机栈包括’Java虚拟机规范’中所描述的虚拟机栈和本地方法栈. 运行时栈帧结构Java虚拟机以方法作为最基本的执行单元, 每一个方法从被调用到执行完毕的过程就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程. 在线程执行时, 每遇到一次方法调用, 虚拟机都会创建一个与调用方法所对应的栈帧, 并将其压入虚拟机栈. 虚拟机栈栈顶的栈帧对应着当前线程正在执行的方法. 栈帧是虚拟机栈的基本元素, 因此有必要进行深入了解. 每一个栈帧都包含局部变量表, 操作数栈, 动态连接, 方法返回地址及附加信息. 其中动态连接, 方法返回地址, 附加信息一般是静态的属性, 称为栈帧信息. 栈帧中各个部分的具体作用及原理如下. 局部变量表: 局部变量表是一组值的存储空间, 用于存放方法参数和方法内部定义的局部变量. 局部变量表的容量以变量槽为最小单位, ‘Java虚拟机规范’并没有明确规定一个变量槽实际占用的内存空间大小, 只是说明了每个变量槽都应当能够存放一个boolean, byte, char, short, int, float, reference或returnAddress类型的数据. 在64位的HotSpot虚拟机中long和double类型占用两个变量槽, 而reference类型只占用一个变量槽. 操作数栈: 操作数栈的元素可以是任意类型, 32位数据类型占用的栈容量为1, 64位数据类型占用的栈容量为2. 在一个方法正式开始执行之前其对应的操作数栈是空的, 在方法执行过程中会有各种字节码指令往操作数栈中写入或提取内容, 即入栈和出栈操作. 以整数加法为例, 字节码指令iadd将提取操作数栈栈顶的两个元素进行相加, 并将结果重新压入操作数栈. 动态连接: 每个栈帧都包含一个指向运行时常量池(见方法区)中该栈帧所属方法的引用, 持有这个引用是为了支持方法调用过程中的动态连接. 方法返回地址: 程序计数器中记录了当前方法正在运行的字节码指令地址, 当正在运行的方法(称主调方法)调用了另一个方法(称被调方法)时, 程序计数器将转而记录被调方法所正在执行的字节码指令地址, 而主调方法所执行到的字节码指令地址将被覆盖. 因此, 主调方法必须将当前程序计数器的值记录到栈帧中, 以便当被调方法成功返回后继续执行主调方法的剩余指令. 这个被记录的程序计数器的值即为方法返回地址. 附加信息: ‘Java虚拟机规范’允许虚拟机实现增加额外的描述信息, 例如与调试, 性能收集相关的信息. 各款虚拟机的附加信息各有不同. 实际上, 局部变量表和操作数栈的最大容量在使用javac编译时就已经确定, 结果会写入所生成的.class文件中. 以如下代码为例(除特殊说明外, 本文使用JDK 8进行实验).12345public class Test { public int test(int a, long b) { return a + 1; }} 上述代码定义了一个简单的Test类, 该类包含一个实例方法test. 根据上述原理可以推断, 在64位的HotSpot虚拟机中test方法的局部变量表有4个变量槽(每个实例方法都包含一个隐形的参数this, test方法共有3个参数, 其中参数this和a占一个变量槽, 参数b占两个变量槽, 共占4个变量槽), 其操作数栈的最大栈容量为2(执行a + 1操作时需要将变量a及1压入操作数栈, 所需栈容量为2). 我们可以用javap -verbose Test.class命令进行验证, 执行javap命令后可以看到如下信息(仅截取了与test方法相关的部分).1234567891011public int test(int, long); descriptor: (IJ)I flags: ACC_PUBLIC Code: stack=2, locals=4, args_size=3 0: iload_1 1: iconst_1 2: iadd 3: ireturn LineNumberTable: line 5: 0 其中, Code属性内的stack表示操作数栈的最大容量, locals表示局部变量表所占用的变量槽个数, args_size表示方法的参数个数. 内存溢出异常‘Java虚拟机规范’对虚拟机栈和本地方法栈定义了两类异常: 如果线程请求的栈深度大于虚拟机所允许的最大深度, 将抛出StackOverflowError异常; 如果Java虚拟机栈容量可以动态扩展, 当栈扩展无法申请到足够的内存时将抛出OutOfMemoryError异常. ‘Java虚拟机规范’明确允许虚拟机自行选择是否支持栈的动态扩展, 而HotSpot虚拟机并不支持栈扩展, 因此HotSpot虚拟机中的虚拟机栈在大部分情况下只会出现StackOverflowError异常. 在HotSpot虚拟机中, 虚拟机栈的大小可用-Xss设定, 在64位Linux操作系统下其默认值为1M, 可设定的最小值为228k. 我们可用如下代码简单探究HotSpot虚拟机的虚拟机栈实现.12345678910111213141516171819public class JavaVMStackSOF { private int stackLength = 1; public void stackLeak() { // long number; stackLength++; stackLeak(); } public static void main(String[] args) { JavaVMStackSOF oom = new JavaVMStackSOF(); try { oom.stackLeak(); } catch (Throwable e) { System.out.println(&quot;stack length: &quot; + oom.stackLength); throw e; } }} 读者可直接运行上述代码, 重复几次后会发现一个有趣的现象: 每次打印的栈深度都不同. 这是由于随着stackLeak函数被反复调用, JIT编译器会将该函数编译为本地字节码, 当被函数调用10,000(也可通过-XX:CompileThreshold参数设定)次之后, 服务端模式下的虚拟机会通过JIT编译器将该函数编译为本地字节码. 编译完成所需的时间视运行条件而定, 每次运行可能都会存在一定差别, 也正是这个编译完成时间的细微差别, 导致了每次打印的栈深度不同, 编译完成后若再次调用此函数则虚拟机会自动调用经JIT编译器编译过的版本(也可采用-XX:-BackgroundCompilation参数静止后台编译, 这样一旦触发JIT编译, 一切对该方法的调用都将被阻塞, 直到编译完成). 被编译为本地字节码后stackLeak函数对应的栈帧会变小. 读者可在运行时增加JVM参数-Djava.compiler=NONE或-Xint明确指示虚拟机关闭JIT编译器. 这样无论运行几次上述代码, 所打印的栈深度都是10780, 这个值一定比不使用-Djava.compiler=NONE参数时小, 这也印证了JIT编译后的栈帧会变小. 为了验证HotSpot虚拟机默认虚拟机栈的大小, 可在运行上述代码时添加JVM参数-Xss1M -Djava.compiler=NONE, 可以看到打印的栈深度也为10780(作者的运行环境是64位Linux OracleJDK 8). 如果取消第5代码的注释再次运行, 可以看到打印的栈深度减小, 这是由于方法中增加了一个局部变量的定义, 从而增加了局部变量表的占用空间, 使得方法对应的栈帧变大, 在相同栈内存的情况下, 能够递归调用的次数就少了. 线程共享区域‘Java虚拟机规范’中规定的线程共享的运行时数据区域包括方法区和堆, Java虚拟机对这部分内存区域的自动化管理是使得Java语言区别于C/C++等需要手动管理内存的语言的重要特征之一. 方法区方法区用于存储已被虚拟机加载的类型信息, 常量, 静态变量, JIT编译器编译后的代码缓存等数据. 虚拟机对方法区的实现存在很大不同和变化, 在JDK 8之前, HotSpot虚拟机采用永久代实现方法区, 从JDK 8开始, 永久代已经被完全废弃, 取而代之的是由本地内存实现的元空间. 方法区中包含运行时常量池, 用于存储由javac编译生成的Class文件中的常量池表, 不过除了保存Class文件中描述的符号引用外, 一般也会把由符号引用翻译过来的直接引用存储在运行时常量池中(视不同虚拟机的具体实现而定, ‘Java虚拟机规范’并未对运行时常量池的实现细节做任何规定). 注意运行时常量池与Class文件中常量池的区别. Class文件中的常量池在编译器已经确定不会更改, 而运行时常量池可能随着程序的运行而变化. 在类加载阶段, 虚拟机会将Class文件中的二进制字节流存储到方法区中, 但’Java虚拟机规范’并为规定方法区的具体数据结构, 一般来说Class文件中的常量池将被加载到运行时常量池中. 根据’Java虚拟机规范’的规定, 如果方法区无法满足新的内存分配需求时, 将抛出OutOfMemoryError异常. 事实上, 在JDK 8之后由于方法区是基于本地内存的元空间实现的, 因此在默认情况下除非本地内存溢出, 否则方法区不会抛出OutOfMemoryError异常. 不过HotSpot虚拟机还是提供了一些参数用于控制方法区: -XX:MaxMetaspaceSize: 设置元空间最大值, 默认是-1, 即只受限于本地内存大小. 该值不能设置得过小, 一般在10M以上, 设置得太小虚拟机将拒绝启动, 并提示MaxMetaspaceSize is too small. -XX:MetaSpaceSize: 指定元空间的初始空间大小, 以字节为单位, 达到该值就会触发垃圾收集进行类型卸载, 同时收集器会对该值进行调整: 如果释放了大量的空间, 就适当降低该值; 如果释放了很少的空间, 那么在不超过-XX:MaxMetaspaceSize(如果设置了的话)的情况下, 适当提高该值. -XX:MinMetaspaceFreeRatio: 作用是在垃圾收集之后控制最小的元空间剩余容量的百分比, 可减少因为元空间不足导致的垃圾收集的频率. 类似的还有-XX:MaxMetaspaceFreeRatio, 用于控制最大的元空间剩余容量的百分比. 运行时常量池中, 与我们最密切相关的当属字符串常量池, 由于字符串常量池的存在会出现一些有趣的现象, 我们都知道Java语言中不能通过==来判断两个字符串是否相等, 对于字符串而言==判断的是两个字符串对象引用的地址是否相等. 但是通过如下代码将会发现b1, b2的值为true, 而b3的值为false.123456789101112public class StringConstantPool { public static void main(String[] args) { String str1 = &quot;str&quot;; String str2 = &quot;str&quot;; String str3 = &quot;s&quot; + &quot;t&quot; + &quot;r&quot;; String str4 = new String(&quot;str&quot;); boolean b1 = str1 == str2; boolean b2 = str1 == str3; boolean b3 = str1 == str4; }} 这其中比较奇怪的现象可能是str1, str2和str3为什么会相等, 而却与str4不相等. 这个现象就与字符串常量池密切相关了: 在Java语言中定义的字面量字符串或其运算都会进入字符串常量池, 当后续对象定义了相同的字符串字面量值时会直接引用字符串常量池中存在的值, 即str1, str2和str3所引用的其实是字符串常量池中的同一个对象. 但是new关键字创建的对象在堆上产生, 不会进入字符串常量池. 此外调用String::intern()也会使字符串进入字符串常量池. 堆堆在虚拟机启动时创建, ‘几乎’所有对象实例及数组元素(为方便描述之后将省略数组元素, 但读者应当明确数组元素也是在堆上进行分配的)都在堆上分配内存. 堆是Java虚拟机运行时数据区域中最为复杂的部分, 堆上的内存空间由垃圾收集器进行管理, 堆内存的分配与回收策略与所选用的垃圾收集器密切相关. 从实现角度来看, 所有对象实例都在堆上分配并非那么绝对, 随着逃逸分析技术的日渐强大, 栈上分配, 标量替换等优化手段已经导致一些对象实例并非分配在堆内存上, 而可能分配在栈中. 出于严谨性考虑, 这里使用了’几乎’. 由于堆内存需要垃圾收集器进行自动管理, 因此我们首先讲述与垃圾收集相关的理论, 包括对象存活判定方法和垃圾收集算法, 最后讲述HotSpot虚拟机中的各款垃圾收集器具体如何运用上述垃圾收集理论管理堆内存. 对象存活判定堆中存放着Java对象实例, 垃圾收集器所收集的实际上是已经’死去’的对象, 即在Java程序中不会再被引用的对象. 因此, 在进行垃圾收集之前, 首先要解决的是如何判定对象实例是否需要被回收. 常用的对象存活判定方法有两种: 引用计数算法和可达性分析算法. 这里说到对象实例一般指对象的实际内容, 存储在堆内存上. 而对象引用指Java程序中的引用变量, 存储在虚拟机栈的局部变量表中. 对象实例占用的堆内存需要在对象’死亡’后, 由垃圾收集器进行回收. 而对象引用会随着栈帧在虚拟机栈中入栈和出栈而出生和消亡, 不需要垃圾收集器的介入. 引用计数算法在对象中添加一个引用计数器, 每当程序中引用该对象时, 计数器的值就加一; 当引用失效时, 计数器的值就减一; 任何时刻计数器为零的对象就是不可能再被使用的对象, 需要垃圾收集器回收该对象实例所占用的堆内存空间. 引用计数算法原理简单, 判定效率高, 在多数情况下是一个不错的算法, 当下十分流行的Python语言就是用引用计数算法来管理内存. 然而, 在Java领域主流的Java虚拟机并未使用引用计数算法进行内存管理. 这是由于单纯的引用计数算法在某些特殊情况下难以正确工作, 必须配合大量的额外工作处理这些特殊情况. 例如, 单纯的引用计数算法无法解决循环引用问题, 如下代码即是一个循环引用的例子.1234567891011121314public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; private byte[] bigSize = new byte[2 * _1MB]; public static void main(String[] args) { ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; System.gc(); }} 上述代码中, 尽管在11行之后, objA和objB所引用的对象实例已经不可能再被访问, 应当由垃圾收集器回收其所占用的内存. 但若采用引用计数算法判定对象是否’存活’, 由于objA所指对象实例中保存着对objB所指对象实例的引用, 反之亦然. 因此两个对象实例中的引用计数器不为零, 相应的垃圾收集器也就无法回收其占用的内存. 而Java虚拟机却可以成功回收objA和objB所引用对象实例的内存, 如下是在JDK 8中的运行结果(启动时需添加JVM参数-XX:+PrintGCDetails以打印垃圾回收日志). 从中可以看到确实发生了垃圾收集, 这也在一定程度上说明了Java虚拟机并非使用引用计数算法判定对象是否’存活’.1234567891011[GC (System.gc()) [PSYoungGen: 9344K-&gt;840K(76288K)] 9344K-&gt;848K(251392K), 0.0014773 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 840K-&gt;0K(76288K)] [ParOldGen: 8K-&gt;619K(175104K)] 848K-&gt;619K(251392K), [Metaspace: 3124K-&gt;3124K(1056768K)], 0.0045232 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap PSYoungGen total 76288K, used 1968K [0x000000076b500000, 0x0000000770a00000, 0x00000007c0000000) eden space 65536K, 3% used [0x000000076b500000,0x000000076b6ec1d8,0x000000076f500000) from space 10752K, 0% used [0x000000076f500000,0x000000076f500000,0x000000076ff80000) to space 10752K, 0% used [0x000000076ff80000,0x000000076ff80000,0x0000000770a00000) ParOldGen total 175104K, used 619K [0x00000006c1e00000, 0x00000006cc900000, 0x000000076b500000) object space 175104K, 0% used [0x00000006c1e00000,0x00000006c1e9ad78,0x00000006cc900000) Metaspace used 3131K, capacity 4496K, committed 4864K, reserved 1056768K class space used 342K, capacity 388K, committed 512K, reserved 1048576K 事实上, Java虚拟机是采用可达性分析算法来判定对象是否’存活’的. 可达性分析算法的基本思路是通过一系列称为’GC Roots’的根对象作为起始节点集, 从这些节点开始, 根据引用关系向下搜索, 搜索过程所走过的路径称为’引用链’, 如果某个对象到GC Roots间没有任何引用链相连, 则证明此对象是不可能再被使用的. 如下图所示, 对象Object 5, Object 6, Object 7之间虽然存在引用, 但是它们到GC Root Set是不可达的, 因此将被判定为可回收对象. 垃圾收集算法根据上节所述的对象存活判定方法, 可将垃圾搜集算法分为两大类: 引用计数式垃圾收集(直接垃圾收集); 追踪式垃圾收集(间接垃圾收集). 由上节可知, 当前主流的Java虚拟机均采用可达性分析算法判定对象是否’存活’, 这就意味着需要使用追踪式的垃圾收集算法, 因此本文介绍的垃圾收集算法均属于追踪式垃圾收集范畴. 由于当前主流Java虚拟机的垃圾收集器都基于分代理论进行设计, 因此在正式介绍垃圾收集算法之前首先讲述分代收集理论, 后面所介绍的垃圾收集算法适用于不同的分代. 分代收集理论实际上是一套符合大多数程序运行实际情况的经验法则, 它建立在两个分代假说之上: 弱分代假说: 绝大多数对象都是朝生夕灭的; 强分代假说: 熬过越多次垃圾收集过程的对象就越难消亡. 这两个分代假说共同奠定了多款常用垃圾收集器的一致设计原则: 收集器应该将Java堆划分出不同的区域, 然后将回收对象依据其年龄(年龄即熬过垃圾收集过程的次数)分配到不同的区域之中存储. 一般的Java虚拟机至少会把堆划分为新生代和老年代两个区域. 在新生代中每次垃圾收集后都有大量的对象实例被回收, 而每次回收后存活的对象将逐步晋升到老年代中. 在G1收集器出现之前, HotSpot虚拟机一般把整块堆内存按比例划分为新生代和老年代, 两个不同的分代区域由各自的垃圾收集器负责管理. 追踪式垃圾收集算法主要有以下三种. 标记-清除算法: 首先标记出所有需要回收的对象, 在标记完成后, 统一回收所有被标记的对象, 也可以反过来, 标记存活的对象, 统一回收所有未被标记的对象. 该算法有两个明显的缺点: 执行效率低下和内存空间碎片化. 标记-复制算法: 将可用内存划分为大小相等的两块, 每次只使用其中的一块, 当需要进行垃圾收集时将其中一块内存上的存活对象复制到另一块内存上, 再把已使用过的内存一次清理掉. G1收集器之前的垃圾收集器大多采用了这种方法进行新生代收集, 不过由于新生代对象大多都是”朝生夕灭”的, 因此虚拟机并未直接把新生代区域一分为二, 只使用其中一份. 而是将新生代分为一个Eden空间和两个Survivor空间(一个Eden空间和一个Survivor空间默认的大小比值为8:1), 首先使用Eden空间和一个Survivor空间, 需要垃圾收集时再将存活对象复制到另一个Survivor空间, 这样就只有10%的空间浪费. 标记-整理算法: 首先标记存活的对象, 在标记完成后, 将存活的对象都向内存空间一端移动, 最后直接清理掉边界以外的内存. 堆内存空间划分从上节关于垃圾收集算法的讲述中可以得知, HotSpot虚拟机中的垃圾收集器大多基于分代收集理论进行设计, 因此使用这些垃圾收集器时就必须对Java堆进行逻辑上的分代划分. 一般来说, Java堆会被划分为新生代(包括Eden空间, Survivor空间)和老年代等多个逻辑区域. 其划分方式随着G1收集器(见Garbage First收集器)的出现, 发生了重要改变. Java堆的大小可用以下虚拟机参数设定: -Xms或-XX:InitialHeapSize: Java堆的初始大小, 及Java虚拟机进程启动时堆的大小. -Xmx或-XX:MaxHeapSize: Java堆的最大值. 读者可使用java -XX:+PrintCommandLineFlags -version命令查看Java堆的初始大小和最大值. 在G1收集器出现以前, 也即JDK 8及之前虚拟机在服务端模式下的默认状态下, 整个Java堆被划分为新生代和老年代. 其中, 新生代又被划分为Eden空间和Survivor空间. 在JDK 8中, 默认情况下新生代和老年代占用的堆内存大小比值为1:2, Eden空间和Survivor空间大小的比值为8:2, 其中Survivor空间被平均分成两个区域, 称为From Survivor和To Survivor. 分代的大小可用如下虚拟机参数设定: -XX:NewSize: 新生代的大小, 在JDK 8中默认为整个Java堆大小的1/3. -XX:MaxNewSize: 新生代大小的最大值. -Xmn: 相当于把-XX:NewSize和-XX:MaxNewSize设定为同一值. -XX:Newratio: 新生代和老年代大小的比值, 在JDK 8中默认为值2, 即新生代和老年代大小的比值为1:2. -XX:Surviorratio: 新生代中Eden空间和一个Survivor空间大小的比值, 在JDK 8中默认为8, 即Eden空间和一个Survivor空间大小的比值为8:1. 在HotSpot虚拟机中, 新生代大多采用标记-复制算法, 开始时新生代中真正能使用的内存空间只占整个新生代的90%(即Eden空间和From Survivor空间), 在进行Minor GC时, 存活的对象会被复制到To Survivor空间, 然后清理整个Eden空间和From Survivor空间, 并将From Survivor和To Survivor空间的角色互换. 若To Survivor的空间不足以容纳一次Minor GC之后存活的对象, 就需要依赖其他内存区域(大多数是老年代)进行分配担保. 垃圾收集器垃圾收集器是Java虚拟机内存管理子系统的重要组成部分, 用于自动回收已经不会再被程序使用的堆内存. HotSpot虚拟机中的垃圾收集器可以G1收集器为分界, 在G1收集器之前每款垃圾收集器都只负责新生代或老年代的垃圾收集, 对于整个堆的垃圾收集, 必须要有两款垃圾收集器配合工作. G1收集器是第一款全功能的垃圾收集器, 负责整个Java堆的垃圾回收. 随着垃圾收集器技术的不断发展, 目前已经出现了两款更高性能的垃圾收集器: Shenandoah收集器和ZGC收集器. 这两款垃圾收集器与G1收集器一样都是基于Region内存布局的, 可见从G1收集器开始, 全功能的, 基于Region内存布局的垃圾收集器渐渐成为主流. 由于这两款收集器尚在实验状态, 几乎不会在生产环境中使用, 因此本文并不会详细讲述这两款垃圾收集器, 而只讲述到G1收集器为止曾经在HotSpot虚拟机中出现过的垃圾收集器. Serial收集器和Serial Old收集器Serial收集器通常搭配Serial Old收集器使用(当然这并非唯一的搭配方式), Serial收集器用于新生代收集, Serial Old收集器用于老年代收集. 这两款收集器在进行垃圾收集时都必须暂停所有用户线程, 且仅使用一个垃圾收集线程. 这看起来似乎十分低效, 然而正因如此, 这两款收集器所占用的额外内存消耗是最小的, 因此一般用于客户端模式, 适用于具有内存限制的桌面端或微服务场景. 32位的Java虚拟机有两种运行模式: 客户端模式(Client VM)和服务器模式(Server VM), 客户端模式启动的是更为轻量级的虚拟机进程, 具有更快的启动速度, 适用于交互式应用, 而服务器模式相对重量级, 适用于分析型应用. 在64位虚拟机中已经弃用了客户端模式. 在HotSpot虚拟机中, 启动时增加-XX:+UseSerialGC参数可以使用Serial + Serial Old收集器组合进行内存回收, 这也是客户端模式下的默认垃圾收集器组合. ParNew收集器和CMS收集器ParNew收集器经常搭配CMS(Concurrent Mark Sweep)收集器使用, 在JDK 9之前, ParNew收集器尚可搭配Serial Old收集器, CMS收集器尚可搭配Serial收集器, 然而从JDK 9开始这两种搭配方式均被废弃了, 这也意味着从JDK 9开始ParNew收集器只能搭配CMS收集器使用. 在HotSpot虚拟机中, ParNew收集器是激活CMS收集器(使用-XX:+UseConcMarkSweepGC参数)后的默认新生代收集器, 当然也可使用-XX:+/-UseParNewGC选项来强制启用或禁用它. ParNew收集器用于新生代收集, 是Serial收集器的多线程版本, 除了支持多线程并行收集外, 其他与Serial收集器相比并没有太多创新, 在实现上这两种收集器也公用了很多代码. ParNew收集器在单核处理器上绝对不会比Serial收集器有更好的效果, 它默认开启的线程数与处理器核心数相同, 在处理器核心很多的情况下可使用-XX:ParallelGCThreads参数来指定垃圾收集线程的数量. CMS收集器用于老年代收集, 是第一款真正意义上的并发收集器(这里的并发是指垃圾收集线程和用户线程可同时运行), 以获取最短停顿时间为目标. CMS收集器是基于标记-清除算法实现的, 回收过程分为以下四个步骤: 初始标记: 需要暂停用户线程, 仅仅标记GC Roots能直接关联的对象, 速度很快; 并发标记: 可与用户线程并行, 从GC Roots的直接关联对象开始遍历整个对象图, 耗时较长; 重新标记: 需要暂停用户线程, 为修正并发标记期间, 因用户线程继续运行而导致标记产生变动的那一部分对象的标记记录, 停顿时间比初始标记稍长, 但远比并发标记短; 并发清除: 可与用户线程并行, 清理上述三个标记阶段判断已经死亡的对象所占用的堆内存空间. 由上述过程可以看出, CMS回收过程中只有两个耗时较短的过程(初始标记和重新标记)需要暂停用户线程, 而耗时较长的并发标记和并发清除阶段则可与用户线程并行, 这大大减少了用户线程的停顿时间. CMS收集器是HotSpot虚拟机追求低停顿的第一次成功尝试, 但仍有以下缺点: CMS收集器对处理器资源非常敏感, 在并发阶段由于占用处理器资源可能使用户线程运行变慢. CMS收集器无法处理浮动垃圾, 可能出现Concurrent Mode Failure, 而启用Serial Old收集器重新回收老年代. CMS收集器基于标记-清除算法, 收集结束后可能会产生大量空间碎片, 导致老年代即使有足够的内存空间, 也无法找到足够大的连续空间来分配大对象, 从而提前触发一次Full GC. Parallel Scavenge和Parallel OldParallel Scavenge收集器和Parallel Old收集器都是吞吐量优先的收集器, 这两款收集器的搭配常用于需要在后台运算而不需要太多交互的分析任务. 可通过-XX:+UseParallelOldGC参数启用这两款收集器的搭配组合. 需要注意的是, JDK 9之前Parallel Scavenge收集器是服务端模式下的默认新生代收集器, 但老年代收集器却是一个称为PS MarkSweep的收集器, 这款收集器的实现与Serial Old几乎一摸一样, 不过它从属于Parallel Scavenge架构, 无法单独使用, 通过-XX:+UseParallelGC参数可启用Parallel Scavenge收集器加PS MarkSweep收集器的组合. Parallel Scavenge是一款新生代收集器, 与ParNew收集器的重要区别是, ParNew收集器关注用户线程的停顿时间, 而Parallel Scavenge收集器的目标是达到一个可控制的吞吐量, 即处理器用于运行用户代码的时间与处理器总消耗时间的比值:$$ 吞吐量 = \\frac{运行用户代码时间}{运行用户代码时间 + 运行垃圾收集时间} $$Parallel Scavenge收集器的一个重要特性是具有自适应调节策略, 可通过-XX:UseAdaptiveSizePolicy参数启用, 当这个参数激活后就不需要人工指定新生代的大小(-Xmn), Eden空间与Survivor空间的比例(-XX:SurvivorRatio), 晋升到老年代对象大小这些参数了, 虚拟机会根据当前系统的运行状况自动调节这些参数. 此外, Parallel Scavenge收集器还有两个参数用于精度控制吞吐量: -XX:GCTimeRatio: 一个大于0小于100的整数, 表示垃圾收集时间占总时间的比率, 即吞吐量的倒数, 默认值为99, 即允许最大1%(1 / (1 + 99))的垃圾收集时间; -XX:MaxGCPauseMillis: 一个大于0的毫秒数, 收集器将尽力保证内存回收花费的时间不超过此值. 需要注意的是, 减小此值并不能使垃圾收集的速度更快, 它只能使每次垃圾收集的停顿时间变短, 这是牺牲吞吐量换来的, 为了降低每次垃圾收集的停顿时间, 只能尽可能将新生代设置地更小, 假设原来500M的新生代每隔10秒进行一次垃圾收集, 每次停顿100毫秒, 现在为了降低停顿时间把新生代设置为300M, 每隔5秒收集一次, 每次停顿时间70毫秒, 显然每次垃圾收集的停顿时间变短了, 但是吞吐量却也下降了. Parallel Old收集器是Parallel Scavenge收集器的老年代版本, 与Parallel Scavenge一样也是一款关注吞吐量的收集器, 直到JDK 6才开始提供. Garbage First收集器Garbage First (G1)收集器是垃圾收集器技术发展史上具有划时代意义的成果, 在G1收集器之前, 垃圾收集的目标要么是整个新生代(Minor GC), 要么是整个老年代(Major GC), 再要么就是整个Java堆(Full GC), 而G1收集器开创了收集器面向局部收集的设计思路和基于Region的内存布局形式. 从JDK 9开始, G1收集器已经成为HotSpot虚拟机在服务端模式下的默认垃圾收集器. G1不再以固定大小及数量的分代区域划分Java堆, 而是把连续的Java堆划分为多个大小相等的独立区域(Region), 每个Region可以根据需要, 扮演新生代的Eden空间, Survivor空间, 老年代空间, 或用于存储大对象的Humongous区域. G1收集器能对扮演不同角色的Region采用不同的策略去处理, 这样使得每个区域的收集都能达到良好的效果. G1收集器的回收过程如下: 初始标记: 仅标记GC Roots能直接关联的对象. 这一阶段需要暂停用户线程, 但是停顿时间很短, 而且是借用进行Minor GC的时候同步完成的, 所以G1收集器在这一阶段实际上并没有额外的停顿. 并发标记: 从GC Roots开始对堆中的对象进行可达性分析, 递归扫描整个堆的对象图, 找出要回收的对象. 这一阶段耗时较长, 但可与用户线程并发执行. 最终标记: 对用户线程做一个短暂的暂停, 用于处理并发标记阶段用户线程继续运行而改变的对象引用. 筛选回收: 负责更新Region的统计数据, 对各个Region的回收价值和成本进行排序, 根据用户所期望的停顿时间来指定回收计划, 可以自由选择任意多个Region构成回收集, 然后把决定回收的那一部分Region的存活对象复制到空的Region中(标记-复制算法), 再清理掉整个旧Region的全部空间. 这里的操作涉及存活对象的移动, 是必须暂停用户线程, 由多条线程并行完成的. 总结这篇博文深入探讨了Java虚拟机的5个运行时数据区域, 重点讲述了Java堆内存区域及与其密切相关的几款垃圾收集器. 相信在认真研读本文后, 就能对Java虚拟机的内存管理子系统有深入的理解. 本文对每个运行时数据区域的描述基本遵循以下几个原则: 这一区域的作用是什么. 这一区域的工作原理. 这一区域可能出现的异常. 虚拟机对这一区域提供了什么样的控制和优化参数. 如果在学习Java虚拟机运行时数据区域时, 能够遵循上述原则, 不断思考和实践, 定能更加深入地理解Java虚拟机的内存管理子系统.","link":"/jvm-runtime-data-area.html"},{"title":"Logistic Regression (LR)","text":"Logistic regression (LR) is a binary classification algorithm for predicting discrete values (In general, the predicted result is 0 or 1). This blog will detail the modeling approach, loss function, forward and backward propagation of LR. In the end, I will use python with numpy to implement LR and give the use on data sets iris and mnist. You can find all the code here. Sigmoid functionSigmoid function also called logistic function is an S-shaped function that “squashes” the variable into the range [0, 1]. The sigmoid function is defined by the following formula:$$ \\delta(x) = \\frac{1}{1 + e^{-x}} $$ The function image of the sigmoid function is an sigmoid curve. In machine learning, we often use the derivative of the sigmoid function. So Let’s first calculate the derivative of the sigmoid function:$$ \\delta’(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}} \\cdot \\frac{1+e^{-x}-1}{1+e^{-x}} = \\delta(x) \\cdot (1-\\delta(x))$$ The sigmoid function has two common uses. Activation function as a neural network; Used for probabilistic modeling for binary classification.In this blog, we mainly use the sigmoid function in binary classification. Modeling approachIn LR we use sigmoid function to model probability. Now suppose we have a binary classification problem, and we got a training set $ {(x_1, y_1), (x_2, y_2), …, (x_m, y_m)} $ of $ m $ labeled examples, where the input features are $ x_i \\in \\Re^{[n, 1]} $. To solve this binary classification problem, we need to build a classification model and train the model with the existing m samples. If we choose the appropriate model and do the right training, then we can use the model we trained to predict the new sample to determine which category it belongs to. Based on our understanding of the sigmoid function, we can see that it is very suitable for solving binary classification problems. We can take the value of the sigmoid function as the probability that the sample belongs to a certain category. More formal, if $ y \\in {0, 1} $. Then LR can be expressed as:$$ P(y=1 | x;w,b)=\\frac{1}{1+e^{-(w^Tx+b)}} $$$$ P(y=0 | x;w,b)=1-p(y=1|x;w,b) $$ Which $ w \\in \\Re ^ {[n, 1]} $ is the weight vector of the model, $ b \\in \\Re $ is the bias term. For the convenience of writing, let $ \\hat{y}=P(y=1 | x;w,b) $. Loss functionBased on the LR model, we can derive its objective function by the maximum likelihood method. Our goal is to minimize the difference between the predicted and actual values of the model, which is equivalent to:$$ maximize \\ \\ \\ \\ P(y=1|x;w,b)^y \\cdot P(y=0|x;w,b)^{1-y} $$$$ maximize \\ \\ \\ \\ P(y=1|x;w,b)^y \\cdot (1-P(y=1|x;w,b))^{1-y} $$$$ maximize \\ \\ \\ \\ \\hat{y}^y \\cdot (1-\\hat{y})^{1-y} $$$$ minimize \\ \\ \\ \\ -log[\\hat{y}^y \\cdot (1-\\hat{y})^{1-y}] $$$$ minimize \\ \\ \\ \\ -[y \\cdot log\\hat{y} + (1-y) \\cdot log(1-\\hat{y})] $$So the loss function of LR is:$$ \\mathcal L(y, \\hat{y})=-[y \\cdot log\\hat{y} + (1-y) \\cdot log(1-\\hat{y})] $$ Gradient derivationIn order to train the LR model, we need to update the gradient of the model’s weight vector. Although the automatic differentiation algorithm can help us automate this operation. In this blog I will still manually derive the gradient of the weight vector to deepen the understanding of LR. The forward propagation of the LR model is as follows: for one example:$$ z=w^Tx+b $$$$ \\hat{y}=\\delta{(z)} $$ vectorization:$$ Z=Xw+b $$$$ \\hat{Y}=\\delta{(Z)} $$Based on the chain rule we can easily find the gradient of any variable. for one example:$$ \\frac{\\partial \\mathcal L}{\\partial \\hat{y}}=-\\frac{y}{\\hat{y}} + \\frac{1-y}{1-\\hat{y}} $$$$ \\frac{\\partial \\hat{y}}{\\partial z}=\\hat{y}\\cdot(1-\\hat{y}) $$$$ \\frac{\\partial z}{\\partial w}=x \\ \\ \\ \\ \\frac{\\partial z}{\\partial b}=1 $$ $$ \\frac{\\partial \\mathcal L}{\\partial w}=\\frac{\\partial \\mathcal L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}=x(\\hat{y}-y) $$$$ \\frac{\\partial \\mathcal L}{\\partial b}=\\frac{\\partial \\mathcal L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}=\\hat{y}-y $$ vectorization (m is the number of examples, n is the number of features):$$ \\frac{\\partial \\mathcal L}{\\partial w}=\\frac{1}{m}X^T(\\hat{Y}-Y) \\ \\ \\ \\ X \\in \\Re^{[m, n]} \\ \\ \\ \\ Y, \\hat{Y} \\in \\Re^{[m, 1]} $$$$ \\frac{\\partial \\mathcal L}{\\partial b}=\\frac{1}{m}\\sum (\\hat{Y}-Y) $$ ImplementationWith the existing library we can implement LR very conveniently. But here we use pure python with numpy to achieve deeper understanding of LR. In order to follow the habit we use the same interface as sklearn.With numpy we can easily implement sigmoid function.12def sigmoid(x): return 1 / (1 + np.exp(-x))The LR model can also be quickly implemented based on the above derivation.12345678910111213141516171819202122232425262728class LogisticRegression: def __init__(self, learning_rate=0.01, max_iter=200): self.learning_rate = learning_rate self.max_iter = max_iter def fit(self, X, Y): m, n = X.shape self.w_ = np.zeros([n, 1]) self.b_ = 0 self.cost_ = [] for i in range(self.max_iter): Y_hat = self.predict(X) cost = -np.sum(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)) / m if i != 0 and i % 10 == 0: print(&quot;Step: &quot; + str(i) + &quot;, Cost: &quot; + str(cost)) self.cost_.append(cost) self.w_ -= self.learning_rate * np.dot(X.T, Y_hat - Y) / m self.b_ -= self.learning_rate * np.sum(Y_hat - Y) / m def predict(self, X): return sigmoid(np.dot(X, self.w_) + self.b_) def score(self, X, Y): Y_hat = self.predict(X).reshape(-1) Y_predict = np.array([1 if y &gt;= 0.5 else 0 for y in Y_hat]) false_num = np.sum(Y_predict == Y.reshape(-1)) return false_num / len(X) ExampleIn order to verify the correctness of the implementation. I experimented on the irsi dataset and the mnist dataset. The parameters and results of the experiment are as follows: iris mnist learnig rate 0.1 0.01 max iterate 100 300 test accuracy 100% 99.86% You can find the all the experimental code here and reproduce the experimental results.","link":"/logistic-regression.html"},{"title":"HBase最佳实践 - HBase过滤器源码剖析及自定义过滤器","text":"本文首先结合HBase过滤器的源码, 讲述HBase过滤器抽象基类Filter中各个函数的作用. 最终给出一个简单的自定义过滤器的案例, 在此基础上分析了Filter中各个方法的执行流程, 读者在理解该案例的基础上可以编写任何个性化的过滤器. 本文涉及的源码基于HBase 1.4.x. HBase自定义过滤器能够在RegionServer上, 执行更加复杂和个性化的过滤操作. 例如, 在利用HBase存储时空数据时, 可以将时空范围的过滤操作放到过滤器中. 使用过滤器主要有以下两个优点: 一是通过使用过滤器将过滤逻辑放到RegionServer上执行, 可以使客户端代码更加简洁; 二是将过滤操作放到RegionServer上执行可以减少网络传输, 提升查询效率. 当然, 使用自定义过滤器也存在一定的缺点: 过滤器对象需要在HBase的Client和RegionServer之间通过网络传输, 对于用户自定义过滤器, 需要重写序列化和反序列化操作, 不同于Go语言, Java语言并没有提供原生的高效序列化支持, HBase预定义的过滤器使用了Protocol Buffers进行序列化与反序列化. 本文给出的案例使用了Java的NIO进行自定义过滤器对象的序列化和反序列化, 在生产环境中建议使用Protocol Buffers. HBse自定义过滤器概览HBase过滤器属于客户端API, 相关的源码均在hbase-client模块的org.apache.hadoop.hbase.filter包下. 对于设定了过滤器的scan操作, 在每个Region上过滤器都会起作用. 自定义HBase过滤器需要继承抽象类Filter或抽象类FilterBase. Filter是FilterBase的基类, FilterBase为我们提供了一些方法的默认实现, 在自定义过滤器时可先继承FilterBase, 实现其抽象方法, 在需要更进一步的个性化操作时再考虑按需实现Filter提供的方法. 在具体探讨HBase过滤器之前, 我们将首先探讨抽象类Filter中各个函数的作用, 理解这些函数的具体作用是我们进行后续研究的基础. 类Filter的定义如下, 这里为每个属性和函数添加了详细的注释, 解释其作用. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130public abstract class Filter { /** * 是否逆序扫描的标志. true表示当前以字节序从大到小扫描, false表示当前以字节序从小到大扫描. */ protected transient boolean reversed; /** * 在每行扫描结束时调用, 一般用于重置过滤标志 */ abstract public void reset() throws IOException; /** * 真正开始过滤的第一步, 用于对row key进行过滤. 若过滤当前行, 返回true, 若保留当前行返回false. * @param buffer 包含row key的字节数组 * @param offset row key在buffer数组中的开始位置 * @param length row key的长度 */ abstract public boolean filterRowKey(byte[] buffer, int offset, int length) throws IOException; /** * 用于判断是否需要继续进行过滤操作, 在调用filterRowKey(), filterKeyValue(), transformCell(), * reset()函数之后都会调用. 返回true表示不需要继续进行过滤, 将结束包含当前过滤器的scan操作, * 返回false表示需要继续进行后续过滤操作. */ abstract public boolean filterAllRemaining() throws IOException; /** * 真正开始过滤的第二步, 用于对每个cell进行过滤, 其返回值的含义参考枚举ReturnCode * @param v 当前行中的每个cell */ abstract public ReturnCode filterKeyValue(final Cell v) throws IOException; /** * 对于filterKeyValue()中通过的cell调用, 用于对cell进行一定的转换操作, 一般直接返回传入的参数即可 * @param v 经过filterKeyValue()决定保留的cell */ abstract public Cell transformCell(final Cell v) throws IOException; /** * 弃用, 使用transformCell()代替 */ @Deprecated abstract public KeyValue transform(final KeyValue currentKV) throws IOException; public enum ReturnCode { // 包含当前cell INCLUDE, // 包含当前cell, 查询下一列 INCLUDE_AND_NEXT_COL, // 与INCLUDE的区别是会忽略旧版本数据 // 跳过当前cell SKIP, // 跳过当前列, 查询当前行的下一列 NEXT_COL, // 与SKIP仅有细微差别, 如果序遍历所有列使用NEXT_COL, 如果不需要查询当前cell之后的列使用SKIP // 查询当前column family中的下一行. 注意: 当前行的其他column family在之后仍可能被查询 NEXT_ROW, // 调用getNextCellHint()决定下一个需要查询的cell SEEK_NEXT_USING_HINT, // 包含当前cell并查询下一行, 当前行中所有剩余的cell都将不会被查询 INCLUDE_AND_SEEK_NEXT_ROW, } /** * 对该函数的具体作用尚未完全明了. 根据源码中的解释, 该函数是对需要提交的的cell进行进行原地更改. * 一般情况下无需重载该函数. */ abstract public void filterRowCells(List&lt;Cell&gt; kvs) throws IOException; /** * 主要用于检查冲突的scans(例如不一次读取整行的scan) */ abstract public boolean hasFilterRow(); /** * 最后用于决定是否过滤当前行的机会, 若返回true过滤当前行, 否则保留当前行 */ abstract public boolean filterRow() throws IOException; /** * 弃用, 使用getNextCellHint()代替 */ @Deprecated abstract public KeyValue getNextKeyHint(final KeyValue currentKV) throws IOException; /** * 当filterKeyValue()返回ReturnCode.SEEK_NEXT_USING_HINT时调用, 根据当前cell计算下一个需要 * 查询的cell, 若不确定下一个需要查询的cell则返回null. 该函数的使用案例可参考HBase自带的 * ColumnPrefixFilter */ abstract public Cell getNextCellHint(final Cell currentKV) throws IOException; /** * 用于判断当前column family是否需要包含, 若返回true则当前column family会被包含, 否则将被过滤. * 该函数对每个column family均会调用, 该函数的功能与Scan.addFamily(byte[] family)相同, 在需要 * 过滤column family时应当使用Scan.addFamily(byte[] family), 而不使用该函数 * @param name column family的名称 */ abstract public boolean isFamilyEssential(byte[] name) throws IOException; /** * 序列化函数, 在Client调用, 用于将Filter实例序列化为二进制数组, 以进行网络传输 */ abstract public byte[] toByteArray() throws IOException; /** * 反序列化函数, 在RegionServer上调用, 用于将Client传来的二进制数组解析为Filter实例. * 每个自定义过滤器必须重写该函数, 序列化与反序列化操作可借助Protocol Buffers. * @param pbBytes 过滤器实例序列化后的二进制数组 */ public static Filter parseFrom(final byte [] pbBytes) throws DeserializationException { throw new DeserializationException( &quot;parseFrom called on base Filter, but should be called on derived type&quot;); } /** * 用于判断两个过滤器可序列化的部分是否相等, 仅用于测试, 继承FilterBase时可以不实现. */ abstract boolean areSerializedFieldsEqual(Filter other); /** * 由框架调用, 不需要用户调用 */ public void setReversed(boolean reversed) { this.reversed = reversed; } public boolean isReversed() { return this.reversed; }} 通过上述对Filter中函数原型的解释, 可以有以下几点结论: 一般真正包含过滤逻辑的函数只有两个: filterRowKey与filterKeyValue; 如果只需对row key进行自定义的过滤, 那么继承FilterBase, 在filterRowKey中重写row key相关的过滤逻辑即可; 如果只需对column qualifier或value进行过滤(对column family的过滤一般调用Scan类的函数), 那么继承FilterBase, 在filterKeyValue中重写与cell相关的过滤逻辑即可; 如果既需要对row key进行过滤又需要对column qualifier或value进行过滤, 则需要同时重写filterRowKey和filterKeyValue中的过滤逻辑. HBase过滤器执行流程在对HBase过滤器基类Filter中的各个函数有了一定了解之后, 我们再通过一个实际的自定义过滤器类来研究过滤器的执行流程. 自定义过滤器的原型如下, 隐去了部分辅助函数, 完整的代码可参看CustomFilter.java. 这个自定义过滤器用于选取row key前缀与给定前缀相同, 且column family, column qualifier, value与给定值相等的cell.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public class CustomFilter extends FilterBase { private static Logger logger = LoggerFactory.getLogger(CustomFilter.class); private boolean filterRow = true; private boolean passedPrefix = false; private byte[] prefix; private byte[] cf; private byte[] cq; private byte[] value; public CustomFilter(final byte[] prefix, final byte[] cf, final byte[] cq, final byte[] value) { logger.info(&quot;Constructor method is called, row prefix: &quot; + new String(prefix) + &quot;, column family: &quot; + new String(cf) + &quot;, column qualifier: &quot; + new String(cq) + &quot;, value &quot; + new String(value)); this.prefix = prefix; this.cf = cf; this.cq = cq; this.value = value; } @Override public void reset() { logger.info(&quot;reset() method is called&quot;); filterRow = true; } @Override public boolean filterRowKey(byte[] buffer, int offset, int length) { logger.info(&quot;filterRowKey() method is called&quot;); if (buffer == null || this.prefix == null) return true; if (length &lt; prefix.length) return true; // if they are equal, return false =&gt; pass row // else return true, filter row int cmp = Bytes.compareTo(buffer, offset, this.prefix.length, this.prefix, 0, this.prefix.length); // if we passed the prefix, set flag if ((!isReversed() &amp;&amp; cmp &gt; 0) || (isReversed() &amp;&amp; cmp &lt; 0)) { passedPrefix = true; } filterRow = (cmp != 0); return filterRow; } @Override public boolean filterAllRemaining() { logger.info(&quot;filterAllRemaining() method is called, passed prefix: &quot; + passedPrefix); return passedPrefix; } @Override public ReturnCode filterKeyValue(Cell cell) { logger.info(&quot;filterKeyValue() method is called, cell: &quot; + cellToString(cell)); if (filterRow) return ReturnCode.NEXT_ROW; if (!(Bytes.equals(CellUtil.cloneFamily(cell), cf))) { return ReturnCode.NEXT_COL; } if (!(Bytes.equals(CellUtil.cloneQualifier(cell), cq))) { return ReturnCode.NEXT_COL; } if (!(Bytes.equals(CellUtil.cloneValue(cell), value))) { return ReturnCode.NEXT_COL; } return ReturnCode.INCLUDE_AND_NEXT_COL; } @Override public Cell transformCell(Cell cell) { logger.info(&quot;transformCell() method is called, cell: &quot; + cellToString(cell)); return cell; } @Override public void filterRowCells(List&lt;Cell&gt; list) { logger.info(&quot;filterRowCells() method is called, size: &quot; + list.size()); } @Override public boolean hasFilterRow() { logger.info(&quot;hasFilterRow() method is called&quot;); return false; } @Override public boolean filterRow() { logger.info(&quot;filterRow() method is called, filterRow: &quot; + filterRow); return filterRow; } @Override public Cell getNextCellHint(Cell cell) { logger.info(&quot;getNextCellHint() method is called, cell: &quot; + cellToString(cell)); return null; } @Override public boolean isFamilyEssential(byte[] bytes) { logger.info(&quot;isFamilyEssential() method is called, param: &quot; + new String(bytes)); return true; }} 为了研究上述自定义过滤器的执行流程, 我们建立一个简单的HBase表, 表名为filter_test, 包含f1, f2两个column family. 为了便于查看RegionServer的日志, 我们仅向filter_test表中插入两行数据. 建表及插入数据命令如下.123456789create &apos;filter_test&apos;, &apos;f1&apos;, &apos;f2&apos;put &apos;filter_test&apos;, &apos;150501&apos;, &apos;f1:a&apos;, &apos;12&apos;put &apos;filter_test&apos;, &apos;150501&apos;, &apos;f1:b&apos;, &apos;13&apos;put &apos;filter_test&apos;, &apos;150501&apos;, &apos;f1:c&apos;, &apos;13&apos;put &apos;filter_test&apos;, &apos;150501&apos;, &apos;f2:d&apos;, &apos;87&apos;put &apos;filter_test&apos;, &apos;150601&apos;, &apos;f1:a&apos;, &apos;18&apos;put &apos;filter_test&apos;, &apos;150601&apos;, &apos;f1:b&apos;, &apos;19&apos;put &apos;filter_test&apos;, &apos;150601&apos;, &apos;f1:c&apos;, &apos;20&apos;put &apos;filter_test&apos;, &apos;150601&apos;, &apos;f2:d&apos;, &apos;12&apos; 在HBase shell中执行scan &apos;filter_test&apos;可看到以下记录:123456789ROW COLUMN+CELL 150501 column=f1:a, timestamp=1583499565071, value=12 150501 column=f1:b, timestamp=1583499565085, value=13 150501 column=f1:c, timestamp=1583499565089, value=13 150501 column=f2:d, timestamp=1583499565093, value=87 150601 column=f1:a, timestamp=1583499565099, value=18 150601 column=f1:b, timestamp=1583499565104, value=19 150601 column=f1:c, timestamp=1583499565108, value=20 150601 column=f2:d, timestamp=1583499565112, value=12 在完成以上准备工作后, 我们便可以编写客户端代码来调用我们编写的自定义过滤器. 以下是主要的代码, 完整的代码请参考FilterExample.java. 这段代码用于在filter_test表中筛选row key前缀为1506, column family为f1, column qualifier为d, value为12的cell.1234567891011Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Table table = connection.getTable(TableName.valueOf(&quot;filter_test&quot;));Scan scan = new Scan();scan.setFilter(new CustomFilter(&quot;1506&quot;.getBytes(), &quot;f2&quot;.getBytes(), &quot;d&quot;.getBytes(), &quot;12&quot;.getBytes()));ResultScanner scanner = table.getScanner(scan);for (Result result : scanner) { for (Cell cell : result.rawCells()) { System.out.println(cellToString(cell)); }} 上述代码的运行结果如下, 可以看到, 使用过滤器之后过滤操作已经在RegionServer端完成, Client只需接收数据即可.1row key: 150601, family: f2, qualifier: d, value: 12 在CustomFilter中, 我们在每个函数体中都打印了调用日志, 在HBase RegionServer的日志中我们可以看到如下类似的记录. HBase的RegionServer日志在${HBASE_HOME}/logs路径下, 其中${HBASE_HOME}为HBase安装路径. 12342020-03-08 20:59:56,950 INFO [RpcServer.default.FPBQ.Fifo.handler=29,queue=2,port=16201] filter.CustomFilter: parseFrom() method is called2020-03-08 20:59:56,950 INFO [RpcServer.default.FPBQ.Fifo.handler=29,queue=2,port=16201] filter.CustomFilter: Constructor method is called, row prefix: 1506, column family: f2, column qualifier: d, value 122020-03-08 20:59:56,953 INFO [RpcServer.default.FPBQ.Fifo.handler=29,queue=2,port=16201] filter.CustomFilter: isFamilyEssential() method is called, param: f1... 下图是根据RegionServer日志整理的CustomFilter中各个函数的执行流程. 从图中我们可以观察到以下几点: 在RegionServer上, 首先会调用parseFrom()函数将Client传来的过滤器二进制数组解析为过滤器实例; isFamilyEssential()函数在过滤器构造完成后即调用, 针对表中的每个column family均会调用一次; 在对每一行的过滤中, 主要的过滤逻辑由filterRowKey()和filterKeyValue()两个函数完成, 对于经过filterKeyValue()过滤而决定保留的cell, 都会调用transformCell()函数; 在每行过滤结束时均会调用reset()函数. 总 结本文在详细讲述HBase过滤器源码中各个函数作用的基础上, 结合一个具体的自定义过滤器案例分析了各个函数的执行流程. 化繁就简, 我们可以得出以下几点在过滤器实践中的经验： 自定义过滤器首先考虑继承FilterBase类; 相关的过滤逻辑主要在filterRowKey()和filterKeyValue()函数中, 可根据过滤器功能选择需要实现的函数; 在调试时, 可借助RegionServer的日志.","link":"/hbase-filter.html"},{"title":"2021年大数据/数据库开发校招总结","text":"我在今年(2021年)参加了大数据和数据库开发相关的校招岗位招聘, 整个应聘流程包括春招的实习生招聘以及秋招的正式招聘, 最终获得了阿里/腾讯/Shopee/网易/华为/有赞等多家公司的实习或正式offer, 当然也收到过一家公司的拒信. 虽然网络上分享计算机相关岗位面试经验的文章已经浩如烟海, 但是这些经验大多针对后端开发和算法, 鲜有介绍大数据或数据库开发的应聘经验. 然而大数据或数据库开发虽也属于系统开发, 但是在技术栈上与后端开发还是有很大不同, 一般公司也会设立单独的岗位. 这也催生了我写这篇博文的想法, 本文主要分享以下内容: 大数据或数据库开发的工作内容究竟是什么? 大数据或数据库开发需要学习哪些内容? 面试会涉及哪些内容? 我的一些大数据或数据库开发面试经历. 本文的目的是希望帮助对数据工程研发感兴趣的同学明确应聘岗位, 理清学习路线, 并分享我自己的面试经历以供参考. 本文不会介绍具体的面试技巧, 比如如何进行自我介绍, 如何介绍项目或论文以及如何进行白板编程. 这些内容在”剑指Offer”或参考[1]中都有不少描述. 在正式开始之前, 还是需要进行免责申明, 由于本人水平有限, 很多观点局限于我的个人经历, 难免会有不当或错误, 希望见谅, 也欢迎指正. 岗位介绍在前文中我并没有严格区分大数据和数据库开发, 虽然二者都是进行与数据存储和计算相关的系统开发, 从一定程度上来说都可以归为数据工程研发的大类, 但是就目前来看二者还是对应了不同的岗位, 开发内容也有所区别. 为方便描述, 在下文中我将使用数据工程研发代指各类与大数据组件或平台相关的开发, 而用数据库开发代指数据库内核开发. 尽管数据工程研发在一定程度上包含了数据库开发, 但是就目前来说数据库开发相对而言是更偏基础且独立的岗位. 首先看数据工程研发, 在公司的招聘网站上对应的岗位一般是大数据开发工程师/大数据平台开发工程师/数据工程师等. 这里需要注意的是某些公司可能还有类似数据研发的岗位, 一般来说数据研发和数据工程研发是有一定区别的, 数据工程研发关注的是底层数据存储和处理引擎或数据平台的研发, 属于系统研发的范畴, 而数据研发更多的是指依据业务进行数据建模, 或提供数据分析支持. 数据研发需要借助数据工程研发形成的数据平台. 当然有些公司并没有分的这么详细, 可能数据研发也包括了一些平台类的建设工作, 具体还是要仔细阅读职位描述. 以下是我从字节跳动和Shopee校招官网摘录的2021年校招数据工程研发相关职位的职位描述. 2021年字节跳动数据工程研发校招职位描述 2021年Shopee数据工程研发校招职位描述 从中可以看出, 数据工程研发大致包括以下工作内容: 最底层的是各类开源大数据组件的开发和维护, 这类工作需要对某个开源组件有源码级的深入了解, 难度较大; 底层引擎之上就是公司内部数据平台的搭建, 比如统一的SQL查询平台, 机器学习平台等; 再往上就是基于现有的引擎和平台进行业务相关的数据链路开发, 这涉及到链路的架构设计以及基础组件的技术选型等; 最后就是数据仓库的设计与开发, 涉及数据建模和相关工具的开发. 再来看数据库开发. 一般所说的数据库开发都是指数据库内核开发, 数据库内核是一个非常复杂而庞大的系统, 专业程度极高, 不过对于校招而言一般不会要求有相关的开发经验. 但是从各个公司的职位介绍和面试情况来看, 对这一岗位的要求是比较高的. 这类岗位一般使用C/C++/Go语言进行开发, 需要对底层的操作系统, 计算机网络, 编译原理以及分布式理论有较深入的理解. 以下是我从智庾科技和PingCAP公司校招官网摘录的2021年数据库研发相关的职位的职位描述. 这两家公司目前在数据库领域都有出色的产品, 智庾科技的时序数据库DolphinDB在DB-Engins上也有不错的排名, PingCAP的TiDB是开源领域优秀的NewSQL和HTAP数据库. 2021年DolphinDB数据库开发校招职位描述 2021年PingCAP数据库开发校招职位描述 面试内容技术面试是十分具有技巧性的, 在开始之前进行一些训练能够在面试中更加游刃有余. 不过本文并不打算讲述面试技巧, 因为技巧虽然有用, 但是最重要的还是实力. 如果我们对面试岗位所需的技术有深入的了解, 那么配以一些最基本的经验, 我们基本就能通过面试. 本文主要讲述面试的内容, 我认为掌握这些内容是面试通过的关键. 这里将面试内容分为两部分: 一是计算机基础, 这无论对开发岗还是算法岗都是通用的; 二是数据系统的相关理论和技术, 这主要针对数据工程研发和数据库开发. 这里所述的内容基本上包含了数据工程研发所需的全部知识, 在相关内容之后我会说明在面试中的出现频率. 不过学习的目的从来都不是通过面试, 而是真正的掌握这些内容, 可以使自己在未来的职业生涯中有更好的发展. 计算机基础关于计算机基础, 我习惯将其分为两大部分内容. 一部分是编码相关的内容, 这部分直接与日常的编码相关, 包括: 编程语言, 是编写代码的基本工具, 至少需要熟悉一到两门编程语言, 在面试中可能会考察一些编程语言的基础内容, 包括语法, 关键字, 标准库等. 对于数据工程研发可能会涉及SQL语言. 数据结构与算法, 这是编写高效代码的关键, 也是面试考察的重中之重, 一般技术岗面试每轮都会考察1-2题白板编程题. 设计模式, 如果说利用数据结构和算法可以编写更高效的代码让计算机运行的更快, 那么利用设计模式就可以编写更优雅和易于维护的代码让程序员更好的理解和维护. 前者的高效是对机器而言, 后者的高效是对人而言. 另一部分是与计算机原理相关的, 这部分看似与编码不直接相关, 但却是写出高效代码的关键, 特别是对于系统开发而言, 了解计算机原理是编写正确代码的关键. 这一部分包括: 计算机组成原理 操作系统, 了解操作系统原理, 对于并发编程有重要作用. 计算机网络 编译原理, 数据库或大数据框架的SQL解析背后都涉及到编译原理的知识. 数据库, 对于数据工程或数据库研发这是必须具备的知识, 本文会在数据系统中更具体的讲述. 在面试中, 上述各个部分的出现频率的排序大概是: 数据结构与算法&gt;操作系统&gt;计算机网络&gt;=数据库&gt;设计模式. 其中计算机组成原理和编译原理一般不会出现, 如果准备时间紧迫可以按面试出现频率准备, 如果时间充分的话可以进行全面的复习总结. 数据系统数据系统这个词实际上是笔者总结的, 并没有官方的定义. 目前与数据存储和计算相关的系统有两大类: 一类是业界常说的大数据系统, 这类系统通常一个组件负责单一的存储或计算功能, 要实现一个完整的数据分析平台需要组合多个组件. 比如一种常见的技术架构是采用Sqoop将线上关系型数据库的数据导入到HDFS, 然后使用Hive或Spark进行分析. 另一类是数据库系统, 分为OLTP和OLAP, OLTP专注于线上业务, OLAP专注于分析业务. OLAP数据库系统集成了大数据存储和计算功能. 不管是大数据系统还是数据库系统, 其本质上都是用于数据存储和计算的数据系统, 当然随着数据量的不断增大, 目前这些系统多数都是分布式数据系统. 实际上, 在数据系统的最早期, 数据量较小, 因此一个单机的数据库系统就可以承担所有的在线业务和分析业务, 比如在百万行数据时, 一个MySQL实例即可承担所有的数据存储和分析任务. 但是随着数据量的增大, 这时候又没有一个可以进行大规模数据分析任务的数据库系统, 因此就衍生了一系列大数据系统, 它们有的负责数据的采集传输, 有的负责数据的存储, 有的负责数据的计算. 但是随着分布式技术的发展, 大家发现这些大数据系统种类繁多部署复杂, 所以又开始回归数据库系统, 现在越来越多的分布式OLAP数据库支持大数据的存储和计算服务. 笔者也认为未来的数据系统会越来越向数据库系统方向发展, HTAP会成为一个重要的发展方向. 当然, 不论是大数据系统和数据库系统, 其本质上都是分布式数据系统, 它们都有分布式系统的特点, 同时用于解决存储和计算问题. 以下是对常用大数据或数据库系统的总结. 学习路线如果在Google上搜索”大数据学习路线”, 出现的内容浩如烟海. 不过这些内容大多有一个共同的特征, 那就是无休止的罗列相关大数据相关框架的教学视频和学习资料. 这些资料确实有一定的参考价值, 但也仅限于让我们了解当前主流的大数据框架以及在需要具体学习某个框架时提供一些入门的资料. 事实上, 这些学习路线太过于大而全了, 对于校招而言, 公司一般不会要求熟练使用所有的框架, 而是希望面试者精通其中的某个或少数几个框架, 掌握其背后的原理, 并能举一反三了解一类框架设计的初衷和所能解决的本质问题. 笔者认为正确的学习过程应该是这样的: 建立全局意识, 首先了解目前的大数据技术体系, 了解哪些框架解决的是同一类问题, 这时可以浏览一下网上现有的大数据学习路线, 尚硅谷的2021年度全网最全大数据学习路线是一个相对不错的参考, 我在上一节中总结的数据系统结构图也是个不错的参考; 钻进去, 了解细节. 这时候需要选择一个具体的框架进行学习, 如果平时有项目可选择与项目相关的框架, 如果没有项目可选择热门的框架进行学习. 如计算框架Spark, Flink或存储框架HBase, 一般某种类型选择一个框架学习即可. 返璞归真, 重回本质. 在了解了几类具体框架的使用或实现原理之后, 应当回过头来看看这些框架背后的本质是什么, 以及为什么需要这么实现. 对于数据工程研发, 笔者认为比较合适的学习路线应该是这样的: 学习关系型数据库的使用和原理, 选择一个数据库进行学习, 如MySQL. 关系型数据库是数据存储与处理技术的起源, Spark, Flink等大数据处理框架也越来越推崇SQL化, 我认为学习大数据系统关系型数据库的知识越多越好, 这方面可参考: 数据库系统内幕, 高性能MySQL, 数据库内核月报 学习Hadoop框架, 尽管现在Hadoop在生产环境中使用越来越少, 但是作为大数据系统的开端还是有必要学习. 重点是掌握HDFS, YARN和MapReduce背后的设计哲学, 尽可能阅读相关论文. 这方面可参考: 尚硅谷大数据Hadoop 3.x, Hadoop权威指南 学习一个计算框架, 目前Flink有一统批流计算的趋势, 如果非项目需要可学习Flink, 如果项目主要是批处理可选择Spark. 同样, 尽可能阅读相关论文. 这方面可参考: Spark权威指南, Flink学习指南 学习一个分布式数据库, 目前分布式数据库种类繁多, 有OLTP也有OLAP, 可先选择一个进行深入学习, Hadoop生态中推荐HBase. 这方面可参考: HBase原理与实践 学习分布式系统的相关理论. 目前的大数据系统本质上都是分布式数据系统, 要真正理解这些系统的运行原理, 分布式系统的理论是必不可少的. 这方面可参考: MIT 6.824, 深入理解分布式事务 有了上面这些前置知识之后, 基本上对大数据框架的类别和实现原理做到心中有数了. 这时候可以进一步将知识体系化, 这方面可参考: 数据密集型应用系统设计 关于数据库开发的学习路线可参考Database Development Learning Map. 应聘经历我的整个校招持续了接近一年的时间(从2020年12月到2021年9月), 包括春招实习生招聘和秋招正式招聘, 以及中间接近三个月的实习经历. 春招的面试经历及之后的实习经历对秋招有重要的作用, 所以对于找工作的同学来说, 因当从春招找实习开始就有充分的准备. 上图概括了一般互联网公司的整个应届生招聘时间线, 跨度接近10个月. 需要说明的是, 这里的时间线是对于互联网公司而言的. 互联网公司春招一般不招聘应届生, 只招聘实习生. 一般来说秋招的正式offer有一部分会发给实习生. 其他行业春招秋招可能都会招聘应届生. 对于春招拿到offer的同学来说, 一般可以在5-8月份选择3个月左右的时间进行实习. 公司一般欢迎尽早实习尽可能长的时间, 对于个人来说有机会也应当尽可能早的出去实习. 一方面可以积累工作经验, 另一方面实习很有可能转正, 这也是一个提前熟悉工作环境的机会. 在实习的过程中, 一些公司的秋招提前批就会陆续开放简历投递. 最早的在7月初就会开放, 多数应该在7月底8月初开放. 对于实习开始比较晚的同学, 这时候实习可能刚刚开始, 相对来说就比较难以权衡, 究竟是认真完成实习工作争取更多产出, 在转正中拿到不错的评价; 还是选择多面几家公司选择更合适自己的. 对于实习开始早的同学, 这时候一般实习已经接近尾声, 可以选择适当投递几家合适的公司进行面试. 在9月份, 秋招会迎来最后一波高潮, 一般这时候实习已经结束了, 大部分公司的秋招都还在进行中, 如果目前还没有十分满意的offer, 可以选在在9月份进行最后的冲刺. 虽说是金九银十, 但实际上9月过后, 一般之后极少数的互联网公司还会进行面试. 10月中下旬, 大多数的公司会发放谈薪并发放正式的offer. 在11月份可能还有少数公司会进行一轮的秋招补招. 这里我也分享一下我的秋招面试经历, 由于我的Base地已经确定是杭州, 因此投递的大多是杭州的岗位. [网易雷火-杭州] 大数据平台开发工程师 [网易-杭州研究院] 大数据开发工程师 [华为-杭州] 数据库开发工程师 [Shopee-深圳] 大数据开发工程师 [有赞-杭州] 大数据开发工程师 如何看待面试实际上, 在面试开始之前, 我们需要认识到面试的本质, 摆正心态, 才能真正从面试中获得我们所需要的东西. 关于如何看待面试, 我想分享以下两点. 首先, 面试是一个双向选择的过程. 面试的过程不仅是让面试官了解你, 也是你了解面试官以及工作岗位的过程. 虽然在投递简历时一般会有简单的职位描述, 但是从面试官口中可以了解更加具体的工作内容, 以及面试官本人的工作感受, 这都可以作为评判面试岗位的进一步参考. 另外, 面试官一般就是入职之后的同事或leader, 通过在面试中的沟通顺畅程度, 你可以判断能否与将来的同事和谐共处. 通常面试的最后环节, 面试官都会给机会让你反问几个问题, 这是一个需要特别把握的机会, 特别是对于秋招而言, 在正式入职之前对于工作岗位的了解最多的就是来自于面试官. 我一般会咨询一下部门的组织架构和人员组成情况, 部门的主要工作内容和支撑业务, 以及面试官本人的工作感受和挑战. 其次, 面试不要害怕失败, 事后要及时复盘. 面试失败是常有的事, 通常每轮面试只会进行一个小时的时间, 在这一个小时内是很难完全展现自己的能力的. 当遇到一些没有经验的面试官时, 面试者只能按照既定的套路回答问题, 这其实存在很大的偶然性和运气. 当然, 也有一些面试官经验丰富, 可以从谈话中找到你感兴趣和擅长的点并不断深入. 所以即使面试失败也不要自我否定, 不论成功还是失败, 我们要做的都是及时复盘. 如果有条件的话可以将面试过程录音, 在事后复盘时要重点挖掘技术上薄弱的部分, 分析成功或者失败的原因, 这样才能不断的从实际面试中积累经验, 在未来更加从容的面对面试. 小结本文首先对大数据和数据库开发的岗位进行了详细的介绍, 希望可以帮助相关方向的同学明确职业目标. 然后介绍了技术面试的基本内容以及数据工程研发的学习路线, 希望可以帮助希望从事数据研发的同学厘清学习内容. 最后分享了我的应聘经历, 可以帮助读者更好的把握应聘的时间节点, 提前做好准备. 其实数据工程研发本质上是与数据存储和计算相关的系统研发, 尽管目前多数框架给我们提供了完善的API, 但是掌握计算机的基础理论知识仍旧可以使我们不断收益. 在学习过程中也切勿忘记探究背后的本源, 框架总有过时的时候, 那些设计思想却在不断延续, 掌握背后的本质才能帮助我们走的更远. 参考[1] 写在20年初的校招面试心得与自学CS经验及找工作分享","link":"/2021-interview.html"},{"title":"Automatic Differentiation Based on Computation Graph","text":"Automatic differentiation (AD), also called algorithmic differentiation or simply “autodiff” is one of the basic algorithms hidden behind the deep learning framework such as tensorflow, pytorch, mxnet, etc. It’s AD technique that allows us to focus on the design of the model structure without paying much attention to the gradient calculations during model training. However, this blog post will focus on the principle and implementation of AD. Finally, we will implement an AD framework based on computational graphs and use it for logistic regression. You could find all the code here. Overview of ADMethods for the computation of derivatives in computer programs can be classified into four categories[2]: Manually working out derivatives and coding them; Numerical differentiation using finite difference approximations; Symbolic differentiation using expression manipulation in computer algebra systems such as Mathematica, Maxima, and Maple; Automatic differentiation, also called algorithmic differentiation. Here, I will give a simple example to illustrate the difference between the first three methods of derivation. As for automatic differentiation, the details will be described in later sections. Suppose we need to calculate the gradient of $x=1$ for the function $ f(x)=x(1-2x)^2 $. The calculation process of different methods is as follows: Method 1：$$ f(x)=x-4x^2+4x^3 $$$$ f’(x)=1-8x+12x^2 $$$$ f’(1)=5 $$ Method 2 ($ h $ can be any other minimum value):$$ f’(x) \\approx \\frac{f(x-h)+f(x+h)}{2h}=\\frac{f(1-0.00001)+f(1+0.00001)}{2*0.00001}=5.000003999999669 $$ Method 3:$$ f’(x)=(1-2x)^2 -4x(1-2x) $$$$ f’(1)=5 $$ From this simple example we can see the shortcomings of the first three methods. Manual differentiation is time consuming and prone to error. Numerical differentiation is simple to implement but can be highly inaccurate and calculation consuming. Symbolic differentiation addresses the weaknesses of both the manual and numerical methods, but often results in complex and cryptic expressions plagued with the problem of “expression swell”. Although AD provide numerical values of derivatives and and it does so by using symbolic rules of differentiation, AD is either a type of numerical or symbolic differentiation. In order to implement the AD, we usually need to build a computation graph. Below is the computation graph of the example $ f(x_1, x_2)=log(x_1) + x_1x_2-sin(x_2) $. Based on computation graph, there are two methods to implement AD: forward mode and reverse mode. AD in forward mode is the conceptually most simple type. For writing convenience, let $ \\dot{v_i}=\\frac{\\partial v_i}{\\partial x_1} $. So we can calculate the gradients in forward mode.     Forward Primal Trace        Forward Derivative Trace     $$ v_0=x_1=2 $$ $$ v_1=x_2=5 $$ $$ \\dot{v_0}=\\dot{x_1}=1 $$ $$ \\dot{v_1}=\\dot{x_2}=0 $$ $$ v_2=log(v_0)=log2 $$ $$ v_3=v_0v_1=10 $$ $$ v_4=sin(v1)=sin5 $$ $$ v_5=v_2+v_3=0.693+10 $$ $$ v_6=-v_4=-sin5 $$ $$ v_7=v_5+v_6=10.693+0.959 $$ $$ \\dot{v_2}=\\dot{v_0}/v_0=1/2 $$ $$ \\dot{v_3}=\\dot{v_0}v_1 + v_0\\dot{v_1}=5 $$ $$ \\dot{v_4}=cos(v_1)\\dot{v_1}=0 $$ $$ \\dot{v_5}=\\dot{v_2} + \\dot{v_3}=5.5 $$ $$ \\dot{v_6}=-\\dot{v_4}=0 $$ $$ \\dot{v_7}=\\dot{v_5} + \\dot{v_6}=5.5 $$ $$ y=v_7=11.652 $$ $$ \\dot{y}=\\dot{v_7}=5.5 $$ Reverse mode is similar to forward mode, except that the gradient needs to be calculated backwards. Let $\\bar{v_i}=\\frac{\\partial y}{\\partial v_i}$. We can calculate the gradients in reverse mode.     Forward Primal Trace        Reverse Derivative Trace     $$ v_0=x_1=2 $$ $$ v_1=x_2=5 $$ $$ \\bar{v_0}=\\bar{v_0}^{(1)} + \\bar{v_0}^{(2)}=5.5 $$ $$ \\bar{v_0}^{(1)}=\\bar{v_2}\\frac{\\partial v_2}{\\partial v_0}=\\frac{1}{2} $$ $$ \\bar{v_0}^{(2)}=\\bar{v_3}\\frac{\\partial v_3}{\\partial v_=}=5 $$ $$ \\bar{v_1}=\\bar{v_4}\\frac{\\partial v_4}{\\partial v_1}=-cos5 $$ $$ v_2=log(v_0)=log2 $$ $$ v_3=v_0v_1=10 $$ $$ v_4=sin(v1)=sin5 $$ $$ v_5=v_2+v_3=0.693+10 $$ $$ v_6=-v_4=-sin5 $$ $$ v_7=v_5+v_6=10.693+0.959 $$ $$ \\bar{v_2}=\\bar{v_5}\\frac{\\partial v_5}{\\partial v_2}=1 $$ $$ \\bar{v_3}=\\bar{v_5}\\frac{\\partial v_5}{\\partial v_3}=1 $$ $$ \\bar{v_4}=\\bar{v_6}\\frac{\\partial v_6}{\\partial v_4}=-1 $$ $$ \\bar{v_5}=\\bar{v_7}\\frac{\\partial v_7}{\\partial v_5}=1 $$ $$ \\bar{v_6}=\\bar{v_7}\\frac{\\partial v_7}{\\partial v_6}=1 $$ $$ \\bar{v_7}=\\frac{\\partial v_7}{\\partial v_7}=1 $$ $$ y=v_7=11.652 $$ $$ \\bar{y}=\\bar{v_7}=1 $$ The above two tables demonstrate AD based on forward mode and reverse mode, respectively. Only the gradient for $ x_1 $ is calculated in the tables, and the gradient calculation for $ x_2 $ is similar, I don’t want to repeat it. AD algorithmThe idea of the reverse mode is closer to backpropagation and is easier to program. Therefore, in practice we usually use reverse mode to implemente AD. The pseudo-code of AD based on the reverse mode is as follows:1234567891011def gradient(output_node): node_to_grad = {} node_to_grad[output_node] = 1 # Get the reverse order topological arrangement of the nodes in computation graph reverse_topo_order = reversed(find_topo_sort(output_node)) for node in reverse_topo_order: grad &lt;-- sum partial adjoints from output edges of node # calculate the gradient of the inputs input_grads &lt;-- node.op.gradient(node, grad) add input_grads to node_to_grad return node_to_grad To better understand this algorithm, let’s look at a concrete example (You can find the implementation of this example in the test_exp function of the autodiff_test.py file). As shown in the computation graph above, the execution flow after calling function $ gradient(x_4) $ is as follows: Changes in node_to_grad during execution (assume $ x_1=2 $): $ x_4: \\bar{x_4}=1 $; $ x_3: \\bar{x_3}=\\bar{x_4}x_2=e^2 $; $ x_2: \\bar{x_2}^{(1)}=\\bar{x_4}x_3=e^2+1 $; $ x_2: \\bar{x_2}^{(2)}=\\bar{x_3}=e^2 $; $ x_1: \\bar{x_1}=\\bar{x_2}x_2=(\\bar{x_2}^{(1)} + \\bar{x_2}^{(2)})x_2=e^2(2e^2+1) $. ImplementationIn order to implement AD, we first need to build a computation graph which is composed of nodes. Each node has its inputs and operation (OP). The inputs records the node or constant that entered the current node, and there may be one or more. The OP records the type of operation of the current node on the input nodes, which may be addition, subtraction, multiplication, division, or any custom mathematical operation. Below is the Python code for node.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Node(object): &quot;&quot;&quot;Node in a computation graph.&quot;&quot;&quot; def __init__(self): &quot;&quot;&quot;Constructor, new node is indirectly created by Op object __call__ method.&quot;&quot;&quot; self.inputs = [] self.op = None self.const_attr = None self.name = &quot;&quot; def __add__(self, other): &quot;&quot;&quot;Adding two nodes return a new node.&quot;&quot;&quot; if isinstance(other, Node): new_node = add_op(self, other) else: # Add by a constant stores the constant in the new node&apos;s const_attr field. # &apos;other&apos; argument is a constant new_node = add_byconst_op(self, other) return new_node def __sub__(self, other): &quot;&quot;&quot;subtracting two nodes return a new node.&quot;&quot;&quot; if isinstance(other, Node): new_node = add_op(self, -1 * other) else: new_node = add_byconst_op(self, -1 * other) return new_node def __rsub__(self, other): &quot;&quot;&quot;allow left-hand-side subtract&quot;&quot;&quot; return -1 * self.__sub__(other) def __mul__(self, other): &quot;&quot;&quot;Multiplying to nodes return a new node.&quot;&quot;&quot; if isinstance(other, Node): new_node = mul_op(self, other) else: new_node = mul_byconst_op(self, other) return new_node # Allow left-hand-side add and multiply. __radd__ = __add__ __rmul__ = __mul__ def __str__(self): &quot;&quot;&quot;Allow print to display node name.&quot;&quot;&quot; return self.name __repr__ = __str__ Each node has an OP that represents the mathematical operations that need to be performed. All OPs have a common base class. Its class definition is as follows:123456789101112131415class Op(object): &quot;&quot;&quot;Op represents operations performed on nodes.&quot;&quot;&quot; def __call__(self): &quot;&quot;&quot;Create a new node and associate the op object with the node.&quot;&quot;&quot; new_node = Node() new_node.op = self return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of input nodes, compute the output value.&quot;&quot;&quot; raise NotImplementedError def gradient(self, node, output_grad): &quot;&quot;&quot;Given value of output gradient, compute gradient contributions to each input node.&quot;&quot;&quot; raise NotImplementedError In order to implement the corresponding mathematical operations on the node, it is only necessary to inherit the class Op and implement the function compute and gradient. In order to better explain the writing of OPs, I will write an example of the addition OP. The rest of the popular OPs will not be listed here, you can see the detailed code here. For the addition OP, there are two cases, one is to add two nodes, and the other is to add a node to a constant. Although there is only a slight difference between the two cases, we still need to treat them differently. Addition OP of two nodes is shown below:12345678910111213141516171819class AddOp(Op): &quot;&quot;&quot;Op to element-wise add two nodes.&quot;&quot;&quot; def __call__(self, node_A, node_B, name=None): new_node = Op.__call__(self) new_node.inputs = [node_A, node_B] if name is not None: new_node.name = name else: new_node.name = &quot;(%s+%s)&quot; % (node_A.name, node_B.name) return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of two input nodes, return result of element-wise addition.&quot;&quot;&quot; assert len(input_vals) == 2 return input_vals[0] + input_vals[1] def gradient(self, node, output_grad): &quot;&quot;&quot;Given gradient of add node, return gradient contributions to each input.&quot;&quot;&quot; return [output_grad, output_grad] Addition OP of a node and a constent is shown below:1234567891011121314151617181920class AddByConstOp(Op): &quot;&quot;&quot;Op to element-wise add a nodes by a constant.&quot;&quot;&quot; def __call__(self, node_A, const_val, name=None): new_node = Op.__call__(self) new_node.const_attr = const_val new_node.inputs = [node_A] if name is not None: new_node.name = name else: new_node.name = &quot;(%s+%s)&quot; % (node_A.name, str(const_val)) return new_node def compute(self, node, input_vals): &quot;&quot;&quot;Given values of input node, return result of element-wise addition.&quot;&quot;&quot; assert len(input_vals) == 1 return input_vals[0] + node.const_attr def gradient(self, node, output_grad): &quot;&quot;&quot;Given gradient of add node, return gradient contribution to input.&quot;&quot;&quot; return [output_grad] You can extend the OPs by mimicking the addition OP. In the next section, I will complete the OPs that logistic regression needs. And using the automatic differential logistic regression model for handwritten digit recognition. After building the computation graph, we need to calculate it. The calculation of the computation graph includes the forward propagation calculation and the backward propagation (gradient) calculation. The topological ordering of the computational graph is required for any calculation. Here we use a simple post-order DFS algorithm to get the topological order.123456789101112131415161718192021222324def find_topo_sort(node_list): &quot;&quot;&quot;Given a list of nodes, return a topological sort list of nodes ending in them. A simple algorithm is to do a post-order DFS traversal on the given nodes, going backwards based on input edges. Since a node is added to the ordering after all its predecessors are traversed due to post-order DFS, we get a topological sort. &quot;&quot;&quot; visited = set() topo_order = [] for node in node_list: topo_sort_dfs(node, visited, topo_order) return topo_orderdef topo_sort_dfs(node, visited, topo_order): &quot;&quot;&quot;Post-order DFS&quot;&quot;&quot; if node in visited: return visited.add(node) for n in node.inputs: topo_sort_dfs(n, visited, topo_order) topo_order.append(node) After the topological sorting of the computation graph is obtained, it can be calculated. The forward propagation calculation is wrapped in the class Executor, the code is as follows:1234567891011121314151617181920class Executor: &quot;&quot;&quot;Executor computes values for a given subset of nodes in a computation graph.&quot;&quot;&quot; def __init__(self, eval_node_list): self.eval_node_list = eval_node_list def run(self, feed_dict): &quot;&quot;&quot;Computes values of nodes in eval_node_list given computation graph.&quot;&quot;&quot; node_to_val_map = dict(feed_dict) # Traverse graph in topological sort order and compute values for all nodes. topo_order = find_topo_sort(self.eval_node_list) # calculated all the nodes in the computation graph. for node in topo_order: inputs = [node_to_val_map[i] for i in node.inputs] if inputs: node_to_val_map[node] = node.op.compute(node, inputs) # Collect node values. node_val_results = [node_to_val_map[node] for node in self.eval_node_list] return node_val_results The specific algorithm for backpropagation has already been mentioned before. The corresponding Python code is as follows.12345678910111213141516171819202122232425def gradients(output_node, node_list): &quot;&quot;&quot;Take gradient of output node with respect to each node in node_list.&quot;&quot;&quot; # a map from node to a list of gradient contributions from each output node node_to_output_grads_list = {} # Special note on initializing gradient of output_node as oneslike_op(output_node): # We are really taking a derivative of the scalar reduce_sum(output_node) # instead of the vector output_node. But this is the common case for loss function. node_to_output_grads_list[output_node] = [oneslike_op(output_node)] # a map from node to the gradient of that node node_to_output_grad = {} # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt. reverse_topo_order = reversed(find_topo_sort([output_node])) for node in reverse_topo_order: output_grad = sum_node_list(node_to_output_grads_list[node]) node_to_output_grad[node] = output_grad input_grads_list = node.op.gradient(node, output_grad) for i in range(len(node.inputs)): if node.inputs[i] not in node_to_output_grads_list: node_to_output_grads_list[node.inputs[i]] = [] node_to_output_grads_list[node.inputs[i]].append(input_grads_list[i]) # Collect results for gradients requested. grad_node_list = [node_to_output_grad[node] for node in node_list] return grad_node_list If you need the full code, go here. Logistic RegressionBased on the AD framework we have built, we only need to add some appropriate OPs to complete the LR model. If you don’t know much about the principles of LR, please refer to my blog post. This blog will not describe the details of LR. In fact, in order to implement LR, we only need to add two OPs, SigmoidOp and SigmoidCrossEntropyOp, on the existing basis.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def sigmoid_fun(x): return 1 / (1 + np.exp(-x))class SigmoidOp(Op): def __call__(self, node_A, name=None): new_node = Op.__call__(self) new_node.inputs = [node_A] if name is not None: new_node.name = name else: new_node.name = &quot;SigmoidOp(%s)&quot; % node_A.name return new_node def compute(self, node, input_vals): assert len(input_vals) == 1 return sigmoid_fun(input_vals[0]) def gradient(self, node, output_grad): # Do not directly use SigmoidOp, use SigmoidCrossEntropyOp instead. raise NotImplementedErrorclass SigmoidCrossEntropyOp(Op): def __call__(self, node_A, node_B, name=None): new_node = Op.__call__(self) new_node.inputs = [node_A, node_B] if name is not None: new_node.name = name else: new_node.name = &quot;SigmoidCrossEntropyOp(%s, %s)&quot; % (node_A.name, node_B.name) return new_node def compute(self, node, input_vals): assert len(input_vals) == 2 z = input_vals[0] y = input_vals[1] m, _ = z.shape loss = np.sum(y * np.log(sigmoid_fun(z)) + (1 - y) * np.log(1 - sigmoid_fun(z))) / m return np.array(loss) def gradient(self, node, output_grad): z = node.inputs[0] y = node.inputs[1] grad_A = (sigmoid_op(z) - y) * output_grad grad_B = zeroslike_op(node.inputs[1]) return [grad_A, grad_B] Then we can built an LR model to solve the handwritten digit recognition problem. Sine LR is an binary classification algrithm, we only select the number 0, 1 in mnist.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768def mnist_lr(num_epochs=10, print_loss_val_each_epoch=False): print(&quot;Build logistic regression model...&quot;) W = ad.Variable(name=&quot;W&quot;) b = ad.Variable(name=&quot;b&quot;) X = ad.Variable(name=&quot;X&quot;) y = ad.Variable(name=&quot;y&quot;) z = ad.matmul_op(X, W) + b y_hat = ad.sigmoid_op(z) loss = ad.sigmoidcrossentropy_op(z, y) grad_W, grad_b = ad.gradients(loss, [W, b]) executor = ad.Executor([loss, grad_W, grad_b, y_hat]) # Read input data train_X, train_Y, test_X, test_Y = load_mnist_for_lr() print(&quot;train set num: %d&quot; % train_X.shape[0]) print(&quot;test set num: %d&quot; % test_X.shape[0]) # Set up minibatch batch_size = 1000 n_train_batches = train_X.shape[0] // batch_size n_test_batches = test_X.shape[0] // batch_size print(&quot;Start training loop...&quot;) # Initialize parameters W_val = np.zeros((784, 10)) b_val = np.zeros((10)) X_val = np.empty(shape=(batch_size, 784), dtype=np.float32) y_val = np.empty(shape=(batch_size, 1), dtype=np.float32) test_X_val = np.empty(shape=(batch_size, 784), dtype=np.float32) test_y_val = np.empty(shape=(batch_size, 1), dtype=np.float32) lr = 1e-3 for i in range(num_epochs): print(&quot;epoch %d&quot; % i) for minibatch_index in range(n_train_batches): minibatch_start = minibatch_index * batch_size minibatch_end = (minibatch_index + 1) * batch_size X_val[:] = train_X[minibatch_start:minibatch_end] y_val[:] = train_Y[minibatch_start:minibatch_end] loss_val, grad_W_val, grad_b_val, _ = executor.run( feed_dict={X: X_val, y: y_val, W: W_val, b: b_val}) # SGD update W_val = W_val - lr * grad_W_val b_val = b_val - lr * grad_b_val if print_loss_val_each_epoch: print(loss_val) correct_predictions = [] for minibatch_index in range(n_test_batches): minibatch_start = minibatch_index * batch_size minibatch_end = (minibatch_index + 1) * batch_size test_X_val[:] = test_X[minibatch_start:minibatch_end] test_y_val[:] = test_Y[minibatch_start:minibatch_end] _, _, _, test_y_predicted = executor.run( feed_dict={ X: test_X_val, y: test_y_val, W: W_val, b: b_val}) correct_prediction = (test_y_predicted &gt;= 0.5).astype(np.int) == test_y_val correct_predictions.extend(correct_prediction) accuracy = np.mean(correct_predictions) print(&quot;test set accuracy=%f&quot; % accuracy) After 10 epochs, we get 100% accuracy on the test set. ConclusionIn this blog post, we detail the principles of AD and give specific derivation cases. On the basis of understanding the mathematical principles of AD, we used numpy to construct a simple calculation graph and implemented some basic OPs. Finally, we constructed the LR model on the calculation graph and used it for handwritten digit recognition. So far, we have actually understood the basic principle behind the deep learning framework-AD based on computational graphs. However, we still have a long way to go before the real deep learning framework. Our numpy-based implementation is undoubtedly inefficient, and the real deep learning framework uses a variety of hardware acceleration. GPU acceleration is very common in deep learning frameworks. If you want to learn about GPU-based acceleration technology, you can refer to Tinyflow. Reference[1] CSE 599W: System for ML[2] Automatic Differentiation in Machine Learning: a Survey","link":"/automatic-differentiation.html"},{"title":"Tinyflow - A Simple Neural Network Framework","text":"In recent years, thanks to the rapid growth of computing power, deep learning has blossomed. The increase in computing power is largely due to the GPUs. As we all know, the current popular deep learning frameworks such as tensorfow, pytorch, mxnet, etc. all support GPU acceleration. In order to explore the implementation principles behind the deep learning framework, this blog post will attempt to build a simple deep learning framework - Tinyflow. We will build a general automatic differentiation framework in which you can add any custom operator. To keep it simple, Tinyflow only implements the operators necessary for multilayer perceptron (MLP) models (such as MatMulOp, ReluOp, SoftmaxCrossEntropyOp), and of course it supports the addition of any other operators (such as ConvOp). At the bottom, we will use GPUs to accelerate matrix operations. Although compared to the mature deep learning framework, Tinyflow is very simple, but it does have the two core elements necessary for deep learning framework: automatic differentiation and GPU operation acceleration. Understanding the content of this blog post requires knowledge of CUDA programming. For the basics of CUDA programming I recommend the book Professional CUDA C Programming. Of course, you can also access the online documentation of CUDA Toolkit. OverviewTinyflow is written jointly by Python and C++. The automatic differentiation framework is written in Python and provides various operators required for building neural network models (such as AddOp, MatMulOp, ReluOp, SoftmaxCrossEntropyOp, etc.). Tinyflow uses GPU to accelerate a large number of matrix operations involved in automatic differentiation framework. Below is the architecture of Tinyflow. Python Layer API provides the implementation of automatic differentiation framework and abstract n-dimensional array interface. When we start training a network built with Python APIs, Tinyflow will automatically call GPU Kernel functions for complex matrix operations implemented by C++. Implementation detailsThe principles behind Tinyflow are very simple, they are automatic differentiation and GPU acceleration. Automatic DifferentiationAutomatic differentiation is the core of all deep learning frameworks. It is automatic differentiation that frees us from tedious gradient calculations, allowing us to focus on building network models. In this blog post we will not explain in detail the principle of automatic differentiation. If you don’t know what automatic differentiation is, please refer to my blog post Automatic Differentiation Based on Computation Graph. GPU OperationBecause multi-dimensional arrays in C language are physically stored row-first and continuously. So in many cases we will use one-dimensional CUDA threads to process a two-dimensional matrix or a one-dimensional vector. To limit the use of GPU resources, we can define the following macros.123456#define MAX_THREADS_NUM 512#define MAX_BLOCKS_NUM 4096#define BLOCK_NUM(count) min(((count + MAX_THREADS_NUM - 1) / MAX_THREADS_NUM), MAX_BLOCKS_NUM)#define CUDA_1D_KERNEL_LOOP(i, n) \\ for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n); \\ i += blockDim.x * gridDim.x) CUDA_1D_KERNEL_LOOP will loop through all the data. The schematic is as follows. Since there are many GPU OPs involved, the GPU OPs will not be described in detail here, but several classic kernels are described. You can find all the GPU OPs code here. As an example, let’s look at a kernel with a matrix addition. In the kernel we can think of the matrix as a one-dimensional array, so we can quickly write out its kernel based on our defined CUDA_1D_KERNEL_LOOP.12345__global__ void matrix_elementwise_add_kernel(float* matAData, float* matBData, float* outputData, int count) { CUDA_1D_KERNEL_LOOP(index, count) { outputData[index] = matAData[index] + matBData[index]; }} The Kernel of Softmax OP is relatively complex. We will use a CUDA thread to process a row of data in the matrix.12345678910111213141516171819__global__ void matrix_softmax_kernel(int nRow, int nCol, float* inputArr, float* outputArr) { int y = blockIdx.x * blockDim.x + threadIdx.x; if (y &gt;= nRow) return; float* input = inputArr + y * nCol; float* output = outputArr + y * nCol; float maxval = *input; for (int i = 1; i &lt; nCol; ++i) { maxval = max(input[i], maxval); } float sum = 0; for (int i = 0; i &lt; nCol; ++i) { sum += expf(input[i] - maxval); } for (int i = 0; i &lt; nCol; ++i) { output[i] = expf(input[i] - maxval) / sum; }} MLP ModelBased on the automatic differentiation framework we built and the OP with GPU acceleration, we can quickly build a MLP model. Below is the code how to build a 3-layer model.123456789101112131415161718192021222324252627282930W1 = ad.Variable(name=&quot;W1&quot;)W2 = ad.Variable(name=&quot;W2&quot;)W3 = ad.Variable(name=&quot;W3&quot;)b1 = ad.Variable(name=&quot;b1&quot;)b2 = ad.Variable(name=&quot;b2&quot;)b3 = ad.Variable(name=&quot;b3&quot;)X = ad.Variable(name=&quot;X&quot;)y_ = ad.Variable(name=&quot;y_&quot;)# relu(X W1+b1)z1 = ad.matmul_op(X, W1)z2 = z1 + ad.broadcastto_op(b1, z1)z3 = ad.relu_op(z2)# relu(z3 W2+b2)z4 = ad.matmul_op(z3, W2)z5 = z4 + ad.broadcastto_op(b2, z4)z6 = ad.relu_op(z5)# softmax(z5 W2+b2)z7 = ad.matmul_op(z6, W3)y = z7 + ad.broadcastto_op(b3, z7)loss = ad.softmaxcrossentropy_op(y, y_)grad_W1, grad_W2, grad_W3, grad_b1, grad_b2, grad_b3 = ad.gradients( loss, [W1, W2, W3, b1, b2, b3])executor = ad.Executor( [loss, grad_W1, grad_W2, grad_W3, grad_b1, grad_b2, grad_b3, y], ctx=executor_ctx) MLP Model for MNISTAfter we implement all the GPU operation we can see the significant performance gain. Below is the training result I ran on my personal computer with a single Quadro K620 GPU with 2G global memory. Softmax Regression Multi-layer NN Epoch 10 10 Numpy Accuracy 92.23% 97.17% Numpy Time 2.0056s 7.4211s GPU Accuracy 92.23% 97.09% GPU Time 1.5424s 2.9890s We can find that GPU training can improve the efficiency of training, and the more complex the model is, the more obvious the effect is.","link":"/tinyflow.html"},{"title":"Flink SQL源码 - Table API & SQL概览","text":"本文已收录在合集Apche Flink原理与实践中. 本文是Flink SQL执行框架源码分析系列的第二篇, 将从整体上介绍与Table API和SQL实现相关的模块, 并解析内部是如何通过组合各个模块实现SQL的解析, 优化与执行的. 最后通过对现有模块的简单扩展来实现一个新的用户接口executeStatements, 该接口可直接运行用户提供的整个SQL脚本. 需要注意的是, 本文仅从宏观角度介绍Table API和SQL内部的各个模块是如何组合使用的, 具体实现原理会在后续的文章的详细介绍. TableTable是Table API的核心抽象, 它只有一个实现类TableImpl. Table仅是对如何获取和转换数据的描述, 并不持有数据. 作为类比, Table可以理解为SQL中的视图. TableEnvironment提供了以下几个接口用于创建Table对象: fromValues() from() sqlQuery() fromDataStream(), fromChangelogStream() TableResult是Table.execute()和TableEnvironment.executeSql()的返回结果类型. Table.execute()返回的TableResult是对JobClient的封装, 后续可以从中取回执行结果. 而TableEnvironment.executeSql()返回的TableResult仅用于标识执行成功, 是一个只有一行一列的表, 其列名为result, 值为OK. TableResult的实现关系如下图所示, 其核心实现在TableResultImpl中. Table API &amp; SQL核心实现模块TableEnviroment是创建Table API和SQL程序的核心入口, 它提供了统一的用于处理有界和无界数据的接口. TableEnviroment主要负责: 注册并管理Catalog, Table, View以及Function等元素, 如registerCatalog(), from(), createTemporaryView()和createFunction()等接口; 执行SQL语句, 如sqlQuery()和executeSql()接口. TableEnviroment的继承关系如下图所示, 其中核心的实现逻辑在TableEnviromentImpl中. StreamTableEnviroment对TableEnviroment进行了扩展, 添加了Table与DataStream之间的转换接口, 如fromDataStream()和toDataStream()等. 在纯Table API和SQL程序中建议使用TableEnviroment, 如果需要在Table和DataStream之间转换, 则使用StreamTableEnviroment. TableEnviroment的核心成员如下, 它正是组合使用这些核心成员来将Table API或SQL描述的Flink程序转化为Transformation并提交给Flink集群运行. CatalogManager: 用于管理Catalog, 并提供了操作Table元数据的接口. FunctionCatalog: 用于管理UDF, 提供了函数的注册和删除接口. 长期来看它会被废弃, 相关的操作会合并到CatalogManager中. ModuleManager: 用于管理Module, 默认会加载CoreModule, 在Hive兼容模式下需要加载HiveModule. ResourceManager: 用于管理各种用户资源, 如依赖的Jar包, 文件等. TableConfig: 用于配置当前TableEnviroment会话. Executor: 提供了将Transformation图转化为StreamGraph, 以及执行StreamGraph的接口. OperationTreeBuilder: 用于将Table API中的各种操作转化为Operation. Planner: 用于将Operation翻译为Transformation图. CatalogManagerCatalogManager用于管理Catalog, Table, View等对象. 在Flink SQL中, 存在类似于关系型数据库中从Cataog到Database到Table/View的层级关系, 每个上一级对象都可以包含多个下一级对象. CatalogManager借助Catalog管理Table和View. CatalogCatalog用于管理Database, Teble/View, UDF等元数据, 需要注意的是Catalog仅管理永久对象, 临时的Table/View或UDF分别由CatalogManager和FunctionCatalog管理. Catalog的继承关系如下图所示: 默认情况下使用的是GenericInMemoryCatalog, 即将所有元数据信息都保存在内存中. HiveCatalog支持与HiveMetaStore集成. AbstractJdbcCatalog的几个实现类并不支持创建对象, 主要是用于Flink CDC集成数据时获取MySQL或PostgreSQL中已有的表. FunctionCatalogFunctionCatalog的实现较为简单, 它直接管理临时UDF, 并借助Catalog管理永久UDF. 从长远来看, FunctionCatalog应当是CatalogManager的一部分. ModuleManagerModuleManager用于管理Mudule, 目前的Mudule支持扩展UDF, 未来可能会考虑支持扩展数据类型, 算子, 优化规则等. 默认情况下会加载CoreModule, Flink也实现了HiveModule以加载Hive内置的函数. TableConfigTableConfig用于配置当前TableEnviroment会话, 其配置加载顺序如下: config.yaml文件中的配置; 客户端启动时指定的参数; StreamExecutionEnvironment中配置的参数; EnvironmentSettings.Builder.withConfiguration(Configuration), TableEnvironment.create(Configuration)创建的配置; 通过set(ConfigOption, Object) / set(String, String)配置的参数. ExecutorExecutor用于执行Planner生成的Transformation图, 可将TableEnviroment与具体的运行时解耦. DefaultExecutor的实现逻辑比较简单, 即调用StreamExecutionEnvironment创建或执行StreamGraph. OperationTreeBuilderFlink在实现中为了统一处理Table API和SQL, 抽象了Operation接口. Table API中的各种操作, 如project, aggregate, join等都会通过OperationTreeBuilder转化为QueryOperation. SQL语句经过Calcite解析之后会转化为PlannerQueryOperation, 其内部持有SQL语句转化来的RelNode树. PlannerPlanner是TableEnviroment中最核心的成员, 它主要有两个用处: 作为SQL解析器(通过getParser()获取), 将SQL文本解析为Operation树; 关系代数优化器, 将ModifyOperation优化并转化为Transformation树. Planner的两个实现类为StreamPlanner和BatchPlanner, 分别为流批作业执行优化, 在内部会使用不同的优化器. Table API &amp; SQL执行流程TableEnvironment/StreamTableEnvironment的创建流程TableEnvironment和StreamTableEnvironment的创建流程基本一致, 分别在TableEnvironmentImpl和StreamTableEnvironmentImpl的create()方法中. Table API最佳实践上文介绍了与Table API和SQL执行相关的模块, 为了加深理解, 接下来我们通过一个案例来说明如何使用这些模块进行开发，以扩展现有API的能力. 如果要对Flink进行平台化, SQL脚本的执行是一个十分重要的能力, 但是目前开源Flink中的TableEnvironment并不支持直接执行SQL脚本. 为了实现执行SQL脚本的能力, 我们当然可以先将SQL脚本按分号分割, 然后调用TableEnvironment.executeSql(). 不过这种实现方式相对繁琐, 实际上Flink已经有了一个支持SQL脚本的解析器, 我们只需要稍加扩展就能实现SQL脚本的执行, 而无需编写繁杂的SQL解析代码. 完成本文的案例之后, 可以通过如下代码执行一个SQL脚本. 笔者提供的可运行示例见StatementsExample.1234567StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);String statements = Utils.readResourceFile( &quot;my/statements.sql&quot;, StatementsExample.class.getClassLoader());tEnv.executeStatements(statements); 要实现上述功能, 我们只需要围绕Parser和TableEnvironment做一些扩展. 具体实现如下: 在Parser接口中新增一个parseStatements()方法用于解析SQL脚本, 并在ParserImpl中实现该方法. 这里需要考虑的一点是, parseStatements()实际上进行了SQL解析和验证两个操作, 最终需要返回的是经过验证的Operation集合. 而对于SQL脚本而言, 后面的DML语句依赖于前面的DDL语句, 只有DDL语句对应的Operation执行完成后, DML语句才能通过验证. 因此parseStatements()不能直接返回List&lt;Operation&gt;, 比较合适的做法是返回一个Iterator&lt;Operation&gt;以实现延迟验证, 将验证的过程添加到Iterator&lt;Operation&gt;的迭代过程中. 在TableEnvironment中新增一个executeStatements()方法用于执行SQL脚本, 并在TableEnvironmentImpl中实现该方法. 为了实现上文所说的延迟验证, 并更好地处理Statement set, 我们添加一个OperationsConverter来帮助将SqlNode进行验证并转化为Operation. 其代码如下.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class OperationsConverter implements Iterator&lt;Operation&gt; { private final Iterator&lt;SqlNode&gt; sqlNodeIterator; private final FlinkPlannerImpl planner; private final CatalogManager catalogManager; private boolean isInStatementSet; private List&lt;ModifyOperation&gt; statementSetOperations; public OperationsConverter( SqlNodeList sqlNodeList, FlinkPlannerImpl planner, CatalogManager catalogManager) { this.sqlNodeIterator = sqlNodeList.iterator(); this.planner = planner; this.catalogManager = catalogManager; } @Override public boolean hasNext() { return sqlNodeIterator.hasNext(); } @Override public Operation next() { Optional&lt;Operation&gt; operation = Optional.empty(); while (!operation.isPresent()) { operation = convert(); } return operation.get(); } private Optional&lt;Operation&gt; convert() { if (!sqlNodeIterator.hasNext() &amp;&amp; isInStatementSet) { throw new ValidationException( &quot;&apos;BEGIN STATEMENT SET;&apos; must be used with &apos;END;&apos;, but no &apos;END;&apos; founded.&quot;); } SqlNode sqlNode = sqlNodeIterator.next(); Operation operation = SqlNodeToOperationConversion.convert(planner, catalogManager, sqlNode) .orElseThrow( () -&gt; new TableException( &quot;Unsupported statement: &quot; + sqlNode.toString())); if (isInStatementSet &amp;&amp; !(operation instanceof SinkModifyOperation || operation instanceof StatementSetOperation || operation instanceof EndStatementSetOperation)) { throw new ValidationException( &quot;Only &apos;INSERT INTO&apos; statement is allowed in statement set.&quot;); } if (operation instanceof ModifyOperation) { if (isInStatementSet) { statementSetOperations.add((ModifyOperation) operation); return Optional.empty(); } else { return Optional.of(operation); } } else if (operation instanceof BeginStatementSetOperation) { isInStatementSet = true; statementSetOperations = new ArrayList&lt;&gt;(); } else if (operation instanceof EndStatementSetOperation) { if (!isInStatementSet) { throw new ValidationException( &quot;&apos;END;&apos; must be used after &apos;BEGIN STATEMENT SET;&apos;, &quot; + &quot;but no &apos;BEGIN STATEMENT SET;&apos; founded.&quot;); } if (statementSetOperations.isEmpty()) { throw new ValidationException(&quot;NO statements in statement set.&quot;); } isInStatementSet = false; StatementSetOperation statementSetOperation = new StatementSetOperation(statementSetOperations); return Optional.of(statementSetOperation); } else { return Optional.of(operation); } return Optional.empty(); }} 总结本文从整体上介绍了Table API和SQL内部实现中的几个相关模块, 了解这些模块是进一步了解Flink Table API和SQL实现原理的基础. 这些模块经过精心的设计, 具有强大的扩展性, 基于这些抽象接口进行扩展开发, 可使得Flink支持更多的上下游组件.","link":"/flink-sql-table-api.html"},{"title":"Flink SQL源码 - 整体架构及处理流程","text":"本文已收录在合集Apche Flink原理与实践中. Flink SQL作为Stream SQL的事实标准, 已经得到了广泛的应用. 然而不同于Hive, Spark等离线引擎或数据库的SQL, 流式系统中的SQL有更多的隐藏特性和复杂度, 理解其内部实现是更好使用Flink SQL的基础. 为此, 笔者计划通过一系列文章对Flink SQL的执行框架进行抽茧剥丝, 详细介绍其内部实现. 本文是该系列的第一篇, 将从整体上介绍Flink SQL的执行流程. 后续文章会进一步介绍各个模块的具体实现. Table API &amp; SQL入口在代码层面, TableEnvironment是使用Table API和SQL的统一入口, 其他Flink SQL的平台化能力, 如SQL Gateway等都是基于此构建的, 其使用方式如下. 12345678910111213141516171819202122232425TableEnvironment tableEnv = TableEnvironment.create(/*…*/);// 通过Table API创建表tableEnv.createTemporaryTable( &quot;SourceTable&quot;, TableDescriptor .forConnector(&quot;datagen&quot;) .schema( Schema.newBuilder() .column(&quot;f0&quot;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L) .build());// 通过SQL DDL创建表tableEnv.executeSql(&quot;CREATE TEMPORARY TABLE SinkTable WITH (&apos;connector&apos; = &apos;blackhole&apos;) LIKE SourceTable (EXCLUDING OPTIONS) &quot;);// 通过Table API执行Flink作业Table table1 = tableEnv.from(&quot;SourceTable&quot;);TableResult tableResult1 = table1.insertInto(&quot;SinkTable&quot;).execute();// 通过SQL DQL创建TableTable table2 = tableEnv.sqlQuery(&quot;SELECT * FROM SourceTable&quot;);// 通过SQL DML执行Flink作业TableResult tableResult2 = tableEnv.executeSql(&quot;INSERT INTO SinkTable SELECT * FROM SourceTable&quot;); 由于Table API的使用场景较少, 在实现上跟SQL也有很大相似性, 因此本文及后续文章会将重点放在SQL的实现上. 此外, DDL语句的执行比较简单, 无非是元数据的操作, 重要的是DQL及DML语句的执行, 本文所介绍的执行流程也是指这两类SQL语句的执行. 整体执行流程 Flink SQL的解析及关系代数的优化都是通过扩展Apache Calcite实现的, 关于Calcite的原理解析在笔者之前的系列文章Apache Calcite原理与实践中已经详细介绍过, 这里将不再介绍过多的细节. Flink SQL的整体执行流程如上图所示(其中黄色框中是Calcite中的核心模块, 包含Flink的扩展), 从端到端的视角来看, Flink SQL模块的任务就是将输入的SQL文本翻译为Flink的Transformation结构, 之后的提交和执行流程就跟DataStream作业一样了. 从SQL到Transformation的翻译过程都是在本地JVM中执行的, 这也给我们调试Flink SQL的执行过程带来了便利. Operation是Flink引入的一种中间结构, 是Table API和SQL的统一中间表达. 以Operation结构作为分界, Flink SQL的执行过程可分为两个大的阶段, 即从SQL文本到Operation的转换以及从Operation到Transformation的转换. 从SQL文本到Operation的转换又可分为以下几个具体步骤: SQL解析: 将SQL文本解析为Calcite的SqlNode结构, Flink在SQL语法上基于Calcite进行了一定扩展; SQL验证: 将解析得到的SqlNode配合元数据进行语义验证, 输出结构仍为SqlNode结构; 关系代数转换: 将验证后的SqlNode结构转换为RelNode结构, 即关系代数; Operation转换: 将RelNode结构封装为Operation, 具体就是封装为PlannerQueryOperation. 从Operation到Transform的转换又可分为以下几个步骤: DAG图预处理: Flink SQL可能包含多个INSERT语句, 因此生成的RelNode 会形成一个DAG结构, 而Calcite优化器不支持直接处理DAG. 因此Flink优化器会先对这个DAG进行一些预处理, 主要包括DAG子图复用, DAG子图分割; 关系代数优化: DAG子图分割后的树结构就能输入Calicte优化器进行优化了, 主要是利用Calcite的HepPlanner和VolcanoPlanner进行关系代数优化, Flink在Calcite的基础上扩展了一系列优化规则和关系节点; ExecNodeGraph生成: 将优化后的关系代数翻译成Flink定义的ExecNodeGraph结构. 它是SQL层最终的执行计划, 基于它可以进一步做DAG层的物理优化; Transformation生成: 将ExecNodeGraph进一步翻译为Transformation结构, 这一步会利用Codegen技术动态生成一些算子的Java代码. 源码模块Flink Table API &amp; SQL相关的实现都在flink-table模块下面, 其各个子模块的核心功能如下. 123456789101112131415161718192021flink-table # Table API &amp; SQL实现父模块├── flink-sql-client # SQL Client实现├── flink-sql-gateway # SQL Gateway实现├── flink-sql-gateway-api # SQL Gateway API接口, 用于其他系统调用SQL Gateway服务├── flink-sql-jdbc-driver # Flink SQL JDBC接口实现├── flink-sql-jdbc-driver-bundle # 打包模块, 用于打包jdbc-driver及相关依赖├── flink-sql-parser # SQL解析实现, 在Calcite之上进行了扩展├── flink-table-api-bridge-base # api-java-bridge和api-scala-bridge中共同依赖的接口├── flink-table-api-java # 包含编写Table/SQL程序的Java API接口├── flink-table-api-java-bridge # 包含Table/SQL API与其他API交互的Java接口├── flink-table-api-java-uber # 打包模块, 包含Table/SQL生态的所有Java API, 用于在Table生态内或其他Flink API之间编写Table/SQL程序├── flink-table-api-scala # 包含编写Table/SQL程序的Scala API接口├── flink-table-api-scala-bridge # 包含Table/SQL API与其他API交互的Scala接口├── flink-table-calcite-bridge # 包含扩展Planner插件的Calcite依赖├── flink-table-code-splitter # 用于对Codegen生成的Java代码进行分割, 以保证满足Java方法不超过64KB的限制├── flink-table-common # 包含Table/SQL API的一些通用实现├── flink-table-planner # Table/SQL程序核心优化处理模块├── flink-table-planner-loader # 用于加载flink-table-planner├── flink-table-planner-loader-bundle # 用于构造被flink-table-planner-loader加载的JAR包├── flink-table-runtime # SQL算子的具体实现└── flink-table-test-utils 总结本文从宏观上介绍了Flink SQL的执行流程, 可以看到整个流程基本上是围绕Calcite框架展开的, 当然在其中也加入了一些Flink特有的步骤. Table Planner的核心目的就是通过分层优化, 将用户输入的SQL代码尽可能转化为Flink的底层执行计划(Transformation结构), 当然这当中涉及众多的优化细节, 将在后续的文章中逐步详细介绍. 参考[1] How to write fast Flink SQL","link":"/flink-sql-overview.html"},{"title":"Flink SQL源码 - SQL函数原理与实现","text":"本文已收录在合集Apche Flink原理与实践中. Flink SQL函数丰富了SQL层的数据处理能力, 除了大量的内置函数, Flink还支持用户自定义函数(User-defined function, UDF). 在Flink SQL优化器中, 会对函数进行多层转换, 本文将对此进行详细介绍. 理解了这一流程, 便可为Flink添加更多内置函数, 亦可理解UDF的执行原理与可能出现的问题. SQL函数分类在Flink SQL中, 函数可以从多个维度进行分类. 其中一个维度是分类为System(or Built-in) Function和Catalog Funtion. 其中System Function可以直接通过名称引用, 而Catalog Function属于某个Catalog和Database, 因此可以通过catalog.db.func或db.func或直接使用函数名这几种方式引用. 另一个维度可分类为Temporary Function和Persistent Function, 其中Temporary Function一般仅存在于一个会话中, 而Persistent Function一般为System Function或持久化在Catalog中. 通过以上两个维度, 可以将Flink函数分为四种类型: Temporary system functions: 在TableEnvironment中可以通过createTemporarySystemFunction()创建. System functions: 由Flink内核提供, 用户无法直接添加, 只有通过修改内核代码后重新编译才能添加. Temporary catalog functions: 在TableEnvironment中可以通过createTemporaryFunction()创建. Catalog functions: 在TableEnvironment中可以通过createFunction()创建. 对于UDF来说, 一般还可从函数的输入输出类型区分为: Scalar Function: 输入输出存在一对一关系, 即输入一行数据输出一行数据, 输入可以有多列, 输出只能有一列. Table Function: 输入输出存在一对多关系, 即输入一行数据输出零行或多行数据, 输入输出均支持多列. Aggregate Function: 输入输出存在一对多关系, 即输入多行数据输出一行数据, 输入可以有多列, 输出只能有一列. Table Aggregate Function: 输入输出存在多对多关系, 即输入多行数据输出多行数据, 输入输出均可有多列. Table Aggregate Function目前无法再SQL中使用, 只能用于Table API. Table Planner处理流程在Flink SQL中, 函数在SQL的解析阶段, 验证阶段和代码生成阶段都会进行相应的转换, 最终包装在相应的StreamOperator实现类中执行. 以下是一个使用了UDF的SELECT语句的转换过程. 解析阶段在SQL解析阶段, 所有类型的函数都会被解析为SqlUnresolvedFunction, 这一阶段直接由Calcite实现, 本文不再赘述. 验证阶段解析阶段仅做语法上的解析, 所有形如f(param)形式的表达式都会被解析为SqlUnresolvedFunction. 验证阶段会验证函数是否已经注册, 对于已注册的函数会根据函数类型转换为SqlFunction的子类型, 并验证其输入输出类型是否符合语义. 函数查找及转换的整体调用流程如下图所示, 这一流程还是在Calcite的验证框架中执行的. 上图中, FlinkCalciteSqlValidator是SqlValidatorImpl的简单扩展. SqlValidatorImpl通过SqlOperatorTable获取函数元数据, 在Flink中其实现类是FunctionCatalogOperatorTable和FlinkSqlOperatorTable, 并组合在ChainedSqlOperatorTable中. 其中: FunctionCatalogOperatorTable用来适配FunctionCatalog, 包含用户注册的Catalog Function, 以及定义在BuiltInFunctionDefinitions中的System Function(通过CoreModule加载). FlinkSqlOperatorTable则定义了Flink特有的一些函数, 如TUMBLE, HOP等. 如果使用了未注册的函数, 验证阶段会直接抛出异常. 对于已注册的函数, 会转换为SqlFunction的子类型, 具体来说: 如果是UDF Scalar Function被转换为BridgingSqlFunction; Table Function被转换为BridgingSqlFunction.WithTableFunction; Aggregate Function和Table Aggregate Function被转换为BridgingSqlAggFunction. 如果是内置函数 如果是有Runtime类的函数, 同UDF一样; 否则被转换为SqlFunction或其子类, 这类函数对应的具体类型都在FlinkSqlOperatorTable中定义. 转换为SqlFunction之后会对函数的输入输出类型进行语义验证, 验证实现在SqlOperator.checkOperandTypes()中(SqlFunction是SqlOperator的子类), 会调用SqlOperandTypeChecker进行类型检验, 在Flink中其实现类是TypeInferenceOperandChecker. 代码生成阶段无论是解析阶段产出的SqlUnresolvedFunction还是验证阶段产出的SqlFunction都是Calcite定义的, 无法提交给Flink执行引擎运行, 代码生成阶段会将SqlFunction对应的可执行函数嵌入到StreamOperator中. 具体来说, 不同类型函数所生成的Transformation结构如下图所示, 其中蓝色部分均是自动生成的代码, Scalar Function和Table Function均会嵌入到自动生成的TableStreamOperator子类中(生成代码可参考附录一和附录二), 在其中的processElement()函数中会调用用户提供的UDF/UDTF进行处理; Aggregate Function会嵌入到自动生成的GeneratedAggsHandleFunction中(生成代码可参考附录三); Table Aggregate Function会嵌入到自动生成的TableAggsHandleFunction中(生成代码可参考附录四). 函数相关的代码生成的逻辑在ExprCodeGenerator.generateCallExpression()中, 它会根据函数的类型选择对应的生成方法. 最佳实践新增系统函数尽管Flink开源版本已经提供了大量的内置函数, 但在实际使用中仍有扩展空间. 如果要添加系统函数, 通过上文的分析可知存在多种方法: 如果仅需扩展少数几个函数, 可直接在BuiltInFunctionDefinitions或FlinkSqlOperatorTable中增加函数定义即可; 如果需要扩展某一类函数, 如支持空间计算的函数, 可创建一个继承自ReflectiveSqlOperatorTable的FlinkSqlSpatialOperatorTable, 在其中定义相关函数, 减少对已有代码的侵入. 并在PlannerContext.getBuiltinSqlOperatorTable()返回的ChainedSqlOperatorTable中添加新增的FlinkSqlSpatialOperatorTable; 如果是为了添加兼容某个已有系统的函数, 可以参考Hive的实现, 添加一个HiveModule来加载对应的函数. UDF重复调用问题Flink SQL的自定义Scalar Function, 在以下两种SQL Pattern中存在重复调用的问题: 第一种是在SELECT列和WHERE条件中都使用了UDF进行计算; 第二种是在子查询中使用了UDF, 并在外层查询中引用了UDF的计算结果. 以下是重复调用的SQL示例, 读者可运行UDFExample查看结果以加深理解.123456789101112131415-- 情况一: 理想情况下对于每行记录, 只需调用1次UDF(b), 实际需调用4次SELECT split_index(UDF(b), &apos;|&apos;, 0) AS y, split_index(UDF(b), &apos;|&apos;, 1) AS m, split_index(UDF(b), &apos;|&apos;, 2) AS dFROM source WHERE UDF(b) &lt;&gt; &apos;NULL&apos;-- 情况二, 理想情况下对于每行记录, 只需调用1次UDF(b), 实际需调用3次SELECT split_index(r, &apos;|&apos;, 0) AS y, split_index(r, &apos;|&apos;, 1) AS m, split_index(r, &apos;|&apos;, 2) AS dFROM ( SELECT UDF(b) AS r FROM source); 为了搞清楚上述SQL出现UDF重复调用的原因, 我们先看一下它们对应的逻辑和物理表达式.1234567891011121314151617181920-- 情况一逻辑表达式LogicalSink(table=[*anonymous_collect$1*], fields=[y, m, d]) LogicalProject(y=[SPLIT_INDEX(UDF($1), _UTF-16LE&apos;|&apos;, 0)], m=[SPLIT_INDEX(UDF($1), _UTF-16LE&apos;|&apos;, 1)], d=[SPLIT_INDEX(UDF($1), _UTF-16LE&apos;|&apos;, 2)]) LogicalFilter(condition=[&lt;&gt;(UDF($1), _UTF-16LE&apos;NULL&apos;)]) LogicalTableScan(table=[[default_catalog, default_database, source, source: [CsvTableSource(read fields: a, b)]]])-- 情况一物理表达式StreamPhysicalSink(table=[*anonymous_collect$1*], fields=[y, m, d]) StreamPhysicalCalc(select=[SPLIT_INDEX(UDF(b), &apos;|&apos;, 0) AS y, SPLIT_INDEX(UDF(b), &apos;|&apos;, 1) AS m, SPLIT_INDEX(UDF(b), &apos;|&apos;, 2) AS d], where=[&lt;&gt;(UDF(b), &apos;NULL&apos;)]) StreamPhysicalLegacyTableSourceScan(table=[[default_catalog, default_database, source, source: [CsvTableSource(read fields: b)]]], fields=[b])-- 情况二逻辑表达式LogicalSink(table=[*anonymous_collect$1*], fields=[y, m, d]) LogicalProject(y=[SPLIT_INDEX(UDF($1), _UTF-16LE&apos;|&apos;, 0)], m=[SPLIT_INDEX(UDF($1), _UTF-16LE&apos;|&apos;, 1)], d=[SPLIT_INDEX(UDF($1), _UTF-16LE&apos;|&apos;, 2)]) LogicalTableScan(table=[[default_catalog, default_database, source, source: [CsvTableSource(read fields: a, b)]]])-- 情况二物理表达式StreamPhysicalSink(table=[*anonymous_collect$1*], fields=[y, m, d]) StreamPhysicalCalc(select=[SPLIT_INDEX(UDF(b), &apos;|&apos;, 0) AS y, SPLIT_INDEX(UDF(b), &apos;|&apos;, 1) AS m, SPLIT_INDEX(UDF(b), &apos;|&apos;, 2) AS d]) StreamPhysicalLegacyTableSourceScan(table=[[default_catalog, default_database, source, source: [CsvTableSource(read fields: b)]]], fields=[b]) 可以看到, 上述两种情况的物理表达式是基本相同的, 所有UDF相关的计算都会合并到StreamPhysicalCalc中. UDF重复调用的本质原因就是Flink Table Planner在对StreamPhysicalCalc进行代码生成时, 未考虑UDF表达式的可重用性(情况二生成的代码可参考附录五). 实际上, Flink UDF都实现了FunctionDefinition接口, 其中的isDeterministic()可让用户返回一个boolean值以指示当前UDF是否具有确定性. 在代码生成时可以借助该方法判断UDF结果是否可重用, 然而Flink目前的实现完全忽略了该信息, FLINK-21573报告了该问题, 不过目前看还没有下文. 重复调用对于计算复杂度高的UDF会产生性能问题, 此外如果在UDF中使用了状态也可能导致意外错误. 要从根本上解决这一问题, 需要更改Flink Codegen实现, 在Codegen时根据UDF是否确定决定是否重用计算值, 具体实现可参考这一Commit, 修改后生成的代码如附录六所示, 只会调用一次UDF. 如果无法修改Flink源码, 在实践中也可以通过以下方法规避: 在Flink 1.17及之前的版本中, 可以将UDF改为UDTF, 虽然UDTF的写法略微复杂, 但是由于在代码生成阶段都是在TableStreamOperator中调用, 性能上不会有太大差距. 从Flink 1.18开始, 将多个Project的合并由在SqlNode到RelNode的转换过程中实现, 变成了由优化器实现, 具体可参考FLINK-20887. 在底层Project列涉及非确定的UDF计算时, 仅在顶层Project仅引用一次底层Project计算列时才能将两个Project合并. 以上述情况二的SQL为例, 如果让UDF的isDeterministic()方法返回false, 那么生成的表达式将如下所示, UDF的计算会在一个单独的StreamPhysicalCalc中, 这样就避免了重复计算. 123456789LogicalSink(table=[*anonymous_collect$1*], fields=[y, m, d]) LogicalProject(y=[SPLIT_INDEX($0, _UTF-16LE&apos;|&apos;, 0)], m=[SPLIT_INDEX($0, _UTF-16LE&apos;|&apos;, 1)], d=[SPLIT_INDEX($0, _UTF-16LE&apos;|&apos;, 2)]) LogicalProject(r=[UDF($1)]) LogicalTableScan(table=[[default_catalog, default_database, source, source: [CsvTableSource(read fields: a, b)]]])StreamPhysicalSink(table=[*anonymous_collect$1*], fields=[y, m, d]) StreamPhysicalCalc(select=[SPLIT_INDEX(r, &apos;|&apos;, 0) AS y, SPLIT_INDEX(r, &apos;|&apos;, 1) AS m, SPLIT_INDEX(r, &apos;|&apos;, 2) AS d]) StreamPhysicalCalc(select=[UDF(b) AS r]) StreamPhysicalLegacyTableSourceScan(table=[[default_catalog, default_database, source, source: [CsvTableSource(read fields: b)]]], fields=[b]) 对于情况一, 我们可以将SQL做如下修改, 这样其效果与情况二将类似.12345678SELECT split_index(r, &apos;|&apos;, 0) AS y, split_index(r, &apos;|&apos;, 1) AS m, split_index(r, &apos;|&apos;, 2) AS dFROM ( SELECT UDF(b) AS r FROM source)WHERE r &lt;&gt; &apos;NULL&apos;; 总结本文从Flink SQL源码的角度分析了SQL函数的实现原理, 通过本文可以了解到, SQL函数是如何从用户定义的函数类转换为Flink可执行代码. 了解了这一过程之后, 在编写和使用UDF时便能妥善处理UDF中的状态, 并规避在使用UDF时可能出现的问题. 参考[1] Flink Documentation - Table API &amp; SQL - Functions - Overview[2] [FLINK-21573] Support expression reuse in codegen[3] [FLINK-20887] Non-deterministic functions return different values even if it is referred with the same column name 附录生成的代码均做了一些简化, 如去掉全路径包名, 变量名称简化等, 以方便阅读. 附录一SELECT UDF(b) FROM source WHERE UDF(b) IS NOT NULL;生成的与UDF计算相关的代码.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class StreamExecCalc$11 extends TableStreamOperator implements OneInputStreamOperator { private final Object[] references; private transient StringDataSerializer typeSerializer$3; private transient UDF udf; private transient StringStringConverter converter$5; BoxedWrapperRowData out = new BoxedWrapperRowData(1); private final StreamRecord outElement = new StreamRecord(null); public StreamExecCalc$11( Object[] references, StreamTask task, StreamConfig config, Output output, ProcessingTimeService processingTimeService) throws Exception { this.references = references; typeSerializer$3 = (((StringDataSerializer) references[0])); udf = (((UDF) references[1])); converter$5 = (((StringStringConverter) references[2])); udf = (((UDF) references[3])); this.setup(task, config, output); if (this instanceof AbstractStreamOperator) { ((AbstractStreamOperator) this).setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); udf.open(new FunctionContext(getRuntimeContext())); converter$5.open(getRuntimeContext().getUserCodeClassLoader()); } @Override public void processElement(StreamRecord element) throws Exception { RowData in1 = (RowData) element.getValue(); BinaryStringData field$2; boolean isNull$2; BinaryStringData field$4; String externalResult$6; BinaryStringData result$7; boolean isNull$7; String externalResult$9; BinaryStringData result$10; boolean isNull$10; isNull$2 = in1.isNullAt(0); field$2 = BinaryStringData.EMPTY_UTF8; if (!isNull$2) { field$2 = ((BinaryStringData) in1.getString(0)); } field$4 = field$2; if (!isNull$2) { field$4 = (BinaryStringData) (typeSerializer$3.copy(field$4)); } externalResult$6 = (String) udf .eval(isNull$2 ? null : ((String) converter$5.toExternal((BinaryStringData) field$4))); isNull$7 = externalResult$6 == null; result$7 = BinaryStringData.EMPTY_UTF8; if (!isNull$7) { result$7 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$6); } boolean result$8 = !isNull$7; if (result$8) { out.setRowKind(in1.getRowKind()); externalResult$9 = (String) udf .eval(isNull$2 ? null : ((String) converter$5.toExternal((BinaryStringData) field$4))); isNull$10 = externalResult$9 == null; result$10 = BinaryStringData.EMPTY_UTF8; if (!isNull$10) { result$10 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$9); } if (isNull$10) { out.setNullAt(0); } else { out.setNonPrimitiveValue(0, result$10); } output.collect(outElement.replace(out)); } } @Override public void finish() throws Exception { super.finish(); } @Override public void close() throws Exception { super.close(); udf.close(); }} 附录二SELECT y, m, d FROM source, LATERAL TABLE(UDTF(b)) AS T(y, m, d);生成的与UDTF计算相关的代码.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147public class StreamExecCorrelate$12 extends TableStreamOperator implements OneInputStreamOperator { private final Object[] references; private TableFunctionCollector$4 correlateCollector$2 = null; private transient UDTF udtf; private transient StringStringConverter converter$7; private transient RowRowConverter converter$9; private TableFunctionResultConverterCollector$10 resultConverterCollector$11 = null; private final StreamRecord outElement = new StreamRecord(null); public StreamExecCorrelate$12( Object[] references, StreamTask task, StreamConfig config, Output output, ProcessingTimeService processingTimeService) throws Exception { this.references = references; udtf = (((UDTF) references[0])); converter$7 = (((StringStringConverter) references[1])); converter$9 = (((RowRowConverter) references[2])); this.setup(task, config, output); if (this instanceof AbstractStreamOperator) { ((AbstractStreamOperator) this).setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); correlateCollector$2 = new TableFunctionCollector$4(); correlateCollector$2.setRuntimeContext(getRuntimeContext()); correlateCollector$2.open(new Configuration()); udtf.open(new FunctionContext(getRuntimeContext())); converter$7.open(getRuntimeContext().getUserCodeClassLoader()); converter$9.open(getRuntimeContext().getUserCodeClassLoader()); resultConverterCollector$11 = new TableFunctionResultConverterCollector$10(); resultConverterCollector$11.setRuntimeContext(getRuntimeContext()); resultConverterCollector$11.open(new Configuration()); udtf.setCollector(resultConverterCollector$11); correlateCollector$2.setCollector(new StreamRecordCollector(output)); resultConverterCollector$11.setCollector(correlateCollector$2); } @Override public void processElement(StreamRecord element) throws Exception { RowData in1 = (RowData) element.getValue(); BinaryStringData field$5; boolean isNull$5; BinaryStringData result$6; boolean isNull$6; correlateCollector$2.setInput(in1); correlateCollector$2.reset(); if (false) { result$6 = BinaryStringData.EMPTY_UTF8; isNull$6 = true; } else { isNull$5 = in1.isNullAt(1); field$5 = BinaryStringData.EMPTY_UTF8; if (!isNull$5) { field$5 = ((BinaryStringData) in1.getString(1)); } result$6 = field$5; isNull$6 = isNull$5; } udtf.eval(isNull$6 ? null : ((String) converter$7.toExternal((BinaryStringData) result$6))); } @Override public void finish() throws Exception { udtf.finish(); super.finish(); } @Override public void close() throws Exception { super.close(); udtf.close(); } public class TableFunctionResultConverterCollector$10 extends WrappingCollector { public TableFunctionResultConverterCollector$10() throws Exception { } @Override public void open(Configuration parameters) throws Exception { } @Override public void collect(Object record) throws Exception { RowData externalResult$8 = (RowData) (RowData) converter$9.toInternalOrNull((Row) record); if (externalResult$8 != null) { outputResult(externalResult$8); } } @Override public void close() { try { } catch (Exception e) { throw new RuntimeException(e); } } } public class TableFunctionCollector$4 extends TableFunctionCollector { JoinedRowData joinedRow$3 = new JoinedRowData(); public TableFunctionCollector$4() throws Exception { } @Override public void open(Configuration parameters) throws Exception { } @Override public void collect(Object record) throws Exception { RowData in1 = (RowData) getInput(); RowData in2 = (RowData) record; joinedRow$3.replace(in1, in2); joinedRow$3.setRowKind(in1.getRowKind()); outputResult(joinedRow$3); } @Override public void close() { try { } catch (Exception e) { throw new RuntimeException(e); } } }} 附录三SELECT UDAF(a) FROM source GROUP BY a;生成的代码.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130public final class GroupAggsHandler$18 implements AggsHandleFunction { private transient UDAF udaf; private transient StructuredObjectConverter converter$8; GenericRowData acc$10 = new GenericRowData(1); GenericRowData acc$11 = new GenericRowData(1); private RowData agg0_acc_internal; private UDAF.Acc agg0_acc_external; GenericRowData aggValue$17 = new GenericRowData(1); private StateDataViewStore store; public GroupAggsHandler$18(Object[] references) throws Exception { udaf = (((UDAF) references[0])); converter$8 = (((StructuredObjectConverter) references[1])); } private RuntimeContext getRuntimeContext() { return store.getRuntimeContext(); } @Override public void open(StateDataViewStore store) throws Exception { this.store = store; udaf.open(new FunctionContext(store.getRuntimeContext())); converter$8.open(getRuntimeContext().getUserCodeClassLoader()); } @Override public void setWindowSize(int windowSize) {} @Override public void accumulate(RowData accInput) throws Exception { long field$13; boolean isNull$13; isNull$13 = accInput.isNullAt(1); field$13 = -1L; if (!isNull$13) { field$13 = accInput.getLong(1); } udaf.accumulate(agg0_acc_external, isNull$13 ? null : ((Long) field$13)); } @Override public void retract(RowData retractInput) throws Exception { throw new RuntimeException(&quot;This function not require retract method, but the retract method is called.&quot;); } @Override public void merge(RowData otherAcc) throws Exception { throw new RuntimeException(&quot;This function not require merge method, but the merge method is called.&quot;); } @Override public void setAccumulators(RowData acc) throws Exception { RowData field$12; boolean isNull$12; isNull$12 = acc.isNullAt(0); field$12 = null; if (!isNull$12) { field$12 = acc.getRow(0, 1); } agg0_acc_internal = field$12; agg0_acc_external = (UDAF.Acc) converter$8.toExternal((RowData) agg0_acc_internal); } @Override public void resetAccumulators() throws Exception { agg0_acc_external = (UDAF.Acc) udaf.createAccumulator(); agg0_acc_internal = (RowData) converter$8.toInternalOrNull((UDAF.Acc) agg0_acc_external); } @Override public RowData getAccumulators() throws Exception { acc$11 = new GenericRowData(1); agg0_acc_internal = (RowData) converter$8.toInternalOrNull((UDAF.Acc) agg0_acc_external); if (false) { acc$11.setField(0, null); } else { acc$11.setField(0, agg0_acc_internal); } return acc$11; } @Override public RowData createAccumulators() throws Exception { acc$10 = new GenericRowData(1); RowData acc_internal$9 = (RowData) (RowData) converter$8.toInternalOrNull((UDAF.Acc) udaf.createAccumulator()); if (false) { acc$10.setField(0, null); } else { acc$10.setField(0, acc_internal$9); } return acc$10; } @Override public RowData getValue() throws Exception { aggValue$17 = new GenericRowData(1); Long value_external$14 = (Long) udaf.getValue(agg0_acc_external); Long value_internal$15 = value_external$14; boolean valueIsNull$16 = value_internal$15 == null; if (valueIsNull$16) { aggValue$17.setField(0, null); } else { aggValue$17.setField(0, value_internal$15); } return aggValue$17; } @Override public void cleanup() throws Exception {} @Override public void close() throws Exception { udaf.close(); }} 附录四12345tEnv.from(&quot;source&quot;) .groupBy($(&quot;a&quot;)) .flatAggregate(call(&quot;UDTAF&quot;, $(&quot;a&quot;)).as(&quot;value&quot;, &quot;rank&quot;)) .select($(&quot;value&quot;), $(&quot;rank&quot;)) .execute().print(); 上述UDTAF相关部分生成的代码.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201public final class GroupTableAggHandler$12 implements TableAggsHandleFunction { private transient UDTAF udtaf; private transient StructuredObjectConverter converter$2; GenericRowData acc$4 = new GenericRowData(1); GenericRowData acc$5 = new GenericRowData(1); private RowData agg0_acc_internal; private UDTAF.Acc agg0_acc_external; private transient StructuredObjectConverter converter$11; private StateDataViewStore store; private ConvertCollector convertCollector; public GroupTableAggHandler$12(java.lang.Object[] references) throws Exception { udtaf = (((UDTAF) references[0])); converter$2 = (((StructuredObjectConverter) references[1])); converter$11 = (((StructuredObjectConverter) references[2])); convertCollector = new ConvertCollector(references); } private RuntimeContext getRuntimeContext() { return store.getRuntimeContext(); } @Override public void open(StateDataViewStore store) throws Exception { this.store = store; udtaf.open(new FunctionContext(store.getRuntimeContext())); converter$2.open(getRuntimeContext().getUserCodeClassLoader()); converter$11.open(getRuntimeContext().getUserCodeClassLoader()); } @Override public void accumulate(RowData accInput) throws Exception { int field$7; boolean isNull$7; isNull$7 = accInput.isNullAt(0); field$7 = -1; if (!isNull$7) { field$7 = accInput.getInt(0); } udtaf.accumulate(agg0_acc_external, isNull$7 ? null : ((java.lang.Integer) field$7)); } @Override public void retract(RowData retractInput) throws Exception { throw new java.lang.RuntimeException(&quot;This function not require retract method, but the retract method is called.&quot;); } @Override public void merge(RowData otherAcc) throws Exception { throw new java.lang.RuntimeException(&quot;This function not require merge method, but the merge method is called.&quot;); } @Override public void setAccumulators(RowData acc) throws Exception { RowData field$6; boolean isNull$6; isNull$6 = acc.isNullAt(0); field$6 = null; if (!isNull$6) { field$6 = acc.getRow(0, 2); } agg0_acc_internal = field$6; agg0_acc_external = (UDTAF.Acc) converter$2.toExternal((RowData) agg0_acc_internal); } @Override public void resetAccumulators() throws Exception { agg0_acc_external = (UDTAF.Acc) udtaf.createAccumulator(); agg0_acc_internal = (RowData) converter$2.toInternalOrNull((UDTAF.Acc) agg0_acc_external); } @Override public RowData getAccumulators() throws Exception { acc$5 = new GenericRowData(1); agg0_acc_internal = (RowData) converter$2.toInternalOrNull((UDTAF.Acc) agg0_acc_external); if (false) { acc$5.setField(0, null); } else { acc$5.setField(0, agg0_acc_internal); } return acc$5; } @Override public RowData createAccumulators() throws Exception { acc$4 = new GenericRowData(1); RowData acc_internal$3 = (RowData) (RowData) converter$2.toInternalOrNull((UDTAF.Acc) udtaf.createAccumulator()); if (false) { acc$4.setField(0, null); } else { acc$4.setField(0, acc_internal$3); } return acc$4; } @Override public void emitValue(Collector&lt;RowData&gt; out, RowData key, boolean isRetract) throws Exception { convertCollector.reset(key, isRetract, out); udtaf.emitValue(agg0_acc_external, convertCollector); } @Override public void cleanup() throws Exception { } @Override public void close() throws Exception { udtaf.close(); } private class ConvertCollector implements Collector { private Collector&lt;RowData&gt; out; private RowData key; private utils.JoinedRowData result; private boolean isRetract = false; private transient UDTAF udtaf; private transient StructuredObjectConverter converter$2; GenericRowData acc$4 = new GenericRowData(1); GenericRowData acc$5 = new GenericRowData(1); private RowData agg0_acc_internal; private UDTAF.Acc agg0_acc_external; private transient StructuredObjectConverter converter$11; public ConvertCollector(java.lang.Object[] references) throws Exception { udtaf = (((UDTAF) references[0])); converter$2 = (((StructuredObjectConverter) references[1])); converter$11 = (((StructuredObjectConverter) references[2])); result = new utils.JoinedRowData(); } public void reset( RowData key, boolean isRetract, Collector&lt;RowData&gt; out) { this.key = key; this.isRetract = isRetract; this.out = out; } public RowData convertToRowData(Object recordInput$8) throws Exception { GenericRowData convertResult = new GenericRowData(2); RowData in1 = (RowData) (RowData) converter$11.toInternalOrNull((Tuple2) recordInput$8); int field$9; boolean isNull$9; int field$10; boolean isNull$10; isNull$9 = in1.isNullAt(0); field$9 = -1; if (!isNull$9) { field$9 = in1.getInt(0); } isNull$10 = in1.isNullAt(1); field$10 = -1; if (!isNull$10) { field$10 = in1.getInt(1); } if (isNull$9) { convertResult.setField(0, null); } else { convertResult.setField(0, field$9); } if (isNull$10) { convertResult.setField(1, null); } else { convertResult.setField(1, field$10); } return convertResult; } @Override public void collect(Object recordInput$8) throws Exception { RowData tempRowData = convertToRowData(recordInput$8); result.replace(key, tempRowData); if (isRetract) { result.setRowKind(RowKind.DELETE); } else { result.setRowKind(RowKind.INSERT); } out.collect(result); } @Override public void close() { out.close(); } }} 附录五1234567SELECT split_index(r, &apos;|&apos;, 0) AS y, split_index(r, &apos;|&apos;, 1) AS m, split_index(r, &apos;|&apos;, 2) AS dFROM ( SELECT UDF(b) AS r FROM source); 上述SQL在未改进前生成的代码.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163public class StreamExecCalc$19 extends TableStreamOperator implements OneInputStreamOperator { private final Object[] references; private transient StringDataSerializer typeSerializer$3; private transient UDF udf; private transient StringStringConverter converter$5; private final BinaryStringData str$8 = BinaryStringData.fromString(&quot;|&quot;); BoxedWrapperRowData out = new BoxedWrapperRowData(3); private final StreamRecord outElement = new StreamRecord(null); public StreamExecCalc$19( Object[] references, StreamTask task, StreamConfig config, Output output, ProcessingTimeService processingTimeService) throws Exception { this.references = references; typeSerializer$3 = (((StringDataSerializer) references[0])); udf = (((UDF) references[1])); converter$5 = (((StringStringConverter) references[2])); udf = (((UDF) references[3])); udf = (((UDF) references[4])); this.setup(task, config, output); if (this instanceof AbstractStreamOperator) { ((AbstractStreamOperator) this) .setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); udf.open(new org.apache.flink.table.functions.FunctionContext(getRuntimeContext())); converter$5.open(getRuntimeContext().getUserCodeClassLoader()); } @Override public void processElement(StreamRecord element) throws Exception { RowData in1 = (RowData) element.getValue(); BinaryStringData field$2; boolean isNull$2; BinaryStringData field$4; String externalResult$6; BinaryStringData result$7; boolean isNull$7; boolean isNull$9; BinaryStringData result$10; String externalResult$11; BinaryStringData result$12; boolean isNull$12; boolean isNull$13; BinaryStringData result$14; String externalResult$15; BinaryStringData result$16; boolean isNull$16; boolean isNull$17; BinaryStringData result$18; isNull$2 = in1.isNullAt(0); field$2 = BinaryStringData.EMPTY_UTF8; if (!isNull$2) { field$2 = ((BinaryStringData) in1.getString(0)); } field$4 = field$2; if (!isNull$2) { field$4 = (BinaryStringData) (typeSerializer$3.copy(field$4)); } out.setRowKind(in1.getRowKind()); // 第1次UDF调用 externalResult$6 = (String) udf .eval(isNull$2 ? null : ((String) converter$5.toExternal((BinaryStringData) field$4))); isNull$7 = externalResult$6 == null; result$7 = BinaryStringData.EMPTY_UTF8; if (!isNull$7) { result$7 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$6); } isNull$9 = isNull$7 || false || false; result$10 = BinaryStringData.EMPTY_UTF8; if (!isNull$9) { result$10 = BinaryStringData.fromString(SqlFunctionUtils.splitIndex(result$7.toString(),((BinaryStringData) str$8).toString(),((int) 0))); isNull$9 = (result$10 == null); } if (isNull$9) { out.setNullAt(0); } else { out.setNonPrimitiveValue(0, result$10); } // 第2次UDF调用 externalResult$11 = (String) udf .eval(isNull$2 ? null : ((String) converter$5.toExternal((BinaryStringData) field$4))); isNull$12 = externalResult$11 == null; result$12 = BinaryStringData.EMPTY_UTF8; if (!isNull$12) { result$12 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$11); } isNull$13 = isNull$12 || false || false; result$14 = BinaryStringData.EMPTY_UTF8; if (!isNull$13) { result$14 = BinaryStringData.fromString(SqlFunctionUtils.splitIndex(result$12.toString(),((BinaryStringData) str$8).toString(),((int) 1))); isNull$13 = (result$14 == null); } if (isNull$13) { out.setNullAt(1); } else { out.setNonPrimitiveValue(1, result$14); } // 第3次UDF调用 externalResult$15 = (String) udf .eval(isNull$2 ? null : ((String) converter$5.toExternal((BinaryStringData) field$4))); isNull$16 = externalResult$15 == null; result$16 = BinaryStringData.EMPTY_UTF8; if (!isNull$16) { result$16 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$15); } isNull$17 = isNull$16 || false || false; result$18 = BinaryStringData.EMPTY_UTF8; if (!isNull$17) { result$18 = BinaryStringData.fromString(SqlFunctionUtils.splitIndex(result$16.toString(),((BinaryStringData) str$8).toString(),((int) 2))); isNull$17 = (result$18 == null); } if (isNull$17) { out.setNullAt(2); } else { out.setNonPrimitiveValue(2, result$18); } output.collect(outElement.replace(out)); } @Override public void finish() throws Exception { super.finish(); } @Override public void close() throws Exception { super.close(); udf.close(); }} 附录六123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157public class StreamExecCalc$19 extends TableStreamOperator implements OneInputStreamOperator { private final Object[] references; private transient StringDataSerializer typeSerializer$3; private transient UDF udf; private transient StringStringConverter converter$5; private final BinaryStringData str$8 = BinaryStringData.fromString(&quot;|&quot;); BoxedWrapperRowData out = new BoxedWrapperRowData(3); private final StreamRecord outElement = new StreamRecord(null); public StreamExecCalc$19( Object[] references, StreamTask task, StreamConfig config, Output output, ProcessingTimeService processingTimeService) throws Exception { this.references = references; typeSerializer$3 = (((StringDataSerializer) references[0])); udf = (((UDF) references[1])); converter$5 = (((StringStringConverter) references[2])); udf = (((UDF) references[3])); udf = (((UDF) references[4])); this.setup(task, config, output); if (this instanceof AbstractStreamOperator) { ((AbstractStreamOperator) this).setProcessingTimeService(processingTimeService); } } @Override public void open() throws Exception { super.open(); udf.open(new org.apache.flink.table.functions.FunctionContext(getRuntimeContext())); converter$5.open(getRuntimeContext().getUserCodeClassLoader()); } @Override public void processElement(StreamRecord element) throws Exception { RowData in1 = (RowData) element.getValue(); BinaryStringData field$2; boolean isNull$2; BinaryStringData field$4; String externalResult$6; BinaryStringData result$7; boolean isNull$7; boolean isNull$9; BinaryStringData result$10; String externalResult$11; BinaryStringData result$12; boolean isNull$12; boolean isNull$13; BinaryStringData result$14; String externalResult$15; BinaryStringData result$16; boolean isNull$16; boolean isNull$17; BinaryStringData result$18; isNull$2 = in1.isNullAt(0); field$2 = BinaryStringData.EMPTY_UTF8; if (!isNull$2) { field$2 = ((BinaryStringData) in1.getString(0)); } field$4 = field$2; if (!isNull$2) { field$4 = (BinaryStringData) (typeSerializer$3.copy(field$4)); } out.setRowKind(in1.getRowKind()); externalResult$6 = (String) udf .eval(isNull$2 ? null : ((String) converter$5.toExternal((BinaryStringData) field$4))) ; isNull$7 = externalResult$6 == null; result$7 = BinaryStringData.EMPTY_UTF8; if (!isNull$7) { result$7 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$6); } isNull$9 = isNull$7 || false || false; result$10 = BinaryStringData.EMPTY_UTF8; if (!isNull$9) { result$10 = BinaryStringData.fromString(SqlFunctionUtils.splitIndex(result$7.toString(),((BinaryStringData) str$8).toString(),((int) 0))); isNull$9 = (result$10 == null); } if (isNull$9) { out.setNullAt(0); } else { out.setNonPrimitiveValue(0, result$10); } // 复用第一次计算结果 externalResult$11 = externalResult$6; isNull$12 = externalResult$11 == null; result$12 = BinaryStringData.EMPTY_UTF8; if (!isNull$12) { result$12 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$11); } isNull$13 = isNull$12 || false || false; result$14 = BinaryStringData.EMPTY_UTF8; if (!isNull$13) { result$14 = BinaryStringData.fromString(SqlFunctionUtils.splitIndex(result$12.toString(),((BinaryStringData) str$8).toString(),((int) 1))); isNull$13 = (result$14 == null); } if (isNull$13) { out.setNullAt(1); } else { out.setNonPrimitiveValue(1, result$14); } // 复用第一次计算结果 externalResult$15 = externalResult$6; isNull$16 = externalResult$15 == null; result$16 = BinaryStringData.EMPTY_UTF8; if (!isNull$16) { result$16 = (BinaryStringData) converter$5.toInternalOrNull((String) externalResult$15); } isNull$17 = isNull$16 || false || false; result$18 = BinaryStringData.EMPTY_UTF8; if (!isNull$17) { result$18 = BinaryStringData.fromString(SqlFunctionUtils.splitIndex(result$16.toString(),((BinaryStringData) str$8).toString(),((int) 2))); isNull$17 = (result$18 == null); } if (isNull$17) { out.setNullAt(2); } else { out.setNonPrimitiveValue(2, result$18); } output.collect(outElement.replace(out)); } @Override public void finish() throws Exception { super.finish(); } @Override public void close() throws Exception { super.close(); udf.close(); }}","link":"/flink-sql-functions.html"},{"title":"Flink SQL源码 - Mini-Batch原理与实现","text":"本文已收录在合集Apche Flink原理与实践中. Mini-Batch是在进行有状态流处理时的一种重要优化手段, 通过在内存中攒批后再处理可以降低State访问次数, 从而提升吞吐量降低CPU资源使用. 目前, Flink SQL已经在多个场景中支持了Mini-Batch优化, 本文首先介绍Flink SQL的Mini-Batch实现原理, 在此基础上通过相关案例进一步介绍具体实现. 什么是Mini-Batch在Flink的有状态算子中, 为了保证状态的一致性, 每次操作都需要将状态保存到状态后端中, 由框架来执行Checkpoint. 然而目前RocksDB等状态的读写需要对记录进行序列化和反序列化, 甚至需要IO操作. 在这种情况下, 有状态算子很可能会成为整个作业的性能瓶颈. 为此, 对于时效性要求不是特别高, 计算状态复杂的作业可以引入Mini-Batch. 其核心思想是在内存中缓存一小批数据, 然后整批数据一起触发计算, 这样在涉及到状态访问时, 同一个Key的状态只需进行一次读写操作, 从而极大降低状态访问操作的开销, 提升作业性能. 以下是一个通过Mini-Batch进行Group Aggregate的案例(图片来自Flink文档), 通过攒批可以将4条记录的状态访问从8次减少到2次. 在Flink SQL中, 以下场景都已经支持了MiniBatch: Group Aggregate Top-N / Deduplicate 基于Window TVF的Window Aggregate(仅有Mini-Batch实现) Regular Join(在后序介绍Regular Join的文章中我们还会进一步展开) 在Flink SQL中可通过设置以下参数启用Mini-Batch.123456789// instantiate table environmentTableEnvironment tEnv = ...;// access flink configurationTableConfig configuration = tEnv.getConfig();// set low-level key-value optionsconfiguration.set(&quot;table.exec.mini-batch.enabled&quot;, &quot;true&quot;); // enable mini-batch optimizationconfiguration.set(&quot;table.exec.mini-batch.allow-latency&quot;, &quot;5 s&quot;); // use 5 seconds to buffer input recordsconfiguration.set(&quot;table.exec.mini-batch.size&quot;, &quot;5000&quot;); // the maximum number of records can be buffered by each aggregate operator task Mini-Batch攒批策略Mini-Batch带来的性能提升取决于攒批的效果, 最简单的攒批方式是在需要攒批的算子上创建一个定时器, 但是这种方式对复杂的计算作业有极大限制. 举例来说, 假设用户可容忍的最大延时是10秒, 理想的攒批策略是攒10秒的数据, 然而在上述攒批方式中, 如果计算图中包含两层聚合, 那么为了保证端到端延时在10秒以内, 每层聚合最多只能攒5秒的数据, 对于聚合更多的计算图, 这种退化将更加明显. Flink在实现攒批时采用了一种更为巧妙的方法, 即使用Watermark作为批次分界. 实现上, 对于不存在Event Time的计算图, 直接在Source后添加一个MiniBatchAssigner算子, 以用户设定的延时为时间间隔下发Watermark即可; 对于存在Event Time的计算图, 为了更好的攒批, 需要将原来的Watermark按Mini-Batch时间间隔进行过滤, 仅下发能够触发每个Mini-Batch窗口的最小Watermark, 例如对于一个Watermark序列[1, 3, 7, 8, 14], 如果Mini-Batch时间间隔为5秒, 那么只需下发[7, 14]两个Watermark即可. 实现上只需在WatermarkAssigner之后新增一个MiniBatchAssigner做相应的Watermark过滤即可. 使用Watermark作为批次分界有一些明显的好处. 首先, Watermark在Flink内部已经有完善的实现机制, 通过Watermark攒批实现简单; 其次, 定时器方案会同时触发所有算子计算, 容易引起反压, 而Watermark是流动的, 攒批算子的计算是异步触发, 降低了反压风险; 最后, 使用Watermark作为批次分界还能避免数据抖动, 这种情况一般发生在两层聚合的场景下, 当内层更新时会下发-U, +U两条记录, 外层会分别处理这两条记录, 又分别下发两条-U, +U记录, 这就导致用户最终看到的结果会有一个突变的过程. 以两层聚合进行每日UV统计为例, SQL代码如下(笔者提供了可运行示例Example), 当源端收到11,2023-12-19这条记录时: 内层聚合会下发-U[2023-12-19,1,1], +U[2023-12-19,1,2]两条记录; 当外层聚合收到-U[2023-12-19,1,1]这条记录时会下发-U[2023-12-19,2], +U[2023-12-19,1]这两条记录, 此时2023-12-19这一天的UV值由2变为1. 当外层聚合收到+U[2023-12-19,1,2]这条记录时会下发-U[2023-12-19,1], +U[2023-12-19,3]这两条记录, 此时2023-12-19这一天的UV值由1变为3. 123456789101112131415161718192021222324252627282930313233343536373839/*输入记录1,2023-12-192,2023-12-1911,2023-12-19*/CREATE TABLE source ( user_id INT, `day` VARCHAR) WITH (...)/*关闭Mini-Batch时输出+----+--------------------------------+----------------------+| op | day | total |+----+--------------------------------+----------------------+| +I | 2023-12-19 | 1 || -U | 2023-12-19 | 1 || +U | 2023-12-19 | 2 || -U | 2023-12-19 | 2 || +U | 2023-12-19 | 1 || -U | 2023-12-19 | 1 || +U | 2023-12-19 | 3 |开启Mini-Batch时输出+----+--------------------------------+----------------------+| op | day | total |+----+--------------------------------+----------------------+| +I | 2023-12-19 | 1 || -U | 2023-12-19 | 1 || +U | 2023-12-19 | 2 || -U | 2023-12-19 | 2 || +U | 2023-12-19 | 3 |*/SELECT `day`, SUM(cnt) totalFROM ( SELECT `day`, MOD(user_id, 10), COUNT(DISTINCT user_id) as cnt FROM source GROUP BY `day`, MOD(user_id, 10))GROUP BY `day`; 可以看到, 上述案例在统计UV值时出现了先变小后变大的过程, 尽管可以保证最终一致性, 但是下游很可能会读到一个中间值. 这一问题实际上是由前文所说的, Flink内部把UPDATE拆分为了UPDATE_BEFORE和UPDATE_AFTER两条记录所导致的. 如果用数据库中的事务来考虑, 这一拆分实际上导致了原本应该被原子执行的一个事务被分开执行了, 在ACID所定义的隔离级别中, 只能满足读未提交, 也就是说可能读到一个未提交事务的中间结果. 那为什么开启Mini-Batch之后就能避免这一问题呢? 这是由于Flink中的Watermark不会出现在UPDATE_BEFORE和UPDATE_AFTER之间, 而Watermark之间的整批数据都是一起计算的, 这样Watermark就相当于一个触发事务提交的操作, 这中间的计算对外部来说是原子的. Table Planner处理流程在物理关系代数(即StreamPhysicalRel)优化阶段, Table Planner会推导出当前计算图中的Mini-Batch时间间隔, 并在相应位置附上MiniBatchAssigner算子. 在Transformation图生成阶段, 会在StreamExecNode.translateToPlanInternal()中根据是否启用Mini-Batch选择对应的StreamOperator实现. 相关概念MiniBatchIntervalMiniBatchInterval包含了Mini-Batch相关的参数, 其中interval的初始值是table.exec.mini-batch.allow-latency配置的最大容忍延时, mode表示事件时间或处理事件, 初始值是ProcTime.123456789101112public class MiniBatchInterval { /** interval of mini-batch. */ private final long interval; /** The type of mini-batch: rowtime/proctime. */ private final MiniBatchMode mode;}public enum MiniBatchMode { ProcTime, RowTime, None} MiniBatchIntervalTraitMiniBatchIntervalTrait是Calcite中RelTrait的一种实现, 用于标识StreamPhysicalRel节点的Mini-Batch属性. 其内部持有一个MiniBatchInterval实例, 可通过getMiniBatchInterval()方法获取. 关系代数优化阶段在关系代数优化阶段, Mini-Batch的相关处理在physical_rewrite阶段进行. 在优化过程中: 首先会通过FlinkMiniBatchIntervalTraitInitProgram为每个StreamPhysicalRel初始化MiniBatchIntervalTrait属性, 其MiniBatchInterval的初始值在StreamCommonSubGraphBasedOptimizer.doOptimize()中确定. MiniBatchIntervalTrait的推导和StreamPhysicalMiniBatchAssigner节点的添加在MiniBatchIntervalInferRule中. 下面我们通过几个示例来具体介绍MiniBatchIntervalTrait的推导流程. Processing Time Mini-Batch推导关于Processing Time的Mini-Batch推导我们还是以文章开头的两层聚合统计每日UV为例. 它的推导流程比较简单, 经过FlinkMiniBatchIntervalTraitInitProgram初始化后, 每个StreamPhysicalRel的MiniBatchIntervalTrait都是ProcTime: 5000(即Processing Time模式, 最大延时5000毫秒). MiniBatchIntervalInferRule自上而下合并各个节点的MiniBatchIntervalTrait, 直到最后一个StreamPhysicalCalc由于输入节点是StreamPhysicalTableSourceScan, 因此会插入一个StreamPhysicalMiniBatchAssigner节点. 推导前后的关系代数结构变化如下所示.123456789101112131415161718StreamPhysicalSink(table=[*anonymous_collect$1*], fields=[day, total]) StreamPhysicalGroupAggregate(groupBy=[day], select=[day, SUM_RETRACT(cnt) AS total]) StreamPhysicalExchange(distribution=[hash[day]]) StreamPhysicalCalc(select=[day, cnt]) StreamPhysicalGroupAggregate(groupBy=[day, EXPR$1], select=[day, EXPR$1, COUNT(DISTINCT user_id) AS cnt]) StreamPhysicalExchange(distribution=[hash[day, EXPR$1]]) StreamPhysicalCalc(select=[day, MOD(user_id, 10) AS EXPR$1, user_id]) StreamPhysicalTableSourceScan(table=[[default_catalog, default_database, source]], fields=[user_id, day])StreamPhysicalSink(table=[*anonymous_collect$1*], fields=[day, total]) StreamPhysicalGroupAggregate(groupBy=[day], select=[day, SUM_RETRACT(cnt) AS total]) StreamPhysicalExchange(distribution=[hash[day]]) StreamPhysicalCalc(select=[day, cnt]) StreamPhysicalGroupAggregate(groupBy=[day, EXPR$1], select=[day, EXPR$1, COUNT(DISTINCT user_id) AS cnt]) StreamPhysicalExchange(distribution=[hash[day, EXPR$1]]) StreamPhysicalCalc(select=[day, MOD(user_id, 10) AS EXPR$1, user_id]) StreamPhysicalMiniBatchAssigner(interval=[5000ms], mode=[ProcTime]) StreamPhysicalTableSourceScan(table=[[default_catalog, default_database, source]], fields=[user_id, day]) Event Time Mini-Batch推导在Event Time场景中, 我们来看一个更加复杂的例子, 其计算图结构如下所示, 在Deduplicate之后存在两个Tumbling窗口聚合操作, 其中一个窗口大小为5s, 另一个为10s. 此时, 若table.exec.mini-batch.allow-latency所设延时为15s, 那么最终MiniBatchAssigner的延迟应该为多少呢? 目前Flink Table Planner的实现中, 并没有考虑窗口大小对Mini-Batch的影响, 也就是说上述案例的MiniBatchAssigner延迟为15s(读者可运行MiniBatchEventTimeExample获得更直观的感受). 从语义上来说并没有问题, 因为用户所能容忍的最大延时是15s. 但是, 如果MiniBatchAssigner的延时远大于窗口大小, 那么会导致在某一时刻触发大量窗口计算. 在这种情况下, 更合理的方案应该是以窗口大小的最大公约数为MiniBatchAssigner延迟(上述案例中即为5s). Transformation图生成阶段在Transformation图生成阶段与Mini-Batch相关的有两个方面: 一是需要将StreamPhysicalMiniBatchAssigner转换为对应的StreamOperator实现, 这一步在StreamExecMiniBatchAssigner中实现. 二是需要将对应支持Mini-Batch的算子转换Mini-Batch实现, 这一步在算子的StreamExecNode.translateToPlanInternal()中实现. 下文中我们以Group Aggregate为例, 具体介绍攒批算子的实现. MiniBatchAssginerOperator实现MiniBatchAssginerOperator在Event Time和Processing Time场景下有不同的实现. Event Time场景下的实现在RowTimeMiniBatchAssginerOperator中, 主要代码如下, 核心逻辑就是在processWatermark()中拦截上游产生的Watermark, 对其进行过滤, 最终仅下发能否出发Mini-Batch批次的最小Watermark.1234567891011121314151617181920212223242526272829303132public class RowTimeMiniBatchAssginerOperator extends AbstractStreamOperator&lt;RowData&gt; implements OneInputStreamOperator&lt;RowData, RowData&gt; { @Override public void processWatermark(Watermark mark) throws Exception { // if we receive a Long.MAX_VALUE watermark we forward it since it is used // to signal the end of input and to not block watermark progress downstream if (mark.getTimestamp() == Long.MAX_VALUE &amp;&amp; currentWatermark != Long.MAX_VALUE) { currentWatermark = Long.MAX_VALUE; output.emitWatermark(mark); return; } currentWatermark = Math.max(currentWatermark, mark.getTimestamp()); // 仅当当前Watermark大于下一个批次窗口触发时间时推进 if (currentWatermark &gt;= nextWatermark) { advanceWatermark(); } } private void advanceWatermark() { output.emitWatermark(new Watermark(currentWatermark)); // 计算Mini-Batch批次的开始和结束时间 long start = getMiniBatchStart(currentWatermark, minibatchInterval); long end = start + minibatchInterval - 1; nextWatermark = end &gt; currentWatermark ? end : end + minibatchInterval; } private static long getMiniBatchStart(long watermark, long interval) { return watermark - (watermark + interval) % interval; }} Processing Time场景下的实现在ProcTimeMiniBatchAssignerOperator中, 主要代码如下. 在processElement()会判断当前时间是否可以出发一个Batch, 如果可以则会下发一个Watermark. 如果没有持续的数据输入, onProcessingTime()会保证每隔一个Mini-Batch窗口下发一个Watermark.12345678910111213141516171819202122232425262728public class ProcTimeMiniBatchAssignerOperator extends AbstractStreamOperator&lt;RowData&gt; implements OneInputStreamOperator&lt;RowData, RowData&gt;, ProcessingTimeService.ProcessingTimeCallback { @Override public void processElement(StreamRecord&lt;RowData&gt; element) throws Exception { long now = getProcessingTimeService().getCurrentProcessingTime(); long currentBatch = now - now % intervalMs; if (currentBatch &gt; currentWatermark) { currentWatermark = currentBatch; // emit output.emitWatermark(new Watermark(currentBatch)); } output.collect(element); } @Override public void onProcessingTime(long timestamp) throws Exception { long now = getProcessingTimeService().getCurrentProcessingTime(); long currentBatch = now - now % intervalMs; if (currentBatch &gt; currentWatermark) { currentWatermark = currentBatch; // emit output.emitWatermark(new Watermark(currentBatch)); } getProcessingTimeService().registerTimer(currentBatch + intervalMs, this); }} 攒批计算Group Aggregate的攒批计算是由AbstractMapBundleOperator和MapBundleFunction配合完成的, Top-N和Deduplicate的实现也类似, 只不过有各自具体的实现. 不过基于Window TVF实现的Window Aggregate有一套独立的攒批逻辑, 具体在后续文章中进一步介绍. AbstractMapBundleOperator继承自StreamOperator, 它有两个实现类, 分别用于keyed-stream和non-keyed-stream. 实际上核心的处理逻辑都实现在AbstractMapBundleOperator中. MapBundleFunction有三个实现类与Group Aggregate相关, 其中MiniBatchGroupAggFunction用在未开启Local-Global Aggregation的场景; MiniBatchLocalGroupAggFunction用在Local聚合阶段; MiniBatchGlobalGroupAggFunction用在Global聚合阶段. 下面我们介绍在不开启Local-Global优化时的实现, 其核心代码如下.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public abstract class AbstractMapBundleOperator&lt;K, V, IN, OUT&gt; extends AbstractStreamOperator&lt;OUT&gt; implements OneInputStreamOperator&lt;IN, OUT&gt;, BundleTriggerCallback { @Override public void processElement(StreamRecord&lt;IN&gt; element) throws Exception { // get the key and value for the map bundle final IN input = element.getValue(); final K bundleKey = getKey(input); final V bundleValue = bundle.get(bundleKey); // get a new value after adding this element to bundle // function是MiniBatchGroupAggFunction实例 final V newBundleValue = function.addInput(bundleValue, input); // update to map bundle // bundle是一个内存Map, 用于在内存中保存整批数据 bundle.put(bundleKey, newBundleValue); numOfElements++; // bundleTrigger是一个CountBundleTrigger实例, // 当批次数据条数达到table.exec.mini-batch.size所设值时便会触发finishBundle()计算 bundleTrigger.onElement(input); } @Override public void finishBundle() throws Exception { if (bundle != null &amp;&amp; !bundle.isEmpty()) { numOfElements = 0; function.finishBundle(bundle, collector); bundle.clear(); } bundleTrigger.reset(); } @Override public void processWatermark(Watermark mark) throws Exception { // 在处理Watermark时也触发计算, 因为Watermark是批次分界 finishBundle(); super.processWatermark(mark); }}public class MiniBatchGroupAggFunction extends MapBundleFunction&lt;RowData, List&lt;RowData&gt;, RowData, RowData&gt; { @Override public List&lt;RowData&gt; addInput(@Nullable List&lt;RowData&gt; value, RowData input) throws Exception { List&lt;RowData&gt; bufferedRows = value; if (value == null) { bufferedRows = new ArrayList&lt;&gt;(); } // input row maybe reused, we need deep copy here bufferedRows.add(inputRowSerializer.copy(input)); return bufferedRows; } @Override public void finishBundle(Map&lt;RowData, List&lt;RowData&gt;&gt; buffer, Collector&lt;RowData&gt; out) throws Exception { // 这里的实现逻辑与GroupAggFunction.processElement()逻辑类似 // 在之前的博文中已经介绍过(https://liebing.org.cn/flink-sql-changelog.html#Transform%E5%9B%BE%E7%94%9F%E6%88%90%E9%98%B6%E6%AE%B5) ... }} 总结目前Flink SQL已经在多个有状态计算场景中引入了Mini-Batch来减少计算过程中对状态的访问次数, 实际上Mini-Batch在无状态计算中也能发挥作用, 对于无状态算子, 可以通过Mini-Batch攒批后使用数据库中常用的向量化计算技术进行计算, 从而减少作业所需的CPU资源. 总结来说, Mini-Batch除了会增加一定的延时, 存在诸多的好处: 对于有状态算子可以减少状态访问, 从而极大提升作业吞吐量; 防止数据抖动, 同时减少下游存储写入压力; 在Regular Join中可减少回撤, 从而提升双流及多流Join的性能. 参考[1] Flink Documentation - Table API &amp; SQL - Performance Tuning","link":"/flink-sql-minibatch.html"},{"title":"论文阅读 - Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores","text":"本文已收录在合集数据系统经典论文阅读中. 本文是对Databricks的Delta Lake论文(Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores)的阅读总结. Delta Lake是前文所说的Lakehouse架构中的数据湖上的表格存储实现. 论文详细描述了Delta Lake的产生背景, 原理以及特征. 阅读论文不仅可以了解Delta Lake的实现原理, 还有助于了解其他表格存储实现, 如Apache Iceberg和Apache Hudi. 本文不会对论文进行完整的翻译, 而是按如下主线剖析论文的核心观点和内容, 并穿插笔者的见解: Delta Lake的产生背景. 对象存储的特性和挑战. Delta Lake的存储格式和访问协议. Delta Lake的产生背景云对象存储(Cloud Object Stores), 比如Amazon S3, Azure Blob Storage和阿里云OSS等, 具有极高的可靠性, 海量的存储空间以及低廉的价格. 除了云服务的传统优点, 云对象存储更重要的特性是支持存储与计算分离. 由于这些优点, 云对象存储很适合用来作为数据仓库(Data Warehouse)和数据湖(Data Lake)的存储底座. 目前开源的大数据系统, 如Spark, Hive, Presto, 以及商业服务, 如AWS Athena, Google BigQuery, Redshift Spectrum, 都支持使用Apache Parquet和ORC文件格式读写云对象存储. 不幸的是, 尽管许多系统支持对云对象存储的读写, 但借助这些系统和云对象存储并不能实现高效和可变的表格存储, 也就导致了难以在它们之上实现数据仓库功能. 这是因为与HDFS等分布式文件系统或DBMS中的自定义存储引擎不同, 大多数云对象存储仅仅是键值存储, 没有跨键一致性保证. 另外它们的性能特征也与分布式文件系统有很大不同. 目前在云对象存储中存储关系数据集最常见的方法是使用列式文件格式, 如Parquet和ORC, 其中每个表作为一组对象(Parquet或ORC文件)存储, 可能按某些字段聚集成分区(例如, 每个日期的单独一组对象). 只要目标文件比较大, 这种方法可以为扫描工作负载提供可接受的性能. 但是, 在如下几种复杂的场景中就不行了: 由于云对象存储中多对象更新不是原子的, 所以多个查询之间没有隔离: 例如, 如果一个查询需要更新表中的多个对象(例如, 删除表中所有Parquet文件中关于一个用户的记录), 其他查询将看到部分更新, 因为查询单独更新每个对象. 回滚写操作也很困难:如果更新查询崩溃, 表就处于部分更新的不一致状态. 对于拥有数百万个对象的大表, 元数据操作的开销很大. 例如, Parquet文件包括带有最小/最大统计信息的页脚, 可用于在选择性查询中跳过读取它们. 在HDFS上读取这样的页脚可能需要几毫秒, 但云对象存储的延迟要高得多, 这些数据跳过检查可能比实际查询花费更长的时间. 总结来说, 用云对象存储作为存储底座来实现数据仓库有两个重要的问题, 一是不支持事务, 二是小文件导致的性能问题. Delta Lake核心其实就是在云对象存储上引入了一个新的中间层, 来解决上述问题. 为了解决上述问题, Databricks设计了Delta Lake(2017年开始供客户使用, 2019年开源), 它是云对象存储之上具有ACID特性的表格存储层. 其核心思想十分简单: 使用本身存储在云对象存储中的预写日志, 以ACID的方式维护关于哪些对象是Delta表的一部分的信息. 也就是说Delta Lake除了原表之外, 还新增了一种预写日志(称为Transaction Log), 并以ACID的方式来维护, 这样就可以支持事务. 关于Transaction Log的具体内容, 下文会详细分析. 有了事务特性之后, Delta Lake还可以支持许多传统数据湖无法支持的特性, 比如: 时间旅行(Time travel) UPSERT, DELETE和MERGE操作(UPSERT, DELETE and MERGE operations) 高效的流式I/O(Effificient streaming I/O) 缓存(Caching): 因为Delta表及其日志中的对象是不可变的, 所以计算集群节点可以安全地将它们缓存在本地存储中. Databricks云服务中利用它来为Delta表实现一个透明的SSD缓存. 数据分布优化(Data layout optimization): 可以自动优化表中对象的大小和数据记录的聚集性(例如, 使用Z-order存储记录, 以实现多个维度的局部性), 而不会影响运行的查询. Schema演化(Schema evolution) 审计日志(Audit logging) 上述特性提高了数据湖的可管理性和性能, 使Lakehouse架构成为了现实. 更重要的是Delta Lake还能进一步简化整体的数据平台架构, 在存储上实现流批一体. 下图是一个案例, 图(a)中为了实现流式分析, BI和数据科学应用, 采用了消息队列, 数据湖和数据仓库系统三种组件, 引入Delta Lake之后可以仅使用这一组件来实现上述所有应用. Delta Lake的特性使其可以在一定程度上实现流批一体, 论文中也说到了在流处理中其延迟是秒级的, 如果需要更低延迟的流处理, 可能还是得借助消息总线. 不过笔者相信流批一体是未来的一种趋势, 随着技术的进步未来更多的系统将会更好地兼顾流处理和批处理. 目前来说, 在存储方面有两种实现流批一体的思路, 其一是使用文件类型的存储来实现流批一体, 即类似Delta Lake这样数据湖上的表格存储; 另一种是使用消息队列来实现流批一体, 即类似Pulsar这样的消息队列. 除了存储之外, 还有计算的流批一体, 这里不再展开. 关于流批一体更详细的内容可参考浅谈大数据的过去, 现在和未来. 对象存储的特性之所以在对象存储之上实现高效的表格存储比较困难, 是因为对象存储的几个特性. 对象存储的API 云对象存储都提供一个简单易扩展的键值存储API. 用户可以创建Bucket来存储多个Object, 每个Object都是一个二进制的Blob, 其大小最大一般限制在几个TB. 对象存储的键可以对应文件系统的路径, 但是与文件系统不同的是云对象存储更改键名(即文件路径)的代价较高. 云对象存储也提供了元数据API, 比如S3的LIST操作可以根据给定的起始键, 按字典顺序列出Bucket中的Object. 不幸的是这些API大多比较昂贵, 比如S3的LIST操作每次仅返回1000个对象, 需要耗时几百毫秒, 如果表的文件数量较多就非常低效了. 在读取Object时, 云对象存储支持按字节寻址, 并且支持高效的顺序读. 这使得利用特定存储格式(如Z-order)来聚集经常访问的值成为可能. 在更新Object时, 需要一次性重写整个Object. 这些更新是具有原子性, 用户要么读到更新前的版本, 要么读到更新后的版本. 有些系统也支持追加写操作. 一致性特性 当前流行的云对象存储为每个键提供最终一致性, 但不能保证跨键一致性, 这给管理由多个对象组成的数据集带来了挑战. 比如, 在客户端上传一个新对象之后, 其他客户端不一定能在LIST中看到该对象或立即进行读取操作. 同样, 对现有对象的更新可能不会立即对其他客户端可见. 此外, 根据对象存储的不同, 即使是进行写操作的客户端自身也可能不会立即看到新对象. 云对象存储确切的一致性模型因云提供商而异, 可能相当复杂. 举一个具体的例子, Amazon S3为写入新对象的客户端提供了写后读的一致性, 这意味着读取操作(如S3的GET)将在PUT之后返回对象内容. 但是有一个例外:如果写入对象的客户端在其PUT之前向(不存在的)键发出GET, 那么后续的GET可能在一段时间内不会读取对象, 这很可能是因为S3使用了负缓存. 另外, S3的LIST操作总是最终一致性的, 这也意味着PUT之后的LIST未必会返回新对象. 性能特性 每个读操作通常会产生至少5-10毫秒的基本延迟, 然后可以以大约50-100MB/s的速度读取数据, 因此一个操作需要至少读取几百KB才能达到顺序读的峰值吞吐量的一半, 并需要多个MB才能接近峰值吞吐量. 此外, 在典型的VM配置中, 应用程序需要并行运行多个读取, 以最大化吞吐量. 例如, 在AWS上最常用于分析的VM类型至少有10Gbps的网络带宽, 因此它们需要并行运行8-10次读取以充分利用这一带宽. S3的LIST操作每个请求最多只能返回1000个对象, 并且需要数十到数百毫秒的时间, 因此客户端需要并行发出数百个LIST来列出大的Bucket下的所有对象. 在DeltaLake中, 关于可用对象的元数据(包括它们的名称和数据统计信息)存储在Delta日志中(为了避免使用LIST). 写操作通常必须替换整个对象(或追加). 这意味着, 如果一个表希望进行点更新, 那么表中的对象应该保持较小, 这与支持大型读取相悖. 另一种方法是使用日志结构的存储格式. 云对象存储的上述性能特性, 对分析性作业有以下指导原则: 将频繁访问的数据按顺序存放在一起, 比如采用列式存储或Z-order排列. 使用大对象, 但不要太大. 大对象增加了更新数据的成本(例如, 删除一个用户的所有数据), 因为它们必须完全重写. 避免LIST操作, 尽可能让这些操作请求按字典顺序排列的键区间. 现有表格存储方法基于对象存储的这些特性, 目前有三种主要的方法在对象存储上管理表数据. 文件目录(Directories of Files): 开源大数据技术栈和许多云服务支持的最常见的方法是将表存储为对象的集合, 通常采用列式存储(如Parquet). 这些对象可进一步按日期或其他字段进行分区. 这种方法源自Hive在HDFS上管理关系型数据. 但是这种方法在云对象存储中有性能和事务问题: 没有跨多个对象的原子性: 任何需要写入或更新多个对象的事务都有部分写入对其他客户端可见的风险. 此外, 如果这样的事务失败, 数据将处于损坏状态. 最终一致性: 即使事务成功了, 客户端也可能看部分更新. 性能差: 列出对象以查找与查询相关的对象的开销很大, 即使它们按键分区到目录中. 此外, 访问存储在Parquet或ORC文件中的每个对象的统计数据非常昂贵, 因为它需要对每个特性进行额外的高延迟读取. 没有管理功能: 对象存储没有实现数据仓库中常见的标准实用程序, 如表版本控制或审计日志. 自定义存储引擎(Custom Storage Engines):比如Snowflake的数据仓库系统通过在一个独立的, 强一致性的服务中管理元数据本身, 可以绕过云对象存储的许多一致性挑战, 该服务拥有关于哪些对象组成一个表的”source of truth”. 在这些引擎中, 云对象存储可以被视为一个简单的块设备. 这类引擎也面临以下一些挑战: 对一个表的所有I/O操作都需要访问元数据服务, 这可能会增加其资源成本, 降低性能和可用性. 与重用现有开放格式(如Parquet)的方法相比, 计算引擎的连接器需要更多的工程工作来实现. 专有元数据服务将用户绑定到特定的服务提供商, 而基于直接访问云存储中的对象的方法使用户始终可以使用不同的技术访问其数据. 在对象存储中维护元数据(Metadata in Object Stores):DeltaLake的方法是在云对象存储中直接存储事务日志和元数据, 并使用一组特定协议来实现可序列化. Apache Hudi和Apache Iceberg都采用了相同的方式. Delta Lake的存储格式和访问协议DeltaLake表是云对象存储或文件系统上的一个目录, 它保存包含表内容的数据对象和事务操作日志(偶尔有检查点). 客户端使用针对云对象存储特性定制的乐观并发控制协议更新这些数据结构. 存储格式Delta表的存储格式如下图所示. 表的内容以Parquet文件进行存储, 可以像Hive那样进行分区. 例如下图中的表按date字段进行了分区, 同一天的数据在同一文件夹下. Delta表中的数据对象都有一个唯一的名称(GUID), 哪个对象属于表的哪个版本由事务日志决定. _delta_log文件夹下存储的是事务日志. 它包含一些列按序增长的JSON文件, 以及Checkpoint文件. 每个日志记录对象(例如000003.json)包含一个操作数组, 用于应用于表的上一个版本, 以生成下一个版本. 可用的操作有: metaData: metaData操作更改表的当前元数据. 表的第一个版本必须包含一个metaData操作. add和remove: add用于向Delta表中添加对象, 也可以包含数据的统计值, 如每列的最大最小值以及null值的数量等. remove用于删除对象, 它包含一个时间戳用于指示删除发生的时间. 物理的删除是惰性的, 可由用户指定删除延迟. protocol: protocol操作用于增加表的版本(在读或写时需要). 该操作还可以用于添加新的特性. commitInfo: commitInfo操作中包含来源信息, 用于指示哪个用户执行了操作. Update Application Transaction IDs: Delta Lake允许用户写入自定义的txn操作, 包含appId和version. 这一特性可用于在流计算中实现Exactly-Once语义, 比如将源的Offset作为version写入, 这样就能在流计算作业崩溃时进行恢复. 为了提升性能, Delta Lake会合并日志文件, 比如下图中的000003.parquet是前三个日志文件合并的结果. _last_checkpoint中记录了最新的Checkpoint ID. 这样客户端在读取时就不需要用LIST列举日志文件, 只需读取最新的Checkpoint文件即可. 访问协议Delta Lake的访问协议旨在让客户端只使用对象存储上的操作来实现可序列化的事务, 尽管对象存储只保证最终一致性. 使之成为可能的关键选择是使用日志作为根数据结构, 例如000003.json, 客户机需要知道它才能读取表的特定版本. 有了日志的内容, 客户端就可以从对象存储中查询其他对象, 如果它们由于最终一致性还不可见, 则可能会等待, 然后读取表数据. 对于执行写操作的事务, 客户端需要一种方法来确保只有一个写入者可以创建下一个日志记录, 然后可以使用这种方法来实现乐观并发控制. 只读事务Delta Lake的只读事务支持客户端安全地读取表的某一个版本, 其执行步骤如下: 读取表的日志目录中的_last_checkpoint对象(如果存在的话), 以获得最近的检查点ID. 使用LIST操作以最后一个检查点ID(如果存在, 否则为0)为开始键, 在表的日志目录中查找任何更新的.json和.parquet文件. 这提供了一个列表文件, 可用于从最近的检查点开始重建表的状态. (注意, 由于云对象存储的最终一致性, 这个LIST操作可能返回一个不连续的对象集, 例如000004. json和000006.json, 而不包含000005.json. 尽管如此, 客户机可以使用返回的最大ID作为目标表版本进行读取, 并等待丢失的对象变得可见.) 使用检查点(如果存在)和上一步中标识的后续日志记录来重构表的状态——即有add记录但没有相应remove记录的数据对象集, 以及它们相关的数据统计信息. Delta Lake的设计使得这个任务可以并行运行: 例如, 在Spark连接器中, 使用Spark作业读取检查点Parquet文件和日志对象. 使用统计信息来识别与查询相关的数据对象文件集. 查询对象存储以读取相关的数据对象, 可能在整个集群中并行读取. 注意, 由于云对象存储的最终一致性, 一些工作节点可能无法查询查询规划器在日志中找到的对象; 这些可以在短时间内重试. Delta Lake的只读事务可以在只支持最终一致性的对象存储上实现快照隔离. 如果由于最终一致性, 导致当前客户端读取到的最大日志ID不是最新的, 这种情况可以视为读取了表在某个时刻的快照. 写事务Delta Lake写事务的执行步骤如下: 使用只读事务的步骤1-2标识一个最近的日志记录ID, 例如$r$(即从最后一个检查点ID向前查找). 然后事务将读取表版本$r$(如果需要)的数据, 并尝试写入日志记录$r+1$. 在表版本$r$上读取数据, 如果需要, 使用与读取协议相同的步骤(即合并之前的检查点和任何进一步的日志记录, 然后读取这些记录中引用的数据对象). 将事务想要增加到表中的任何新数据写入新的对象存储文件中, 使用GUID生成对象名称. 这个步骤可以并行进行. 最后, 可以在新的日志记录中引用这些对象. 如果没有其他客户端写入该对象, 则尝试将事务的日志记录写入r+1.json. 这个步骤需要是原子的. 如果步骤失败, 可以重试事务; 根据查询的语义, 客户机还可以重用它在步骤3中写入的新数据对象, 并简单地尝试将它们添加到新的日志记录中的表中. 也可以为日志记录$r+1$编写一个新的.parquet检查点(默认每10条记录执行一次). 然后, 在这个写入完成之后, 更新_last_checkpoint文件, 使其指向检查点$r+1$. 在第4步中, 创建日志的操作需要是原子的. 在不同存储中有不同的实现方法: Google Cloud Storage和Azure Blob Store支持原子的put-if-absent操作, 可以直接使用它. 在像HDFS这样的分布式文件系统上, 使用原子rename将一个临时文件重命名为目标名称(例如000004.json), 或者如果它已经存在则失败. Azure Data Lake Storage还提供了一个带有原子重命名的文件系统API, 因此这里也可使用相同的方法. Amazon S3不支持原子的put-if-absent操作以及重命名操作. 在Databricks的服务中, 新增了一个Coordinate服务来保证只有一个客户端可以创建特性ID的日志. 在Delta Lake开源的Spark Connector中, 保证通过同一个Spark Context的写入会获得不同的日志ID. 不过也提供了LogStore API用于实现自定义的服务. 总结Delta Lake在数据湖上实现了表格存储, 用于支撑Lakehouse架构. 从实现上来看, Delta Lake并没有使用什么复杂的新技术, 只是将数据库系统中的日志引入进来, 却产生了意想不到的效果. 由此可见, 大数据系统中的很多创新就是将传统数据库系统中的技术重新应用到现有的分布式架构中. 参考[1] Databricks Delta Lake 论文阅读笔记[2] Understanding the Delta Lake Transaction Log [译文]","link":"/paper-delta-lake.html"},{"title":"Flink SQL源码 - Changelog原理与实现","text":"本文已收录在合集Apche Flink原理与实践中. Changelog起源于数据库领域, 代表变更操作, 可用于增量的数据同步. Flink SQL同样也引入了Changelog记录数据的变更, 来实现增量的数据处理, 只不过在实现上与数据库的Changelog有所不同. Changelog是隐藏在Flink SQL背后的重要概念, 是流与表可以统一的基础, 也是多个流式操作(如Group By)正确性的保障. 本文在介绍Changelog在Flink SQL中的实现原理, 揭开流式SQL背后的神秘面纱. Changelog概念数据库中的ChangelogChangelog源自数据库领域, 用于记录数据库的变更操作, 包括插入(INSERT), 更新(UPDATE)和删除(DELETE)操作. MySQL的Binlog就是一种常见的数据库Changelog实现, 可用于增量的数据同步. Change Data Capture(CDC)是一种常用的数据同步技术, 用于监控数据库中的数据变更, 并将这些变更转换为事件流, 以便进行实时处理. Flink SQL为了方便地接入数据库的Changelog数据, 已经支持从多种Changelog格式(如Debezium, Canel等)的数据流创建源表. 甚至为了直接摄取数据库的Binlog, 社区还提供了CDC Connector. 不过这些涉及的都是Flink与外部数据库Changelog的交互与集成, 本文重点讲述Flink SQL内部的Changelog机制. Flink SQL中的ChangelogFlink SQL内部也采用了Changelog机制来实现准确的增量数据处理. 我们将只包含INSERT操作的Changelog序列称为Append Stream, 而包含其他类型(例如UPDATE)操作的Changelog序列称为Update Stream. 不同于数据库中的Changelog, Flink SQL中的Changelog包含4种类型的操作: INSERT, UPDATE_BEFORE, UPDATE_AFTER, DELET. 可以看到在Flink中, UPDATE被分成了UPDATE_BEFORE和UPDATE_AFTER两个操作, 具体原因下文会进一步介绍. 在Flink SQL中, 每行数据都由RowKind标识变更类型, 其中 INSERT(+I)表示插入行; UPDATE_BEFORE(-U)表示回撤行, 只能与UPDATE_AFTER(+U)一起出现, 用于撤回之前已经向下游发送的行; UPDATE_AFTER(+U)表示更新行, 可以单独出现, 用于执行真正的更新操作; DELETE(-D)表示删除行. 1234567@PublicEvolvingpublic enum RowKind { INSERT(&quot;+I&quot;, (byte) 0), UPDATE_BEFORE(&quot;-U&quot;, (byte) 1), UPDATE_AFTER(&quot;+U&quot;, (byte) 2), DELETE(&quot;-D&quot;, (byte) 3);} UPDATE_BEFORE被称为Retraction(回撤), 用于对提前下发的结果进行修正. 在无界聚合及双流Join等场景中常需要进行Retraction操作, Retraction是保证流计算正确性的关键操作之一. 我们通过一个具体的例子来说明为什么需要Retraction以及什么是Retraction. 在电商商品运输过程中, 同一个订单的商品因为各种原因, 物流公司是有可能从A变成B的. 现在我们假设使用如下SQL统计各个物流公司所承运的订单数. 读者可以运行ChangelogGroupExample获得更直观的感受.123456789101112131415161718192021222324252627282930313233343536373839404142434445-- 源表输入/*源表输入001,ZhongTong002,YuanTong003,ZhongTong001,YuanTong*/CREATE TABLE source ( order_id STRING, tms_company STRING, proctime AS PROCTIME()) WITH (...)-- 这里为简单起见我们直接使用Processing Time下的最后一条记录, 实际上应该使用Event Time.-- 这里使用的是Flink SQL的Deduplication语义.-- 在保留最后一条记录的情况下, Processing Time和Event Time有相同的回撤语义.SELECT tms_company, count(DISTINCT order_id) AS order_cntFROM ( SELECT order_id, tms_company FROM ( SELECT order_id, tms_company, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY proctime DESC) AS rownum FROM source ) WHERE rownum = 1) GROUP BY tms_company;/*输出+----+--------------------------------+----------------------+| +I | ZhongTong | 1 || +I | YuanTong | 1 || -U | ZhongTong | 1 || +U | ZhongTong | 2 || -U | ZhongTong | 2 || +U | ZhongTong | 1 || -U | YuanTong | 1 || +U | YuanTong | 2 |+----+--------------------------------+----------------------+在最后一条记录(001,YuanTong)到来时, 内层的Deduplicate会向下游发送一条回撤记录(001,ZhongTong), 这时外层的GROUP BY会将ZhongTong对应的值减1并更新到下游. 然后内层Deduplicate向下游发送正常的插入记录(001,YuanTong), 外层GROUP BY将YuanTong对应的值加1并更新到下游. 最终得到的结果是:ZhongTong 1YuanTong 2*/ 通过上面的例子可以看到, Retraction本质上就是一个删除操作, 不过它总是与UPDATE_AFTER一起出现. 那为什么Flink SQL不像数据库那样, 把变更操作整合为一个复合的UPDATE操作呢, 主要有以下两个原因: 不采用复合操作可以简化数据的序列化和反序列化, 将UPDATE拆为UPDATE_BEFORE和UPDATE_AFTER两条记录之后, Flink SQL中的所有Changelog都是同质的数据行, 仅有RowKind不同. 在分布式计算环境中, 对于聚合等需要数据Shuffle的计算, 即使用复合的UPDATE, 在一些场景下也需要将其拆分为一条DELETE和一条INSERT记录. 在上述案例中就可能出现这种情况, 如果Group算子存在两个Task, 那么当+I(001,圆通)这条记录来临时, 由于Deduplicate算子和Group算子之间的数据传输需要根据tms_company进行Hash, 那么Deduplicate算子产生的-U(001,中通)和+U(001,圆通)就需要分别传输到两个下游算子中. 目前在Flink SQL中可能产生Retraction记录的场景有: Group Aggregate Over Aggregate TopN / Deduplication Left/Right/Full Join 除了上述场景, 在支持Early Fire或Late Fire的窗口聚合中也会产生回撤记录. 在Flink SQL中, Group Window Aggregate是支持Early Fire或Late Fire的, 启用这一特性的参数定义在WindowEmitStrategy中. 不过一方面这几个参数都是实验性质的, 另一方面Group Window Aggregate已经被基于Window TVF的聚合取代, 而后者不支持Early Fire或Late Fire. Table Planner处理流程Flink Table Planner对Changelog语义进行了完善的建模, 在physical_rewrite优化阶段, 会根据StreamPhysicalRel节点的类型及其上下游节点推导出ModifyKind和UpdateKind, 下文将详细介绍推导流程. 相关概念ChangelogModeChangelogMode用于标识算子的变更类型, 是多个不同类型RowKind的集合.123456789101112131415161718192021public final class ChangelogMode { private static final ChangelogMode INSERT_ONLY = ChangelogMode.newBuilder().addContainedKind(RowKind.INSERT).build(); private static final ChangelogMode UPSERT = ChangelogMode.newBuilder() .addContainedKind(RowKind.INSERT) .addContainedKind(RowKind.UPDATE_AFTER) .addContainedKind(RowKind.DELETE) .build(); private static final ChangelogMode ALL = ChangelogMode.newBuilder() .addContainedKind(RowKind.INSERT) .addContainedKind(RowKind.UPDATE_BEFORE) .addContainedKind(RowKind.UPDATE_AFTER) .addContainedKind(RowKind.DELETE) .build(); private final Set&lt;RowKind&gt; kinds;} ModifyKindSetTraitModifyKindSetTrait是Calcite中RelTrait的一种实现(如果不了解Calcite可以参考笔者的博文合集Apache Calcite原理与实践), 属于StreamPhysicalRel的其中一种物理属性, 用于标识当前节点的变更类型. 其所持有的变更类型由ModifyKindSet表示, 实现如下, 实际上就是保存了该节点可能出现的ModifyKind类型.123456789101112131415161718192021222324public class ModifyKindSet { /** Insert-only modify kind set. */ public static final ModifyKindSet INSERT_ONLY = ModifyKindSet.newBuilder().addContainedKind(ModifyKind.INSERT).build(); /** A modify kind set contains all change operations. */ public static final ModifyKindSet ALL_CHANGES = ModifyKindSet.newBuilder() .addContainedKind(ModifyKind.INSERT) .addContainedKind(ModifyKind.UPDATE) .addContainedKind(ModifyKind.DELETE) .build(); private final Set&lt;ModifyKind&gt; kinds;}public enum ModifyKind { /** Insertion operation. */ INSERT, /** Update operation. */ UPDATE, /** Deletion operation. */ DELETE} UpdateKindTraitUpdateKindSetTrait同样也是RelTrait的一种实现, 属于StreamPhysicalRel的一种物理属性, 用于标识当前节点的更新类型, 具体由UpdateKind表示, 其中: NONE代表当前节点不存在更新操作; ONLY_UPDATE_AFTER表示当前节点可不产生回撤记录; BEFORE_AND_AFTER表示当前节点需要产生回撤记录. 12345public enum UpdateKind { NONE, ONLY_UPDATE_AFTER, BEFORE_AND_AFTER} 关系代数优化阶段在关系代数优化阶段, Changelog相关的处理在physical_rewrite阶段进行. 核心的实现逻辑在FlinkChangelogModeInferenceProgram中. 下文以开头的例子为例介绍关系代数优化阶段的处理流程, 下图是对应的物理表达式及优化后各个算子所对应的ModifyKind和UpdateKind. ModifyKindSetTrait推理流程ModifyKindSetTrait推理的目标是确定每个算子的变更类型, 算子的变更类型受其输入算子的变更类型及其本身所执行的操作所影响, 例如对于StreamPhysicalGroupAggregate算子而言: 如果其输入算子只产生INSERT, 那么该算子只会产生INSERT和UPDATE; 如果其输入算子产生除了产生INSERT, 还会产生UPDATE或DELETE, 那么该算子就会同时产生INSERT, UPDATE和DELETE. 在推理过程中, 父节点会向子节点Require变更类型, 如果子节点的变更类型不满足父节点Require的输入变更类型, 就会抛出异常. 当子节点变更类型为父节点Require输入变更类型的子集时即为满足, 例如: 子节点变更类型[I,U]满足父节点Require输入变更类型[I,U,D]; 子节点变更类型[I,U,D]不满足父节点Require输入变更类型[I,U]. ModifyKindSetTrait的推理过程在SatisfyModifyKindSetTraitVisitor.visit()中实现, 其本质上就是对算子树进行深度优先遍历. 这里先介绍一下两个重要的辅助函数: visitChildren()函数包含两个参数, parent: StreamPhysicalRel和requiredChildrenTrait: ModifyKindSetTrait, 其功能就是对parent的所有子节点进行深度优先遍历, 其子节点的ModifyKindSetTrait必须满足requiredChildrenTrait, 否则就会抛出异常. createNewNode()函数包含五个参数, node: StreamPhysicalRel, children: List[StreamPhysicalRel], providedTrait: ModifyKindSetTrait, requiredTrait: ModifyKindSetTrait和requestedOwner: String. 其功能是更改node的ModifyKindSetTrait为providedTrait, 不过在创建新节点的过程中会检查providedTrait是否满足requiredTrait, 若不满足则会抛出异常. 其中, providedTrait是当前节点会产生的变更类型, 而requiredTrait是父节点Require的变更类型. 下面我们从下往上具体介绍一下上文案例中所涉及的各个算子的变更类型推导流程. StreamPhysicalLegacyTableSourceScan的推导比较简单, 它本身只会产生INSERT, 因此直接调用createNewNode添加ModifyKindSetTrait.INSERT_ONLY即可.1234case _: StreamPhysicalDataStreamScan | _: StreamPhysicalLegacyTableSourceScan | _: StreamPhysicalValues =&gt; // DataStream, TableSource and Values only support producing insert-only messages createNewNode(rel, List(), ModifyKindSetTrait.INSERT_ONLY, requiredTrait, requester) StreamPhysicalCalc和StreamPhysicalExchange等一众算子本身并不会改变输入的变更类型, 因此只需要将子节点的变更类型进行穿透即可.123456789case _: StreamPhysicalCalcBase | _: StreamPhysicalCorrelateBase | _: StreamPhysicalLookupJoin | _: StreamPhysicalExchange | _: StreamPhysicalExpand | _: StreamPhysicalMiniBatchAssigner | _: StreamPhysicalWatermarkAssigner | _: StreamPhysicalWindowTableFunction =&gt; // transparent forward requiredTrait to children val children = visitChildren(rel, requiredTrait, requester) val childrenTrait = children.head.getTraitSet.getTrait(ModifyKindSetTraitDef.INSTANCE) // forward children mode createNewNode(rel, children, childrenTrait, requiredTrait, requester) StreamPhysicalDeduplicate仅支持INSERT输入, 因此首先通过visitChildren()向子节点Require ModifyKindSetTrait.INSERT_ONLY. StreamPhysicalDeduplicate在保持处理时间下的第一条记录时仅会产生INSERT, 否则会产生所有变更类型.1234567891011case deduplicate: StreamPhysicalDeduplicate =&gt; // deduplicate only support insert only as input val children = visitChildren(deduplicate, ModifyKindSetTrait.INSERT_ONLY) val providedTrait = if (!deduplicate.keepLastRow &amp;&amp; !deduplicate.isRowtime) { // only proctime first row deduplicate does not produce UPDATE changes ModifyKindSetTrait.INSERT_ONLY } else { // other deduplicate produce update changes ModifyKindSetTrait.ALL_CHANGES } createNewNode(deduplicate, children, providedTrait, requiredTrait, requester) StreamPhysicalGroupAggregate可接受所有变更类型的输入, 如果输入包含UPDATE和DELETE那么可能产生DELETE, 否则不会产生DELETE.12345678910111213141516case agg: StreamPhysicalGroupAggregate =&gt; // agg support all changes in input val children = visitChildren(agg, ModifyKindSetTrait.ALL_CHANGES) val inputModifyKindSet = getModifyKindSet(children.head) val builder = ModifyKindSet .newBuilder() .addContainedKind(ModifyKind.INSERT) .addContainedKind(ModifyKind.UPDATE) if ( inputModifyKindSet.contains(ModifyKind.UPDATE) || inputModifyKindSet.contains(ModifyKind.DELETE) ) { builder.addContainedKind(ModifyKind.DELETE) } val providedTrait = new ModifyKindSetTrait(builder.build()) createNewNode(agg, children, providedTrait, requiredTrait, requester) StreamPhysicalSink的处理流程相对特殊, 它首先通过deriveQueryDefaultChangelogMode()推导出附带的查询语句可能产生的变更类型, 然后通过对应的DynamicTableSink.getChangelogMode()来获取Sink节点可接受的变更类型, 最后会通过visitChildren()再次验证子节点的变更类型能否满足Sink节点的输入要求.123456789case sink: StreamPhysicalSink =&gt; val name = s&quot;Table sink &apos;${sink.contextResolvedTable.getIdentifier.asSummaryString()}&apos;&quot; val queryModifyKindSet = deriveQueryDefaultChangelogMode(sink.getInput, name) val sinkRequiredTrait = ModifyKindSetTrait.fromChangelogMode(sink.tableSink.getChangelogMode(queryModifyKindSet)) val children = visitChildren(sink, sinkRequiredTrait, name) val sinkTrait = sink.getTraitSet.plus(ModifyKindSetTrait.EMPTY) // ignore required trait from context, because sink is the true root sink.copy(sinkTrait, children).asInstanceOf[StreamPhysicalRel] 关于StreamPhysicalSink的上述处理流程, 这里在补充说明一下. Sink节点能够接受的变更类型是由对应输出目标决定的, 如果目标存储只能支持INSERT, 这时候其getChangelogMode()的实现如下, 此时如果对应的SELECT语句会输出UPDATE和DELETE(例如使用了GROUP BY), Table Planner在优化阶段就会直接抛出异常.123public ChangelogMode getChangelogMode(ChangelogMode requestedMode) { return ChangelogMode.insertOnly();} UpdateKindTrait推理流程UpdateKindTrait推理的目标是确定算子的更新类型, 需要注意的是变更类型和更新类型之间的区别, 更新类型是变更类型中UPDATE的两种具体形式, 即UPDATE_BEFORE和UPDATE_AFTER. UpdateKindTrait的推理过程在SatisfyUpdateKindTraitVisitor.visit()中实现, 其本质上也是对算子树进行深度优先遍历. 这里同样先介绍一下两个辅助函数: visitChildren()函数包含两个参数, parent: StreamPhysicalRel和requiredChildrenTrait: UpdateKindTrait, 其功能就是对parent的所有子节点进行深度优先遍历, 如果子节点的UpdateKindTrait不满足requiredChildrenTrait就会返回NONE, 否则返回UpdateKindTrait属性满足要求的相应节点. createNewNode()函数包含三个参数, node: StreamPhysicalRel, childrenOption: Option[List[StreamPhysicalRel]], providedTrait: UpdateKindTrait. 其功能是在node的UpdateKindTrait中添加providedTrait, 不过在创建新节点的过程中会检查providedTrait是否符合要求. 判断条件如下: 当providedTrait为UpdateKind.NONE时, node的ModifyKindSetTrait不能包含ModifyKind.UPDATE, 否则必须包含ModifyKind.UPDATE. 在不满足这一条件时便会抛出异常. 下面同样从下往上具体介绍一下上文案例中所涉及的各个算子的更新类型推导流程. StreamPhysicalLegacyTableSourceScan仅会产生INSERT, 因此providedTrait直接传入UpdateKindTrait.NONE即可.123case _: StreamPhysicalDataStreamScan | _: StreamPhysicalLegacyTableSourceScan | _: StreamPhysicalValues =&gt; createNewNode(rel, Some(List()), UpdateKindTrait.NONE) StreamPhysicalCalc仅在父节点需要UpdateKindTrait.ONLY_UPDATE_AFTER且存在过滤条件时返回None. 其他条件下直接将requiredTrait穿透给子节点即可.123456789101112131415161718case calc: StreamPhysicalCalcBase =&gt; if ( requiredTrait == UpdateKindTrait.ONLY_UPDATE_AFTER &amp;&amp; calc.getProgram.getCondition != null ) { // we don&apos;t expect filter to satisfy ONLY_UPDATE_AFTER update kind, // to solve the bad case like a single &apos;cnt &lt; 10&apos; condition after aggregation. // See FLINK-9528. None } else { // otherwise, forward UpdateKind requirement visitChildren(rel, requiredTrait) match { case None =&gt; None case Some(children) =&gt; val childTrait = children.head.getTraitSet.getTrait(UpdateKindTraitDef.INSTANCE) createNewNode(rel, Some(children), childTrait) } } StreamPhysicalExchange直接将requiredTrait穿透给子节点即可.1234567891011case _: StreamPhysicalCorrelateBase | _: StreamPhysicalLookupJoin | _: StreamPhysicalExchange | _: StreamPhysicalExpand | _: StreamPhysicalMiniBatchAssigner | _: StreamPhysicalWatermarkAssigner | _: StreamPhysicalWindowTableFunction =&gt; // transparent forward requiredTrait to children visitChildren(rel, requiredTrait) match { case None =&gt; None case Some(children) =&gt; val childTrait = children.head.getTraitSet.getTrait(UpdateKindTraitDef.INSTANCE) createNewNode(rel, Some(children), childTrait) } StreamPhysicalDeduplicate不需要向子节点Require任何更新类型, 因为它只支持INSERT类型的变更.12345678910case _: StreamPhysicalWindowAggregate | _: StreamPhysicalWindowRank | _: StreamPhysicalWindowDeduplicate | _: StreamPhysicalDeduplicate | _: StreamPhysicalTemporalSort | _: StreamPhysicalMatch | _: StreamPhysicalOverAggregate | _: StreamPhysicalIntervalJoin | _: StreamPhysicalPythonOverAggregate | _: StreamPhysicalWindowJoin =&gt; // WindowAggregate, WindowTableAggregate, WindowRank, WindowDeduplicate, Deduplicate, // TemporalSort, CEP, OverAggregate, and IntervalJoin, WindowJoin require nothing about // UpdateKind. val children = visitChildren(rel, UpdateKindTrait.NONE) createNewNode(rel, children, requiredTrait) StreamPhysicalGroupAggregate当输入包含UPDATE变更时需要UPDATE_BEFORE, beforeAfterOrNone()在输入包含UPDATE时返回UpdateKind.BEFORE_AND_AFTER否则返回UpdateKind.NONE.12345678910case _: StreamPhysicalGroupAggregate | _: StreamPhysicalGroupTableAggregate | _: StreamPhysicalLimit | _: StreamPhysicalPythonGroupAggregate | _: StreamPhysicalPythonGroupTableAggregate | _: StreamPhysicalGroupWindowAggregateBase =&gt; // Aggregate, TableAggregate, Limit and GroupWindowAggregate requires update_before if // there are updates val requiredChildTrait = beforeAfterOrNone(getModifyKindSet(rel.getInput(0))) val children = visitChildren(rel, requiredChildTrait) // use requiredTrait as providedTrait, because they should support all kinds of UpdateKind createNewNode(rel, children, requiredTrait) StreamPhysicalSink由于涉及到与外部系统的交互, 相对复杂. inferSinkRequiredTraits()会推导出Sink所需的更新类型, 例如当Sink节点仅支持UPDATE_AFTER时, 如果Upert Key是结果表Primary Key的子集, 那么Sink的子节点可以只产生UPDATE_AFTER从而提升性能. analyzeUpsertMaterializeStrategy用于确定是否需要添加SinkUpsertMaterializer, 后文会进一步讲述.1234case sink: StreamPhysicalSink =&gt; val sinkRequiredTraits = inferSinkRequiredTraits(sink) val upsertMaterialize = analyzeUpsertMaterializeStrategy(sink) visitSink(sink.copy(upsertMaterialize), sinkRequiredTraits) Sink算子Upsert物化通过上面的分析可以看到, 在推导StreamPhysicalSink算子的UpdateKindTrait属性时, 需要分析是否需要添加SinkUpsertMaterializer算子, 这主要是为了解决分布式环境下Changelog乱序导致的结果不准确问题. 我们还是通过一个例子来具体地说明SinkUpsertMaterializer算子的作用. 示例的SQL如下, 读者可运行ChangelogJoinExample进行调试.1234567891011121314151617181920212223242526272829303132CREATE TABLE source1 ( id BIGINT, level BIGINT, proctime AS PROCTIME(), PRIMARY KEY (id) NOT ENFORCED) WITH (...)-- 通过Deduplication来模拟源表的Changelog生成CREATE VIEW s1 ASSELECT id, levelFROM ( SELECT id, level, ROW_NUMBER() OVER (PARTITION BY id ORDER BY proctime DESC) AS rownum FROM source1) WHERE rownum &lt;= 1;CREATE TABLE s2 ( id BIGINT, attr STRING, PRIMARY KEY (id) NOT ENFORCED) WITH (...)CREATE TABLE t1 ( id BIGINT, level BIGINT, attr STRING, PRIMARY KEY (id) NOT ENFORCED) WITH (...)INSERT INTO t1SELECT s1.*, s2.attrFROM s1 JOIN s2 ON s1.level = s2.id; 我们假设s2中已有两条记录+I(10,a1)和+I(20,b1). 此时s1下发了三条Changelog记录, 分别是+I(1,10), -U(1,10)和+U(1,20). 同时我们假设在运行时Join算子存在两个Task, 那么整个计算图如下图所示. 很明显在不存在乱序的情况下, 最终结果表中的记录应该是(1,20,b1). 但是由于有两个Join Task, 最终到达Sink算子的Changelog顺序可能存在以下3中情况. 注意由于+I(1,10,a1), -U(1,10,a1)来自同一个算子, 因此这两条记录之间一定会保持顺序. 可以看到, 后两种情况下乱序会导致结果表中不存在id=1的记录. 在介绍上述问题的解决方案之前, 我们先来分析一下出现这类问题的根本原因. 在SQL中有两个关于键的重要概念: Unique Key表示一个或多个能唯一标识一行数据的列. 如果某些Unique Key的顺序能够得到保证, 那么这些键称为Upsert Key. 对于Sink算子而言, 如果输入存在Upsert Key, 那么由于它是天然有序的, 直接按接受顺序更新到外部表即可. 在上述Join示例中, Sink算子的Unique Key是s1.id, 但是由于Join Key是s1.level, 所以同一个s1.id可能来自不同的Join算子, 这就导致s1.id的顺序无法保证, 因此也就不存在Upsert Key. 这种情况下如果不存在额外的操作, 就可能导致最终结果错误. 而对于文中开头的案例, 如果最终Sink表的Unique Key是tms_company, 那么由于Group by的Key也是tms_company, 因此同一个tms_company只可能来自于同一个Group算子, 这就意味着tms_company就是Upsert key, Sink算子只需直接写外部表即可, 不需要额外操作. 对于没有Upsert Key或者Upsert Key与Sink的Unique Key不同的情况, 为了保证乱序情况下结果的准确性, 必须要有额外的操作. Flink SQL会在Sink算子之前添加一个SinkUpsertMaterializer算子, 上述示例经过该算子之后的输出结果如下所示. 它会在状态中维护某个Upsert Key(如果没有Upsert Key就是整行记录)的更新历史, 这样在收到乱序的记录后就可以对其进行纠正, 具体的实现逻辑可参考SinkUpsertMaterializer. Transform图生成阶段在关系代数优化阶段, Table Planner已经推导出了各个算子的ModifyKindSetTrait和UpdateKindTrait属性. 根据这些属性, 在Transform图生成阶段, 即可进行相应的代码生成和处理优化. 对于不同的算子, 需要有不同的逻辑来生成Changelog事件. 本文以Group聚合算子为例进行介绍, 在不开启Mini-Batch时其与Changelog相关的实现逻辑在GroupAggFunction中(算子调用链可参考笔者之前的博文), 处理流程如下图所示. 这里作几点简要说明: GroupAggFunction实际上就是KeyedProcessFunction的继承类, 通过State管理历史聚合结果来实现Changelog的生成. 也就是说, 我们完全可以在DataStream API中借助KeyedProcessFunction来实现同样的效果, Flink SQL只不过是对整个过程进行了标准化. 关系代数优化阶段推导出的ModifyKindSetTrait和UpdateKindTrait可以帮助算子在实现时进行相应的优化, 例如如果StreamPhysicalGroupAggregate的下游算子不支持接受UPDATE_BEFORE, 那么GroupAggFunction完全可以不发送这一事件来减少数据传输和下游的处理压力, 在GroupAggFunction中是否需要发送Retraction记录是由generateUpdateBefore变量决定的, 仅当StreamPhysicalGroupAggregate的UpdateKindTrait为BEFORE_AND_AFTER时它的值为true. 总结本文介绍了Flink SQL内部的Changelog机制, 它的形成也不是一蹴而就的, 而是在实践中不断演化, 才有了目前较为完善的Changelog实现. 即便如此, 在实践中还是可能存在问题, 例如多个流的Regular Join会导致Retraction放大, 降低执行效率. 在流计算场景中, 要在高效解决此类问题的同时保证语义的正确, 获取还需要额外的存储组件支持, 例如Apache Paimon的Partial Update. 本文围绕示例中的算子, 介绍了Changelog属性的推导流程, 虽然没有涉及全部算子, 不过在理解整个Changelog机制原理的基础上, 相信可以轻松理解其他算子的推导流程. 最后以Group算子为例介绍了Changelog语义的算子实现. 参考[1] Retraction for Flink Streaming[2] FLIP-95: New TableSource and TableSink interfaces[3] FLIP-105: Support to Interpret Changelog in Flink SQL (Introducing Debezium and Canal Format)[4] Flink SQL Secrets: Mastering the Art of Changelog Event Out-of-Orderness[5] Flink Table的三种Sink模式","link":"/flink-sql-changelog.html"},{"title":"论文阅读 - Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics","text":"本文已收录在合集数据系统经典论文阅读中. 本文是对Databricks的Lakehouse(湖仓一体)论文(Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics)的阅读总结. 论文详细阐述了需要Lakehouse的原因, Lakehouse的具体架构以及在Lakehouse构建中可进一步探索的研究性问题. 通过阅读论文可以更深刻地了解Lakehouse产生的前因后果, 从而更加客观地看待这一新兴数据平台架构. 本文不会对论文进行完整的翻译, 而是按如下主线剖析论文的核心观点和内容, 并穿插笔者的见解: 现有的数据平台架构及存在的问题. 支撑Lakehouse成功的必备技术条件. Lakehouse的架构及特性. 现有数据平台及问题第一代数据平台数据仓库(Data Warehouse)起源于上世纪80年代, 它将分散在各个业务数据库中的数据, 采集传输到集中的数据仓库中, 以帮助企业领导者获得分析见解, 这些数据仓库还可进一步用于决策支持和商业智能(BI). 第一代数据平台主要用于构建数据仓库, 其处理流程如下图所示. 包含在业务数据库(一般是关系型数据库)中的结构化数据通过ETL导入到数据仓库系统中. 数据仓库中的数据将使用写时模式(Schema-on-write)写入, 这确保了数据模型针对下游BI消费进行了优化. 随着数据量的增长, 第一代数据平台出现了一些问题(以下是论文原话的翻译): 首先, 它们通常将计算和存储耦合到本地设备中. 这迫使企业为峰值用户负载和受管理的数据进行调配和付费, 随着数据集的增长, 这变得非常昂贵. 其次, 不仅数据集在快速增长, 而且越来越多的数据集完全是非结构化的, 例如视频, 音频和文本文档, 难以在数据仓库中存储和查询. 实际上, 对于第一点笔者认为论文的描述并不完全准确, 数据仓库的实现技术也在不断发展. 在初期的小数据量时代, 数据仓库的实现系统一般以传统的关系型数据库为主, Oracle等厂商也有商用的数据仓库系统, 不过它们大多采用集中式架构. 这确实使得横向扩展困难, 当数据量增大而出现性能问题时, 只能纵向扩展底层系统资源, 需要昂贵的成本. 但是随着数据量的不算增长, MPP架构的出现使得廉价处理更大的数据集成为可能, 当前仍有广泛使用的MPP数据仓库, 如Greenplum, Clickhouse等. 另外, 随着云计算技术的发展, 云原生数据仓库的出现也为低成本地处理大规模数据提供了解决方案, 如AWS的Redshift, 阿里云的AnalyticDB等. 因此, 使用当前最先进的数据仓库系统在大规模数据集下不一定会有多么昂贵的成本, 不过这些数据仓库系统基本上只能处理结构化数据, 确实难以用于半结构化或非结构化数据的处理. 第二代数据平台由于第一代数据平台的上述问题, 在2010年前后, 第二代数据平台开始出现. 在这一架构中, 所有的原始数据都会被导入到统一的数据湖中. 数据湖是具有文件API的低成本存储系统, 以通用且开放的文件格式保存数据, 如Apache Parquet和ORC. 之后, 数据湖中的一部分数据可进一步被ETL到下游的数据仓库系统中, 用于最重要的决策支持和BI应用. 另外, 开放格式的使用也使得数据湖中的数据可以被其他各种分析引擎直接访问, 例如机器学习系统. 其处理流程如下图所示. 数据湖最开始一般使用Apache Hadoop的HDFS进行存储, 不过近来随着云存储(如AWS S3, 阿里云OSS)的发展, 逐渐取代了HDFS. 主要是由于它们具有卓越的持久性(通常大于10个9), 异地多副本等特点, 最重要的是成本极低, 可以自动进行更廉价的分层存储. 数据湖是一种读时模式(Schema-on-read)架构, 它支持以低成本灵活地存储任何数据, 但另一方面, 将数据质量和治理问题放在了下游. 表面上来看, 第二代数据平台可以将存储和计算分离, 从而降低成本, 但其仍然存在问题(以下是论文原话的翻译). 可靠性(Reliability). 保持数据湖和数据仓库的一致性既困难又昂贵. 需要持续在两个系统之间进行ETL, 使其可用于高性能决策支持和BI. 每个ETL步骤也有招致失败或引入降低数据质量的错误的风险, 例如, 由于数据湖和数据仓库引擎之间的细微差别而引起的. 数据陈旧(Data staleness). 与数据湖中的数据相比, 数据仓库中的数据是陈旧的, 新数据经常需要几天才能加载. 与第一代数据平台架构相比这是一种倒退, 在第一代数据平台中, 新的业务数据可以立即用于查询. 根据Dimensional Research和Five-tran的调查, 86%的分析师使用过时的数据, 62%的分析师报告每月要等待工程资源多次. 对高级分析的支持有限(Limited support for advanced analytics). 企业希望使用他们的数据仓库数据回答预测性问题, 例如, “我应该向哪些客户提供折扣?” 尽管对ML和数据管理的融合进行了大量研究, 但没有一个领先的机器学习系统, 如TensorFlow, PyTorch和XGBoost, 能够在数据仓库上很好地工作. 与提取少量数据的BI查询不同, 这些系统需要使用复杂的非SQL代码处理大型数据集. 通过ODBC/JDBC读取这些数据是低效的, 而且没有办法直接访问数据仓库内部的专有格式. 对于这些用例, 数据仓库供应商建议将数据导出到文件, 这进一步增加了复杂性和陈旧性(增加了第三个ETL步骤!). 或者, 用户可以针对开放格式的数据湖数据运行这些系统. 然而, 它们会失去数据仓库丰富的管理特性, 比如ACID事务, 数据版本控制和索引. 总拥有成本(Total cost of ownership). 除了为连续ETL付费之外, 用户还要为复制到数据仓库的数据支付双倍的存储成本, 而且商业仓库将数据锁定为专有格式, 这增加了将数据或工作负载迁移到其他系统的成本. Lakehouse架构概览由于当前数据平台架构存在的各种问题, 论文提出了一个问题: 有没有可能将基于标准开放数据格式(如Parquet和ORC)的数据湖转变为高性能系统, 既能提供数据仓库的性能和管理功能, 又能提供来自高级分析工作负载的快速直接I/O? 当然, 论文的作者认为上述问题显然是可能的. 于是就出现了如下这种Lakehouse平台架构. 它在数据湖之上添加了一层轻量级的封装, 从而能够支持数据仓库类的SQL查询, 也能提供高性能的数据I/O供机器学习系统使用. Lakehouse必备的技术条件上述Lakehouse数据平台架构的实现是需要一定技术条件作为支撑的, 论文指出了三个重要的技术条件, 它们是Lakehouse时代来临的重要保障. 数据湖上的可靠数据管理: Lakehouse需要能够存储原始数据, 类似于今天的数据湖, 同时支持管理这些数据的ETL/ELT流程, 以提高其分析质量. 传统来说, 数据湖将数据作为半结构化格式的”一堆文件”来管理, 因此很难提供一些关键的管理功能来简化数据仓库中的ETL/ELT, 如事务, 回滚到旧表版本和零拷贝克隆. 然而, 最近的一系列系统, 如Delta Lake和Apache Iceberg, 提供了数据湖的事务视图, 并启用了管理功能. 当然, 人们仍然需要做艰苦的工作来编写ETL/ELT逻辑来使用Lakehouse创建经过管理的数据集, 但总体上ETL步骤更少, 并且分析者还可以轻松且高效地查询原始数据表, 这与第一代分析平台中的情况非常相似. 支持机器学习和数据科学: ML系统对数据湖格式直接读取的支持, 已经使它们处于有效访问Lakehouse的良好位置. 此外, 许多ML系统采用DataFrame作为操作数据的抽象, 最近的系统设计了声明式DataFrame API, 这些API允许在ML工作负载中执行数据访问的查询优化. 这些API使ML工作负载能够直接受益于Lakehouse中的许多优化. SQL性能: Lakehouse将需要在过去十年(或者长期来说, 是为直接访问而公开的一些其他标准格式)积累的海量Parquet/ORC数据集的基础上提供最先进的SQL性能. 相比之下, 传统的数据仓库接受SQL, 可以自由地优化一切, 包括专有的存储格式. 尽管如此, 我们表明可以使用各种技术来维护有关Paracquet/ORC数据集的辅助数据, 并在这些现有格式中优化数据布局, 以获得具有竞争力的性能. 我们展示了在Paracquet上的SQL引擎(Databricks Delta Engine)的结果, 该引擎在TPC-DS上的性能超过了领先的云数据仓库. 从上述条件可以看出, 要使Lakehouse平台成为现实, 需要在存储和计算两个方面都有完备的技术支撑. 存储层面, 在传统的数据湖上需要支持事务, 索引, 多版本等高级功能. 同时还要支持声明式的DataFrame API以支持机器学习系统的直接访问. 目前开源的Delta Lake, Apache Hudi/Iceberge等都是期望解决这类问题的系统, 它们在传统数据湖之上提供了table format. 计算层面, 需要有高效的SQL引擎, 能够直接访问优化后的数据湖中的数据, 并且提供与数据仓库相当的查询性能. Databricks采用了自研的Delta Engine, 当然也可以采用开源的Spark/Flink计算引擎, 或Presto等MPP引擎, 或混合引擎, 并根据具体的查询场景适配最优的引擎. 阿里云的DLF, 以及火山引擎的LAS实际上都采用了混合引擎的解决方案. Lakehouse架构及特性Lakehouse的定义基于上述知识, 论文给Lakehouse下了一个明确的定义: 我们将Lakehouse定义为基于低成本和可直接访问存储的数据管理系统, 它还提供传统的分析DBMS管理和性能特性, 如ACID事务, 数据版本控制, 审计, 索引, 缓存和查询优化. 由上述定义可以看出, Lakehouse结合了数据湖和数据仓库的主要优点: 开放式格式的低成本存储, 可由前者的各种系统访问, 而后者具有强大的管理和优化功能. 值得注意的是, Lakehouse特别适合存储与计算分离的云环境: 不同的计算应用程序可以在完全独立的计算节点(例如, ML的GPU集群)上按需运行, 同时直接访问相同的存储数据. 然而, 也可以在HDFS等内部存储系统上实现Lakehouse. Lakehouse的架构论文提出的Lakehouse架构如下图所示. 在传统数据湖的基础之上, 增加了Metadata层, 这一层不仅在廉价的数据湖存储上增加了事务, 多版本等功能, 并且还能提供缓存, 辅助数据结构(如索引和统计信息), 以及数据布局优化. 在Metadata层之上, 可以通过SQL API直接访问数据湖中的原始数据用于BI应用, 也可以通过声明式的DataFrame API读取原始数据用于数据科学和机器学习类应用. 可以看出, Lakehouse架构有很大的灵活性, 由于存储和计算分离, 在实现Lakehouse架构时, 可自由组合多种存储和计算引擎. 论文详细介绍了Databricks在Lakehouse平台架构上的实践, 其采用的各个引擎如下图所示. Metadata层采用Delta Lake, 目前已经开源, 同类的开源产品还有Apache Iceberge/Hudi, 也就是说在构建Lakehouse时可以采用任意一个上述系统. 计算引擎是Databricks内部基于C++自研的Delta Engine, 完全兼容Spark SQL, 并且提供了大量的查询优化. 也可以采用开源的Spark/Flink或Presto等引擎代替. 图片来自What does Databricks do? Metadata Layers for Data ManagementLakehouse中的Metadata层构建在现有的数据湖之上, 一般与现有的存储格式如Parquet/ORC兼容, 并提供数据管理功能, 如: 事务支持, 零拷贝克隆, 时间旅行; 数据约束, 如域约束等, 可以提升数据湖中的数据质量; 权限验证, 如觉得哪个用户可以访问哪张表. 目前开源的Delta Lake, Apache Iceberg/Hudi, 都支持上述的全部或部分功能. 不过论文也指出了由于数据湖的Metadata层由于刚刚发展, 也还存在很多问题值得进一步解决. 比如, Delta Lake目前将事务日志存储在与底层数据湖相同的对象存储上, 由于对象存储的高延时性, 如果可以把事务日志存储在更快的存储引擎上则可能进一步降低数据湖的访问延迟; Delta Lake, Iceberge和Hudi一次只支持一张表上的事务, 但应该可以扩展它们以支持跨表事务; 优化事务日志的格式和被管理对象的大小也是有待解决的问题. SQL Performance in a LakehouseLakehouse架构能否成功的一个重要因素是能否在数据湖之上实现媲美数据仓库的SQL查询效率. 因为SQL查询几乎是一个数据平台不可或缺的, 而且有很大一部分SQL查询对延迟有很高的要求, Lakehouse去掉了专用于SQL查询的数据仓库系统之后提供与之相近的SQL查询性能是一个重要的挑战. 而对于高级分析和机器学习系统而言, 这类系统对数据读取的实时性要求没有那么高, 在数据湖之上提供供这类系统读取数据的接口不会有很大的挑战. 论文也指出了对于如何在放弃传统DBMS设计中的很大一部分数据独立性的同时, 提供最先进的SQL性能, 是受很多因素影响的. 论文提出了几种优化技术, 这几种技术已经在Databricks的Delta Engine中得到了实现, 以下是论文原文的翻译. 缓存(Caching): 当使用诸如Delta Lake之类的事务性Metadata层时, Lakehouse系统可以安全地将来自云对象存储的文件缓存在更快的存储设备上, 例如处理节点上的SSD和RAM. 正在运行的事务可以很容易地确定缓存的文件何时仍然可以读取. 此外, 缓存可以采用代码转换的格式, 这种格式对于查询引擎的运行更有效, 与传统”封闭世界”数据仓库引擎中使用的任何优化相匹配. 例如, 我们在Databricks的缓存部分解压缩了它加载的Parquet数据. 辅助数据(Auxiliary data): 尽管Lakehouse需要公开直接I/O的表格存储格式, 但它可以在其完全控制的辅助文件中维护有助于优化查询的其他数据. 在Delta Lake和Delta Engine中, 我们为表中的每个数据文件维护列最小-最大统计信息, 这些数据文件位于用于存储事务日志的同一个Parquet文件中, 这使得在基本数据按特定列聚集时可以使用数据跳过优化. 我们还在实现基于Bloom Filter的索引. 人们可以想象在这里实现广泛的辅助数据结构, 类似于索引”原始”数据. 数据布局(Data layout): 数据布局对访问性能影响很大. 即使我们确定了Parquet等存储格式, Lakehouse系统也可以优化多种布局决策. 最明显的是记录排序: 哪些记录聚集在一起, 因此最容易一起读取. 在Delta Lake中, 我们支持使用个体维度或空间填充曲线(如z阶和希尔伯特曲线)来排序记录, 以提供跨多个维度的局部性. 您还可以想象支持在每个数据文件中以不同顺序放置列的新格式, 为不同的记录组选择不同的压缩策略或其他策略. 论文提供了使用Delta Lake和Delta Engine时的SQL查询性能, 下图是Lakehouse架构与四种云数据仓库的SQL查询性能和成本对比. 从中可以看到, Lakehouse具有最低的成本, 其SQL查询性能甚至比三个云数据仓库更好. 根据上述分析也可以看出, 性能强悍的计算引擎是Lakehouse能否成功的关键. Databricks的Delta Engine目前尚未开源, 而Spark/Flink/Presto等产品虽然亦可作为Lakehouse架构中的计算引擎, 但是它们还存在诸多优化的余地. 笔者也认为未来在计算引擎的查询优化方面是一个重要的研究热点. Efficient Access for Advanced Analytics高级数据分析库通常使用命令式代码编写, 而不能使用SQL运行, 但仍需要访问大量数据. 论文也指出了这当中一个有趣的研究问题是: 如何设计这些库中的数据访问层, 以最大限度地提高运行在上面的代码的灵活性, 但仍然可以从Lakehouse中的优化机会中获益. Databricks成功使用的一种方法是在这些库中提供声明式的DataFrame API, 它将数据读取计划映射为等价的Spark SQL查询计划, 这样就可以从Delta Lake和Delta Engine的优化中获益. 其流程如下. 机器学习API的情况就相对复杂了, 有些数据访问API如Tensorflow的tf.data并不支持数据的查询语言. 最近的系统工作表明, 保持现代查询加速器得到很好的利用, 特别是对于ML推断, 可能是一个困难的问题, 因此Lakehouse访问库将需要解决这一挑战. 论文也指出了, 目前仍然需要标准接口来让数据科学家充分利用Lakehouse(甚至数据仓库)中的强大数据管理功能. 例如, 在Databricks, 已经将Delta Lake与MLflow中的ML实验跟踪服务集成在一起, 让数据科学家轻松地跟踪实验中使用的表版本, 并在以后重现该版本的数据. Lakehouse的特性根据上述内容, 我们可以对Lakehouse的特性做一个总结, 不过论文中并没有描述相关内容, 以下内容来自Databricks的博文What Is a Lakehouse? Lakehouse具有以下主要特征: 事务支持(Transaction support): 在企业的Lakehouse中, 许多数据管道经常会并发地读写数据. 对ACID事务的支持确保了多方并发读写数据(通常使用SQL)时的一致性. 模式执行和治理(Schema enforcement and governance): Lakehouse应该有一种支持模式执行和演化的方法, 支持DW模式体系结构, 比如星型/雪花型模式. 系统应该能够推断数据完整性, 并且应该有健壮的治理和审计机制. BI支持(BI support): Lakehouse支持直接在源数据上使用BI工具. 这减少了过时性, 减少了延迟, 并降低了必须操作数据湖和数据仓库中的两个数据副本的成本. 存储与计算分离(Storage is decoupled from compute): 实际上, 这意味着存储和计算使用独立的集群, 因此这些系统能够扩展到更多并发用户和更大的数据规模. 一些现代数据仓库也具有这种特性. 开放性(Openness): 它们使用的存储格式是开放和标准化的, 如Parquet, 它们提供了一个API, 因此各种工具和引擎, 包括机器学习和Python/R库, 可以有效地直接访问数据. 支持从非结构化数据到结构化数据的各种数据类型(Support for diverse data types ranging from unstructured to structured data): Lakehouse可用于存储, 精化, 分析和访问许多新数据应用程序所需的数据类型, 包括图像, 视频, 音频, 半结构化数据和文本. 支持不同的工作负载(Support for diverse workloads): 包括数据科学, 机器学习, SQL和分析. 可能需要多个工具来支持所有这些工作负载, 但它们都依赖于相同的数据存储库. 端到端流(End-to-end streaming): 实时报告是许多企业的标准. 对流媒体的支持消除了为实时数据应用提供服务的独立系统的需要. 总结本文是对Databricks的Lakehouse论文的阅读总结. 从论文中我们可以看到, Lakehouse这一数据平台架构的产生是多种因素共同推动的, 首要的原因是当前的数据平台架构复杂性高, 需要多次ETL, 不仅增加了数据延时, 也引入了更多出错的风险; 其次, 随着数据量的不断增长, 数据种类的不断增多, 无论是第一代还是第二代数据平台架构都不能很好地用于高级数据分析和机器学习; 另外, 随着云计算技术的发展, Lakehouse这种更为彻底的存算分离架构更适合云环境. 尽管Lakehouse看起来是一个更完美的架构, 但是目前来说由于开源计算引擎在数据湖上的查询性能, 工具的完善程度等原因, Lakehouse尚未得到广泛的使用. 并且在流批一体的存储方面, 数据湖还存在进步的空间, 因此目前大多实时应用还运行在支持实时数据写入的数据仓库中. 不过笔者也坚信, 随着数据湖存储和计算引擎的发展, 数据湖这种低成本的方案未来一定会被广泛使用. 与Lakehouse相关的存储和处理技术未来也是大数据系统领域的研究热点. 参考[1] What Is a Lakehouse?[2] Delta Engine Introduction and Overview of How it Works[3]《 Delta Lake 数据湖专题系列5讲》文章回顾[4] 深度对比 Delta、Iceberg 和 Hudi 三大开源数据湖方案[5] 湖仓一体会成为企业的必选项吗？","link":"/paper-lakehouse.html"},{"title":"Softmax Regression (SR)","text":"Softmax regression (SR) (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. Same as the blog about LR, this blog will detail the modeling approach, loss function, forward and backward propagation of SR. In the end, I will use python with numpy to implement SR and give the use on data sets iris and mnist. You can find all the code here. Softmax functionThe softmax function is defined by the following formula. Where $ K $ is the number of classes and $ K \\geqslant 2 $.$$ softmax(x)=\\frac{1}{\\sum_{j=1}^{K}e^{x^{(j)}}}\\begin{bmatrix} e^{x^{(1)}}\\ e^{x^{(2)}}\\ \\vdots \\ e^{x^{(K)}} \\end{bmatrix} $$Unfortunately, the original softmax definition has a numerical overflow problem in actual use. For a large positive $ x^{(i)} $ value, the value of $ e^{x^{(i)}} $ may be quite large and cause a numerical overflow. Similarly, for a smaller negative $ x^{(i)} $ value, the value of $ e^{x^{(i)}} $ may be very close to zero, resulting in a numerical underflow. Therefore, in practice we use the following equivalent formula.$$ softmax(x-D)=\\frac{1}{\\sum_{j=1}^{K}e^{x^{(j)-D}}}\\begin{bmatrix} e^{x^{(1)}-D}\\ e^{x^{(2)}-D}\\ \\vdots \\ e^{x^{(K)}-D} \\end{bmatrix}=\\frac{1}{e^{-D}\\sum_{j=1}^{K}e^{x^{(j)}}}\\begin{bmatrix} e^{-D}e^{x^{(1)}}\\ e^{-D}e^{x^{(2)}}\\ \\vdots \\ e^{-D}e^{x^{(K)}} \\end{bmatrix}=softmax(x) $$Where $ D=max(x) $.We need to use the derivative of softmax in backpropagation, so let’s calculate it first. For writing convenience, let $ \\hat{y}=softmax(x) $, then $ \\hat{y^{(i)}}=\\frac{e^{x^{(i)}}}{\\sum_{j=1}^{K}e^{x^{(j)}}} $.$$ if \\ j=i, \\ \\ \\ \\ \\frac{\\partial \\hat{y^{(i)}}}{\\partial x^{(j)}}=\\frac{e^{x^{(i)}}\\sum_{j=1}^{K}e^{x^{(j)}}-e^{x^{(i)}}e^{x^{(j)}}}{(\\sum_{j=1}^{K}e^{x^{(j)}})^2}=\\hat{y^{(i)}}-(\\hat{y^{(i)}})^2 $$$$ if \\ j \\neq i, \\ \\ \\ \\ \\frac{\\partial \\hat{y^{(i)}}}{\\partial x^{(j)}}=\\frac{-e^{x^{(i)}e^{x^{(j)}}}}{(\\sum_{j=1}{k}e^{x^{(j)}})^2}=-\\hat{y^{(i)}}\\hat{y^{(j)}} $$ Modeling approachIn LR we assumed that the labels were binary: $ y\\in { {0, 1} } $. SR allows us to handle $ K $ classification problem. In SR we often use one hot vector to represent the label. For example, in the MNIST digit recognition task, we will use $ y=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]^T $ to represent the label of the image with the number 3. In SR we use softmax function to model probability. Suppose we have a training set $ { {(x_1, y_1), (x_2, y_2), …, (x_m, y_m)} } $ of $ m $ labeled examples, where the input features are $ x_i \\in \\Re^{[n, 1]} $. We can use the $i$-th output of the softmax function as the probability that the current sample belongs to the $i$-th class. The formal expression is as follows.$$ P(y^{(i)}=1|x;w^{(i)}, b^{(i)})=\\frac{e^{(w^{(i)})^Tx+b^{(i)}}}{\\sum_{j=1}^{K}e^{(w^{(j)})^Tx+b^{(j)}}} $$ Where$$ w^{(i)} \\in \\Re^{[n, 1]} $$$$ w=\\begin{bmatrix} |&amp; |&amp; |&amp; |\\ w^{(1)}&amp; w^{(2)}&amp; \\cdots &amp; w^{(K)} \\ |&amp; |&amp; |&amp; | \\end{bmatrix} \\in \\Re^{[n, K]} $$$$ b^{(i)} \\in \\Re $$$$ b=\\begin{bmatrix} b^{(1)} &amp; b^{(2)} &amp; \\cdots &amp; b^{(K)} \\end{bmatrix} \\in \\Re^{[1, K]} $$ For the convenience of writing, let $ \\hat{y^{(i)}}=P(y^{(i)}=1|x;w^{(i)}, b^{(i)}) $. Loss functionIn SR our goal is to$$ maximize \\ \\ \\ \\ \\prod_{i=1}^{K}(P(y^{(i)}=1|x;w^{(i)},b^{(i)}))^{I{y^{(i)}=1}} $$$$ maximize \\ \\ \\ \\ \\sum_{i=1}^{K}I{y^{(i)}=1}log(\\hat{y^{(i)}}) $$$$ minimize \\ \\ \\ \\ -\\sum_{i=1}^{K}I{y^{(i)}=1}log(\\hat{y^{(i)}})=-\\sum_{i=1}^{K}y^{(i)}log(\\hat{y^{(i)}}) $$So the loss function of SR is:$$ \\mathcal L(y,\\hat{y})=-\\sum_{i=1}^{K}y^{(i)}log(\\hat{y^{(i)}}) $$ Note: $I$ is the indicator function, in which$$ I{condition} = \\begin{cases} 1 \\ if \\ condition \\ is \\ true \\ 0 \\ else \\end{cases} $$ GradientThe forward propagation of SR is similar with LR, for one example:$$ z=w^Tx+b^T $$$$ \\hat{y}=softmax(z) $$vectorization:$$ Z=Xw+b $$$$ \\hat{Y}=softmax(Z) $$ We can derivative the gradient based on the chain rule. For one example:$$ \\frac{\\partial \\mathcal L}{\\partial z^{(i)}}=\\left{\\begin{matrix}-\\frac{1}{\\hat{y^{(k)}}}(\\hat{y^{(k)}}-(\\hat{y^{(k)}})^2)=\\hat{y^{(k)}}-1 &amp; if \\ i=k \\-\\frac{1}{\\hat{y^{k}}}(-\\hat{y^{(k)}}\\hat{y^{(i)}})=\\hat{y^{(i)}} &amp; if \\ i\\neq k\\end{matrix}\\right. \\Rightarrow \\frac{\\partial \\mathcal L}{\\partial z}=\\hat{y}-y $$ $$ \\frac{\\partial z^{(i)}}{\\partial w^{(i)}}=x,\\ \\ \\ \\ \\frac{\\partial z^{(i)}}{\\partial b^{(i)}}=1 $$$$ \\frac{\\partial \\mathcal L}{\\partial w^{(i)}}=\\frac{\\partial \\mathcal L}{z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial w^{(i)}}=x(\\hat{y^{(i)}}-y^{(i)}) \\Rightarrow \\frac{\\partial \\mathcal L}{\\partial w}=x(\\hat{y}-y)^T $$$$ \\frac{\\partial \\mathcal L}{b^{(i)}}=\\frac{\\partial \\mathcal L}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial b^{(i)}}=\\hat{y^{(i)}}-y^{(i)} \\Rightarrow \\frac{\\partial \\mathcal L}{\\partial b}=(\\hat{y}-y)^T $$vectorization:$$ \\frac{\\partial \\mathcal L}{\\partial w}=\\frac{1}{m}X^T(\\hat{Y}-Y) $$$$ \\frac{\\partial \\mathcal L}{\\partial b}= \\frac{1}{m}\\sum_{row}(\\hat{Y}-Y) $$ ImplementationHere we use python with numpy to implement the forward and backward propagation of SR.1234567891011def softmax(x): &quot;&quot;&quot; Softmax regression for a vector or matrix. Args: x: [n_examples, n_classes] Returns: values after softmax. &quot;&quot;&quot; b = x - np.max(x, axis=1, keepdims=True) expb = np.exp(b) softmax = expb / np.sum(expb, axis=1, keepdims=True) return softmax 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class SoftmaxRegression: def __init__(self, max_iter=200, learning_rate=0.01): self.max_iter = max_iter self.learning_rate = learning_rate def fit(self, X, Y): &quot;&quot;&quot; Train the model. Args: X: [n_samples, n_features] Y: [n_samples, n_classes] &quot;&quot;&quot; m, n = X.shape _, K = Y.shape self.w_ = np.zeros([n, K]) self.b_ = np.zeros([1, K]) self.cost_ = [] for i in range(self.max_iter): Y_hat = self.predict(X) cost = -np.sum(Y * np.log(Y_hat)) / m if i != 0 and i % 10 == 0: print(&quot;Step: &quot; + str(i) + &quot;, Cost: &quot; + str(cost)) self.cost_.append(cost) self.w_ -= self.learning_rate * np.dot(X.T, Y_hat - Y) / m self.b_ -= self.learning_rate * np.sum(Y_hat - Y, axis=0) / m def predict(self, X): &quot;&quot;&quot; Predict the given examples. Args: X: [n_samples, n_features] &quot;&quot;&quot; z = np.dot(X, self.w_) return softmax(np.dot(X, self.w_) + self.b_) def score(self, X, Y): Y_hat = self.predict(X) Y_hat = np.argmax(Y_hat, axis=1) Y = np.argmax(Y, axis=1) true_num = np.sum(Y_hat == Y) return true_num / len(X) ExampleIn order to verify the correctness of the implementation. I experimented on the irsi dataset and the mnist dataset. The parameters and results of the experiment are as follows: iris mnist learnig rate 0.1 0.01 max iterate 100 10000 test accuracy 100% 90.98% You can find the all the experimental code here and reproduce the experimental results.","link":"/softmax-regression.html"},{"title":"Flink MT + Paimon - 开源低成本增量计算的曙光","text":"本文已收录在合集Apche Flink原理与实践中. 自Google”三驾马车”伊始, 大数据处理技术已经发展了二十年. 在前十年中, Hive+Spark这套离线处理技术就已经基本完善; 近十年来, Flink的快速发展又有效地解决了实时处理的问题. 然而, 低成本的近实时处理依然面临挑战. 近来, 随着业界对近实时处理及流批一体架构的需求愈发强烈, 增量计算开始重新被关注. Flink在1.20中推出了Materialized Table(MT)来统一流批两种模式的处理, 配合Paimon已有的Changelog存储能力, 开源低成本增量计算的曙光已至.﻿本文首先介绍增量计算相关的概念, 随后结合Flink和Paimon两个引擎通过具体案例来介绍当前开源引擎增量计算的能力. 从中我们可以得出当前的增量计算还有哪些不足, 亦可窥视其未来发展方向. 增量计算为何被重视?增量计算在数据库领域早有研究, 在数据库领域称为Incremental View Maintenance(IVM), 其核心是为了降低基础数据变化时, 更新物化视图的成本, PostgreSQL对此也有实现. 而大数据处理领域一向是大力出奇迹, 为什么近来也开始关注增量计算呢? 笔者认为有两个核心原因: 一是对流批一体架构的追求, 流批一体的口号业界已经喊了多年, 但是实际落地的场景并不多.即使像Flink这类支持流批两种处理模式的引擎, 不同处理模式下的代码也难以共用.笔者认为其核心原因是底层计算模型没有统一, 批处理是全量计算模型, 而Flink的流处理是基于Changelog的增量计算模型. 不同计算模型下, 对输入和输出的要求不通, 就难以做到流批完全一致. 另一个原因是对成本和数据新鲜度的权衡. 目前的批处理通过调度+全量计算的模式能够做到很低的计算成本, 但延迟在理想情况下也是T+1; 流处理通过Long Running+增量计算的模式能够做到很低的计算延迟, 但Long Running的计算作业带来了极大的运维和资源成本. 这几乎是两个极端, 在近实时场景下, 想要牺牲一定的延迟(从流处理的秒级到分钟级), 来降低成本几乎不可能. 目前来看, 增量计算是能够解决上述两个问题的. 从计算模型上来看, 全量计算是增量计算的一个特例, 能够统一成增量计算. 从成本和数据新鲜度方面, 增量计算能够通过调整计算频率来平衡延迟和成本. 这里需要注意的是, Flink的计算模式是基于Changelog的增量计算, 它是增量计算的一种实现方式, 它把变更作为状态记录在引擎内部, 理论上要保证计算结果准确, 状态是需要永久保存的. 在实操中之所以能给状态设定一个TTL, 是因为存在一种隐含的假设, 增量数据产生的回撤操作不会影响那个TTL之前的历史结果. Flink默认是逐条数据处理的, 在Mini-Batch模式下可以攒小批, 适合低延迟的计算, 但由于Long Runing作业的存在其成本还是较高, 要实现低成本的增量计算显然要另寻方法. 增量计算的模型全量计算中, 每一次的计算结果只与本次输入有关, 而且是幂等的.增量计算中, 每一批次的计算结果, 是由本批数据和历史数据结合计算出来的, 即$R’ = f(R, \\Delta R)$. 从以上公式上来看, 实现增量计算有两个要点: 一是要捕获增量变更, 即$\\Delta R$; 二是如何基于增量变更进行增量计算, 即如何实现函数$f$. 目前来看, 数据库领域和大数据处理领域采用了不太一样的实现方案: 数据库领域, 在增量数据捕获上通常会基于其内部机制记录变更数据, 例如PostgreSQL基于AFTER触发器和Transition Table来捕获增量数据; 在增量计算上, 通过查询改写来实现增量计算, 例如一个InnerJoin $V = R \\Join S$, 若能够捕获关系$R$和$S$的变更$\\Delta R$和$\\Delta S$, 那么其增量可改写为$\\Delta V = \\Delta R \\Join S \\cup R \\Join \\Delta S \\cup \\Delta R \\Join \\Delta S$. 在大数据处理领域, 在增量数据捕获上会基于通用的Changelog模型, 例如Paimon的Changelog Producer. 在计算上, 使用通用的Changelog模型就可以大力出奇迹了, 引擎不需要为每个处理操作制定特有的增量算法, 只需按常规方式实现并考虑数据的Changelog类型即可. 增量计算的实现形式上文我们说到数据库领域和大数据处理领域在增量计算的实现上有所不同, 数据库领域多倾向于通过算法进行查询改写来实现, 虽然可以更加精细化, 但是难度大没有统一的模型, 因此本文不作详细介绍. 这里我们重点介绍一下大数据处理领域增量计算的实现形式. 目前最为成熟的是流处理引擎中基于Changelog+状态的增量计算, Flink就是其中的代表. 这种基于内置状态的增量计算的好处是可以逐条记录处理, 从而达到最低延迟. 但也面临两个问题: 首先是回撤带来大量无用计算, 回撤意味着随着数据的增量变化, 之前的计算结果是无效的, 需要删除并重新更新结果. 实践上, 很多场景下不会需要每新增一条数据就计算一次结果, 因此可以通过攒批来减少回撤, Flink也实现了Mini-Batch Aggregate和Mini-Batch Join来减少回撤. 但这仍无法解决在超长时间跨度回撤的场景下, 要保证计算结果准确, 状态TTL必须无穷的问题. 上述模式最大的问题是状态TTL无穷. 状态本质上是截止当前所有增量数据计算的结果, 设想如果我们去掉状态就无法将增量结果与历史结果合并, 从而得到新的计算结果. 但是换一种思路, 我们并不一定要输出合并后的计算结果, 而是可以输出增量计算结果(当然增量结果也需要符合Changelog格式), 在查询时再对多个增量结果进行合并, 从而得到最终结果. 这种模式就是目前流处理引擎+数据湖的增量计算, 与上述模式不同的是流处理引擎不再需要无穷状态, 只计算并输出增量结果到数据湖(要求数据湖支持Changelog存储), 结果的合并由数据湖进行, 可以选择写时合并或读时合并. 并且通过攒批可以减少回撤, 从而降低合并时的压力. 目前Paimon提供了多种类型的Merge Engine, 可以减少或消除Flink状态. 上述模式尽管解决了状态问题, 但是仍需要一个Long Running的流处理作业. 并且可以发现, 数据湖的写是通过基于Checkpoint的两阶段提交来保证一致性的, 那数据可见性延迟实际上就是Checkpoint的时间间隔. 既然如此, 为了进一步降低成本, 我们可以去掉这个Long Running的流处理作业, 使用低延迟调度的批处理作业作为替代, 只要保证批处理作业的调度时间间隔与流处理作业Checkpoint的时间间隔一致, 就能保证一致的数据可见性延迟. 由于批处理引擎每次只处理增量数据, 把延迟降低到分钟级是可能的. 当然这种基于批处理引擎低延时调度+数据湖的增量计算, 要求批处理引擎也能识别和处理Chengelog数据. 目前Flink在Streaming模式下能配合Paimon处理Changelog数据, 但是Batch模式下并没有Changelog的处理能力, 因此开源引擎目前尚无法实现这一模式. 不过已有商业数据平台实现了类似的能力, 国外的如Snowflake的Dynamic tables, Databricks的Delta Live Tables;国内的如MaxCompute的Delta Table, 云器科技的Dynamic Table. 增量计算示例上文介绍了增量计算的理论, 目前开源系统中, 增量计算实现最完善的是Flink+Paimon这套组合, 这里以这两个系统为例通过一个具体案例, 来更直观地展示增量计算的效果, 主要涉及两个系统的以下几个特性: Flink对处理Changelog数据的支持以及1.20新增的Materialized Table特性. 由于Paimon也是从Flink衍生, 其Changelog存储的实现在原理上也与Flink类似, 因此理解Flink的Changelog原理是关键, 具体可参考笔者之前的文章FlinkSQL源码-Changelog原理与实现. Paimon的Merge Engine能力以及Changelog Producer. 不使用Flink MT语法实际上也能配合Paimon实现上述第二种模式的增量计算, 不过Paimon将会支持Flink MT, 并且MT语法提供了更好的封装方便链路开发. 这里给出一个FlinkMT+Paimon的尝鲜案例. 这一案例统计各个物流公司的运单数量, 但是由于各种原因某个订单的物流公司可以从A变成B.12345678910111213141516171819202122232425262728293031323334353637383940SET &apos;sql-client.execution.result-mode&apos; = &apos;tableau&apos;;SET &apos;table.exec.sink.upsert-materialize&apos; = &apos;NONE&apos;;SET &apos;execution.runtime-mode&apos; = &apos;streaming&apos;;-- 创建一个Paimon Catalog, 我们的操作都将在此Catalog下进行CREATE CATALOG paimon WITH ( &apos;type&apos;=&apos;paimon&apos;, &apos;warehouse&apos;=&apos;file:/tmp/paimon&apos;);-- 创建源表CREATE TABLE tms_source ( order_id STRING PRIMARY KEY NOT ENFORCED, tms_company STRING NOT NULL) WITH ( &apos;connector&apos; = &apos;paimon&apos;, &apos;path&apos; = &apos;file:/tmp/paimon/default.db/tms_source&apos;, &apos;changelog-producer&apos; = &apos;lookup&apos;, &apos;scan.remove-normalize&apos; = &apos;true&apos;);-- 使用Flink MT语法创建一个Materialized TableCREATE MATERIALIZED TABLE continues_tms_res(CONSTRAINT `pk_tms_company` PRIMARY KEY (tms_company) NOT ENFORCED)WITH ( &apos;format&apos; = &apos;debezium-json&apos;, &apos;scan.remove-normalize&apos; = &apos;true&apos;, &apos;changelog-producer&apos; = &apos;lookup&apos;, &apos;merge-engine&apos; = &apos;aggregation&apos;, &apos;fields.order_cnt.aggregate-function&apos; = &apos;sum&apos;)FRESHNESS = INTERVAL &apos;30&apos; SECONDAS SELECT tms_company, 1 AS order_cntFROM tms_source; -- 执行以下语句向源表中插入数据INSERT INTO tms_source VALUES (&apos;001&apos;, &apos;ZhongTong&apos;);INSERT INTO tms_source VALUES (&apos;002&apos;, &apos;ZhongTong&apos;);-- 订单001的承运公司从ZhongTong换成了圆通INSERT INTO tms_source VALUES (&apos;001&apos;, &apos;YuanTong&apos;); 如果在向源表插入数据的过程中观察tms_source和continues_tms_res, 我们可以得到如下结果. 由于两个表都启用了Changelog Producer, 因此表中会记录所有的Changelog记录(+I, -U, +U, -D). 这里需要说明的是, 以下结果是在每条记录的插入时间间隔超过30s时得到的, 如果三条记录在一个Checkpoint批次内插入, 那由于攒批操作, 回撤操作将被消除.12345678910111213141516171819Flink SQL&gt; SELECT * FROM tms_source;+----+-----------------+--------------------+| op | order_id | tms_company |+----+-----------------+--------------------+| +I | 001 | ZhongTong || +I | 002 | ZhongTong || -U | 001 | ZhongTong || +U | 001 | YuanTong |Flink SQL&gt; SELCT * FROM continues_tms_res;+----+-----------------+--------------------+| op | tms_company | order_cnt |+----+-----------------+--------------------+| +I | ZhongTong | 1 || -U | ZhongTong | 1 || +U | ZhongTong | 2 || +I | YuanTong | 1 || -U | ZhongTong | 2 || +U | ZhongTong | 1 | 最终查询两个表的最终状态, 可以得到如下结果, Changelog中间结果(增量计算结果)将在读时被合并.12345678910111213Flink SQL&gt; SELECT * FROM tms_source;+----+------------------+-------------------+| op | order_id | tms_company |+----+------------------+-------------------+| +I | 001 | YuanTong || +I | 002 | ZhongTong |Flink SQL&gt; SELECT * FROM continues_tms_res;+----+------------------+-------------------+| op | tms_company | order_cnt |+----+------------------+-------------------+| +I | YuanTong | 1 || +I | ZhongTong | 1 | continues_tms_res这个Materialized Table背后其实是有一个Long Running的Flink作业, 在实现上SQL Gateway会将Materialized Table语法解析为一个普通的INSERT INTO语句并提交给Flink集群, 它的作业图如下, 可以看到整个作业中是不存在状态算子的, 但是我们最终实现了增量的聚合计算. 以上示例运行在Flink的Streaming模式下, 由于目前Flink Batch模式并不支持基于Changelog数据的计算, 因此FlinkMT + Paimon的组合目前还无法实现调度+批处理模式的增量计算. 不过受益于流处理模式下已有的经验, 相信Flink是最有可能成为补足这一能力的批处理引擎. 总结本文介绍了大数据处理中的增量计算技术, 目前看来不管是开源还是商业化引擎, 都在不断补足这一部分能力. 通过目前趋势来看, 个人认同的一个观点是: 基于批处理引擎低延时调度+增量计算的方式将是未来真正能够实现流批一体的架构, 并且能在分钟级延迟的数据新鲜度下保持较低的成本. 不过这套架构难以解决纯实时的场景. 因此最终离线和近实时处理是能够在存储和计算上都做到流批一体的, 但是纯实时领域还是需要另外一套架构. 参考[1] FLIP-435: Introduce a New Materialized Table for Simplifying Data Pipelines﻿[2] [DISCUSS] FLIP-435: Introduce a New Dynamic Table for Simplifying Data Pipelines﻿[3] Incremental View Maintenance[4] 离线数仓近实时化的成本问题— 增量数仓系列其一[5] 是时候准备结束数仓领域流批一体的讨论了—增量数仓系列其二﻿[6] Incremental Database Computations﻿[7] Tempura: a general cost-based optimizer framework for incremental data processing (Journal Version)﻿[8] 云器科技-产品文档-开发动态表实现近实时增量处理","link":"/flink-mt.html"}],"tags":[{"name":"Compilers","slug":"Compilers","link":"/tags/Compilers/"},{"name":"JavaCC","slug":"JavaCC","link":"/tags/JavaCC/"},{"name":"Apache Calcite","slug":"Apache-Calcite","link":"/tags/Apache-Calcite/"},{"name":"Principles of Distributed System","slug":"Principles-of-Distributed-System","link":"/tags/Principles-of-Distributed-System/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"Transaction","slug":"Transaction","link":"/tags/Transaction/"},{"name":"Flink","slug":"Flink","link":"/tags/Flink/"},{"name":"Flink SQL","slug":"Flink-SQL","link":"/tags/Flink-SQL/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"Glink","slug":"Glink","link":"/tags/Glink/"},{"name":"GeoMesa","slug":"GeoMesa","link":"/tags/GeoMesa/"},{"name":"Hadoop YARN","slug":"Hadoop-YARN","link":"/tags/Hadoop-YARN/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"求职","slug":"求职","link":"/tags/%E6%B1%82%E8%81%8C/"},{"name":"AI System","slug":"AI-System","link":"/tags/AI-System/"},{"name":"Data Platform","slug":"Data-Platform","link":"/tags/Data-Platform/"},{"name":"Delta Lake","slug":"Delta-Lake","link":"/tags/Delta-Lake/"},{"name":"Lakehouse","slug":"Lakehouse","link":"/tags/Lakehouse/"},{"name":"Paimon","slug":"Paimon","link":"/tags/Paimon/"}],"categories":[{"name":"Compilers","slug":"Compilers","link":"/categories/Compilers/"},{"name":"Database","slug":"Database","link":"/categories/Database/"},{"name":"SQL Engine","slug":"Database/SQL-Engine","link":"/categories/Database/SQL-Engine/"},{"name":"Distributed System","slug":"Distributed-System","link":"/categories/Distributed-System/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"求职","slug":"求职","link":"/categories/%E6%B1%82%E8%81%8C/"},{"name":"Data Platform","slug":"Data-Platform","link":"/categories/Data-Platform/"}]}